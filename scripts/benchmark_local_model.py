# -*- coding: utf-8 -*-
"""
Opus 4 vs æœ¬åœ°æ¨¡å‹ Benchmark
æ¸¬è©¦é¡Œï¼šåˆ†æ•£å¼é™æµå™¨ï¼ˆ8/10 é›£åº¦ï¼‰
å‘¼å«æœ¬åœ° Ollama qwen3:32bï¼Œæ¯”è¼ƒå›ç­”å“è³ª
"""
import json
import time
import urllib.request
import sys

OLLAMA_BASE = "http://localhost:11460"

CHALLENGE_PROMPT = """ä½ æ˜¯è³‡æ·±å¾Œç«¯æ¶æ§‹å¸«ã€‚è«‹è¨­è¨ˆä¸¦å¯¦ä½œä¸€å€‹ã€Œåˆ†æ•£å¼æ»‘å‹•çª—å£é™æµå™¨ã€ï¼Œè¦æ±‚ï¼š

1. **æ»‘å‹•çª—å£æ¼”ç®—æ³•**ï¼šä¸æ˜¯å›ºå®šçª—å£ï¼Œè¦ç²¾ç¢ºçš„æ»‘å‹•çª—å£ï¼ˆsorted set æˆ–ç’°å½¢ç·©è¡ï¼‰
2. **Redis åŸå­æ“ä½œ**ï¼šç”¨ Redis LUA script å¯¦ç¾åŸå­æ€§çš„æ»‘å‹•çª—å£è¨ˆç®—ï¼ˆZADD + ZREMRANGEBYSCORE + ZCARDï¼‰
3. **é™ç´šæ©Ÿåˆ¶**ï¼šRedis æ›æ‰æ™‚è‡ªå‹•é™ç´šç‚ºæœ¬åœ°è¨˜æ†¶é«”é™æµï¼Œä¸èƒ½è®“æœå‹™æ•´å€‹æ›æ‰
4. **è‡ªå‹•æ¢å¾©**ï¼šRedis æ¢å¾©å¾Œè‡ªå‹•åˆ‡å›åˆ†æ•£å¼æ¨¡å¼ï¼ˆéœ€æœ‰æ¢é‡åµæ¸¬ï¼‰
5. **å¤šç§Ÿæˆ¶æ”¯æ´**ï¼šæ¯å€‹ tenant æ ¹æ“šè¨‚é–±æ–¹æ¡ˆï¼ˆfree: 60req/min, pro: 600req/min, enterprise: 6000req/minï¼‰å‹•æ…‹èª¿æ•´é™é¡
6. **FastAPI middleware**ï¼šæä¾›å¯ç›´æ¥æ›è¼‰çš„ ASGI middleware
7. **å®Œæ•´å‹åˆ¥æç¤º + async + éŒ¯èª¤è™•ç† + logging**

è«‹è¼¸å‡ºå®Œæ•´å¯åŸ·è¡Œçš„ Python ç¨‹å¼ç¢¼ï¼ˆå–®ä¸€æª”æ¡ˆï¼‰ï¼Œä¸¦åœ¨æœ€å¾Œè§£é‡‹ï¼š
- ç‚ºä»€éº¼é¸æ»‘å‹•çª—å£è€Œéä»¤ç‰Œæ¡¶ï¼Ÿ
- é«˜ä½µç™¼ä¸‹çš„ç«¶æ…‹æ¢ä»¶å¦‚ä½•é¿å…ï¼Ÿ
- Redis LUA script çš„åŸå­æ€§ä¿è­‰æ˜¯ä»€éº¼ï¼Ÿ"""

def call_ollama(model: str, prompt: str, timeout: int = 300) -> tuple[str, float, float]:
    """å‘¼å« Ollama APIï¼Œå›å‚³ (response, elapsed_sec, tok_per_sec)"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 8000,
            "num_ctx": 8192,
        },
    }
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        f"{OLLAMA_BASE}/api/generate",
        data=data,
        headers={"Content-Type": "application/json"},
        method="POST",
    )
    t0 = time.perf_counter()
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        result = json.loads(resp.read().decode("utf-8"))
    elapsed = time.perf_counter() - t0
    response_text = result.get("response", "")
    eval_count = result.get("eval_count", 0)
    tok_per_sec = eval_count / elapsed if elapsed > 0 else 0
    return response_text, elapsed, tok_per_sec


def check_ollama():
    """æª¢æŸ¥ Ollama æ˜¯å¦åœ¨ç·š"""
    try:
        req = urllib.request.Request(f"{OLLAMA_BASE}/api/tags", method="GET")
        with urllib.request.urlopen(req, timeout=5) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            models = [m.get("name", "") for m in data.get("models", [])]
            return True, models
    except Exception as e:
        return False, str(e)


def main():
    print("=" * 70)
    print("  Opus 4 vs æœ¬åœ°æ¨¡å‹ Benchmark")
    print("  æ¸¬è©¦é¡Œï¼šåˆ†æ•£å¼æ»‘å‹•çª—å£é™æµå™¨ï¼ˆé›£åº¦ 8/10ï¼‰")
    print("=" * 70)

    # 1. æª¢æŸ¥ Ollama
    print("\n[1/4] æª¢æŸ¥ Ollama ç‹€æ…‹...")
    online, models = check_ollama()
    if not online:
        print(f"  âŒ Ollama æœªå•Ÿå‹•: {models}")
        print(f"  è«‹ç¢ºèª Ollama é‹è¡Œåœ¨ {OLLAMA_BASE}")
        sys.exit(1)
    print(f"  âœ… Ollama åœ¨ç·šï¼Œå¯ç”¨æ¨¡å‹: {len(models)} å€‹")

    # æ‰¾åˆ° qwen3:32b æˆ–æœ€å¤§çš„æ¨¡å‹
    target_model = None
    for m in models:
        if "qwen3:32b" in m or "qwen3:latest" in m:
            target_model = m
            break
    if not target_model:
        for m in models:
            if "qwen" in m and ("32" in m or "14" in m):
                target_model = m
                break
    if not target_model:
        # fallback to first available
        target_model = models[0] if models else "qwen3:32b"
    print(f"  ğŸ¯ æ¸¬è©¦æ¨¡å‹: {target_model}")

    # 2. é€å‡ºæŒ‘æˆ°
    print(f"\n[2/4] é€å‡ºæŒ‘æˆ°é¡Œçµ¦ {target_model}...")
    print(f"  ï¼ˆé è¨ˆéœ€è¦ 60-180 ç§’ï¼Œè«‹è€å¿ƒç­‰å¾…...ï¼‰")

    try:
        response, elapsed, tps = call_ollama(target_model, CHALLENGE_PROMPT, timeout=600)
    except Exception as e:
        print(f"  âŒ å‘¼å«å¤±æ•—: {e}")
        sys.exit(1)

    print(f"  âœ… å›ç­”å®Œæˆï¼")
    print(f"  â±ï¸  è€—æ™‚: {elapsed:.1f} ç§’")
    print(f"  ğŸš€ é€Ÿåº¦: {tps:.1f} tok/s")
    print(f"  ğŸ“ å›ç­”é•·åº¦: {len(response)} å­—å…ƒ")

    # 3. å„²å­˜çµæœ
    print("\n[3/4] å„²å­˜çµæœ...")
    output = {
        "model": target_model,
        "challenge": "åˆ†æ•£å¼æ»‘å‹•çª—å£é™æµå™¨ï¼ˆé›£åº¦ 8/10ï¼‰",
        "elapsed_sec": round(elapsed, 1),
        "tok_per_sec": round(tps, 1),
        "response_length": len(response),
        "response": response,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
    out_path = "d:/zhe-wei-tech/scripts/benchmark_result.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ“„ å·²å„²å­˜: {out_path}")

    # 4. è¼¸å‡ºå›ç­”
    print("\n[4/4] æœ¬åœ°æ¨¡å‹å›ç­”ï¼š")
    print("=" * 70)
    print(response)
    print("=" * 70)

    # æ‘˜è¦
    print(f"\nğŸ“Š Benchmark æ‘˜è¦")
    print(f"  æ¨¡å‹: {target_model}")
    print(f"  è€—æ™‚: {elapsed:.1f}s | é€Ÿåº¦: {tps:.1f} tok/s | é•·åº¦: {len(response)} å­—å…ƒ")
    print(f"  çµæœå·²å­˜: {out_path}")
    print(f"\n  â†’ æ¥ä¸‹ä¾†ç”± Opus 4 å°æ¯”è©•åˆ†")


if __name__ == "__main__":
    main()

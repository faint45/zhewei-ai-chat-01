# Zhe-Wei Tech — Cloud Failover (Contabo Tokyo 8GB)
# Usage: docker compose -f docker-compose.failover.yml up -d
# Purpose: Keep core services alive when local machine is offline
#
# Memory budget (~7.5GB usable):
#   gateway:      ~20MB
#   tunnel:       ~30MB
#   portal:       ~150MB
#   brain_server: ~500MB (with Ollama for local inference)
#   ollama:       ~3GB  (qwen2.5:3b + nomic-embed-text)
#   cms:          ~200MB
#   codesim:      ~200MB
#   smart_bridge: ~200MB
#   prediction:   ~200MB
#   Total:        ~4.5GB (leaves ~3GB headroom)
#
# NOT included (needs GPU):
#   - Vision (YOLOv8)
#   - ComfyUI (Stable Diffusion)
#   - Dify (optional, can add later)

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "5m"
    max-file: "2"

x-common: &common
  build:
    context: .
    dockerfile: Dockerfile
  env_file: [.env]
  restart: unless-stopped
  logging: *default-logging
  deploy:
    resources:
      limits:
        memory: 300M

services:
  # ── Ollama (local inference on CPU) ──
  ollama:
    image: ollama/ollama:latest
    container_name: zhewei_ollama
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 3G
    volumes:
      - ollama_models:/root/.ollama
    networks: [default]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:11434/api/tags || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ── Nginx Gateway ──
  gateway:
    image: nginx:alpine
    container_name: zhewei_gateway
    volumes:
      - ./gateway/nginx.cloud.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 50M
    depends_on:
      portal: { condition: service_started }
      brain_server: { condition: service_started }
    networks: [default]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost/nginx-health || exit 1"]
      interval: 60s
      timeout: 5s
      retries: 3

  # ── Cloudflare Tunnel (failover instance) ──
  tunnel:
    image: cloudflare/cloudflared:latest
    container_name: zhewei_tunnel
    restart: always
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 60M
    command: ["tunnel", "--protocol", "http2", "run", "--token", "${CLOUDFLARE_FAILOVER_TOKEN}"]
    depends_on:
      gateway: { condition: service_started }
    networks: [default]

  # ── Portal (PWA) ──
  portal:
    <<: *common
    container_name: zhewei_portal
    environment:
      PORTAL_PORT: "8888"
    volumes:
      - .:/app
      - ./portal:/app/portal
    command: ["python", "portal_server.py"]
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8888/health')"]
      interval: 60s
      timeout: 5s
      retries: 3

  # ── Brain Server (smart route with Ollama + cloud AI) ──
  brain_server:
    <<: *common
    container_name: zhewei_brain
    deploy:
      resources:
        limits:
          memory: 500M
    environment:
      BRAIN_WORKSPACE: /app/brain_workspace
      ZHEWEI_MEMORY_ROOT: /memory
      BRAIN_WS_PORT: "8000"
      PORT: "8000"
      AI_COST_MODE: smart_route
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: qwen2.5:3b
    depends_on:
      ollama: { condition: service_healthy }
    volumes:
      - .:/app
      - ./zhewei_memory:/memory
      - ./brain_workspace:/app/brain_workspace
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ── Smart Bridge ──
  smart_bridge:
    <<: *common
    container_name: zhewei_smart_bridge
    environment:
      SMART_BRIDGE_PORT: "8003"
      SMART_BRIDGE_HOST: "0.0.0.0"
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - .:/app
      - ./bridge_workspace:/app/bridge_workspace
      - ./zhewei_memory:/memory
    command: ["python", "smart_bridge.py"]
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8003/health')"]
      interval: 60s
      timeout: 10s
      retries: 3

  # ── CMS ──
  cms:
    <<: *common
    container_name: zhewei_cms
    volumes:
      - .:/app
      - ./construction_mgmt:/app/construction_mgmt
    command: ["python", "construction_mgmt/app.py"]
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8020/')"]
      interval: 60s
      timeout: 5s
      retries: 3

  # ── CodeSim ──
  codesim:
    <<: *common
    container_name: zhewei_codesim
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    volumes:
      - .:/app
      - ./simulator:/app/simulator
    command: ["python", "simulator/code_simulator.py"]
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8001/')"]
      interval: 60s
      timeout: 5s
      retries: 3

  # ── Prediction System ──
  prediction:
    <<: *common
    container_name: zhewei_prediction
    volumes:
      - .:/app
      - ./prediction_modules:/app/prediction_modules
    command: ["python", "prediction_modules/prediction_service.py"]
    networks: [default]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8025/')"]
      interval: 60s
      timeout: 5s
      retries: 3

volumes:
  ollama_models:

networks:
  default:
    driver: bridge

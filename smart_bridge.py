# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€ â€” Smart Bridge æ™ºæ…§å‹å°è©±æ©‹æ¥æœå‹™
Port: 8003

åŠŸèƒ½ï¼š
1. å¤–ç¶²å¯è¨ªå•çš„å³æ™‚å°è©±ç•Œé¢ (WebSocket)
2. è¦–è¦ºåŒ–é¡¯ç¤º AI é‹ä½œéç¨‹ï¼ˆæ€è€ƒ â†’ åŸ·è¡Œ â†’ å„ªåŒ–ï¼‰
3. æ™ºæ…§å‹æ§åˆ¶ APIï¼šå…©éšæ®µç”Ÿæˆï¼ˆä½æˆæœ¬80% + é«˜å“è³ªç²¾ä¿®ï¼‰
4. æœ¬åœ° Ollama æ¨¡å‹å­¸ç¿’æ§åˆ¶èˆ‡å”åŠ©

æ¶æ§‹ï¼š
- Phase 1 (80%): Ollama æœ¬åœ°æ¨¡å‹ / Groq å…è²»æ¨¡å‹ å¿«é€Ÿå»ºç«‹æ¡†æ¶
- Phase 2 (20%): Gemini / Claude é«˜å“è³ªç²¾ä¿®èˆ‡å„ªåŒ–
"""

import asyncio
import json
import os
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional, Callable

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
import httpx
import uvicorn

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))
try:
    from dotenv import load_dotenv
    load_dotenv(ROOT / ".env")
except ImportError:
    pass

# å¼•å…¥ç¾æœ‰ AI æœå‹™
from ai_service import SmartAIService, OllamaService, GeminiService, ClaudeService, AIServiceFactory

# å¼•å…¥ Ollama å­¸ç¿’æ§åˆ¶å™¨
try:
    from ollama_learning_controller import OllamaLearningController
    LEARNING_AVAILABLE = True
except ImportError:
    LEARNING_AVAILABLE = False
    print("âš ï¸ OllamaLearningController æœªè¼‰å…¥")

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PORT = int(os.environ.get("SMART_BRIDGE_PORT", "8003"))
HOST = os.environ.get("SMART_BRIDGE_HOST", "0.0.0.0")
BRIDGE_WORKSPACE = ROOT / "bridge_workspace"
BRIDGE_WORKSPACE.mkdir(exist_ok=True)
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://host.docker.internal:11460")

# å…©éšæ®µæ¨¡å‹é…ç½®
PHASE1_PROVIDERS = ["ollama", "groq"]  # ä½æˆæœ¬å¿«é€Ÿç”Ÿæˆ
PHASE2_PROVIDERS = ["gemini", "claude"]  # é«˜å“è³ªç²¾ä¿®

# æˆæœ¬ä¼°è¨ˆï¼ˆæ¯ 1K tokensï¼‰
COST_RATES = {
    "ollama": 0.0,      # æœ¬åœ°å…è²»
    "groq": 0.0,        # å…è²»é¡åº¦
    "gemini": 0.0005,   # $0.5 / 1M tokens
    "claude": 0.003,    # $3 / 1M tokens
}

# â”€â”€ å°ˆæ¡ˆç®¡ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ProjectManager:
    """å°ˆæ¡ˆç®¡ç†å™¨ï¼šæ”¯æ´å¤šå°ˆæ¡ˆç¨ç«‹é–‹ç™¼ + æ™ºæ…§åŒæ­¥"""
    
    def __init__(self):
        self.projects_dir = BRIDGE_WORKSPACE / "projects"
        self.projects_dir.mkdir(exist_ok=True)
        self._projects_file = self.projects_dir / "projects.json"
        self._lock_file = self.projects_dir / ".lock"
        self._last_sync_time = None
        self._load()
    
    def _acquire_lock(self, timeout=5):
        """å–å¾—æª”æ¡ˆé–ï¼ˆé¿å…å¤šå€‹å¯¦ä¾‹åŒæ™‚å¯«å…¥ï¼‰"""
        start_time = time.time()
        while self._lock_file.exists():
            if time.time() - start_time > timeout:
                # è¶…æ™‚å‰‡å¼·åˆ¶ç§»é™¤é–ï¼ˆå¯èƒ½æ˜¯ç•°å¸¸é€€å‡ºå°è‡´ï¼‰
                self._lock_file.unlink()
                break
            time.sleep(0.1)
        self._lock_file.touch()
    
    def _release_lock(self):
        """é‡‹æ”¾æª”æ¡ˆé–"""
        if self._lock_file.exists():
            self._lock_file.unlink()
    
    def _load(self):
        """è¼‰å…¥å°ˆæ¡ˆåˆ—è¡¨ï¼ˆæ™ºæ…§åˆä½µé ç«¯èˆ‡æœ¬åœ°è®Šæ›´ï¼‰"""
        if self._projects_file.exists():
            try:
                disk_data = json.loads(self._projects_file.read_text(encoding="utf-8"))
                
                # é¦–æ¬¡è¼‰å…¥æˆ–éœ€è¦åŒæ­¥
                if not hasattr(self, 'projects') or self._should_sync():
                    self.projects = disk_data
                    self._last_sync_time = time.time()
                else:
                    # æ™ºæ…§åˆä½µï¼šä¿ç•™è¨˜æ†¶é«”ä¸­è¼ƒæ–°çš„è³‡æ–™
                    self._merge_projects(disk_data)
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥å°ˆæ¡ˆå¤±æ•—: {e}")
                self._init_default_project()
        else:
            self._init_default_project()
    
    def _init_default_project(self):
        """åˆå§‹åŒ–é è¨­å°ˆæ¡ˆ"""
        self.projects = {
            "default": {
                "id": "default",
                "name": "é è¨­å°ˆæ¡ˆ",
                "description": "Smart Bridge é è¨­å·¥ä½œå€",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "color": "cyan",
                "icon": "cube",
                "history": [],
                "context": {},
                "stats": {"messages": 0, "tokens": 0, "cost": 0.0}
            }
        }
        self._save()
    
    def _should_sync(self):
        """åˆ¤æ–·æ˜¯å¦éœ€è¦åŒæ­¥ï¼ˆæ¯ 5 ç§’æª¢æŸ¥ä¸€æ¬¡ï¼‰"""
        if not self._last_sync_time:
            return True
        return time.time() - self._last_sync_time > 5
    
    def _merge_projects(self, disk_data: dict):
        """æ™ºæ…§åˆä½µå°ˆæ¡ˆè³‡æ–™ï¼ˆä»¥ updated_at æ™‚é–“æˆ³ç‚ºæº–ï¼‰"""
        merged = {}
        all_ids = set(self.projects.keys()) | set(disk_data.keys())
        
        for pid in all_ids:
            mem_proj = self.projects.get(pid)
            disk_proj = disk_data.get(pid)
            
            if not disk_proj:
                # åƒ…å­˜åœ¨è¨˜æ†¶é«”ä¸­ï¼ˆæ–°å»ºç«‹çš„ï¼‰
                merged[pid] = mem_proj
            elif not mem_proj:
                # åƒ…å­˜åœ¨ç£ç¢Ÿä¸­ï¼ˆå…¶ä»–å¯¦ä¾‹å»ºç«‹çš„ï¼‰
                merged[pid] = disk_proj
            else:
                # å…©é‚Šéƒ½æœ‰ï¼Œæ¯”è¼ƒæ™‚é–“æˆ³
                mem_time = mem_proj.get("updated_at", "")
                disk_time = disk_proj.get("updated_at", "")
                
                if mem_time >= disk_time:
                    merged[pid] = mem_proj
                else:
                    # ç£ç¢Ÿè¼ƒæ–°ï¼Œä½†ä¿ç•™è¨˜æ†¶é«”ä¸­æœªå„²å­˜çš„è¨Šæ¯
                    merged[pid] = disk_proj
                    if len(mem_proj.get("history", [])) > len(disk_proj.get("history", [])):
                        merged[pid]["history"] = mem_proj["history"]
                        merged[pid]["stats"] = mem_proj["stats"]
        
        self.projects = merged
        self._last_sync_time = time.time()
    
    def _save(self):
        """å„²å­˜å°ˆæ¡ˆåˆ—è¡¨ï¼ˆå¸¶æª”æ¡ˆé–ï¼‰"""
        try:
            self._acquire_lock()
            
            # å„²å­˜å‰å…ˆé‡æ–°è¼‰å…¥ï¼Œç¢ºä¿ä¸è¦†è“‹å…¶ä»–å¯¦ä¾‹çš„è®Šæ›´
            if self._projects_file.exists():
                disk_data = json.loads(self._projects_file.read_text(encoding="utf-8"))
                self._merge_projects(disk_data)
            
            # å¯«å…¥æª”æ¡ˆ
            self._projects_file.write_text(
                json.dumps(self.projects, ensure_ascii=False, indent=2), 
                encoding="utf-8"
            )
            self._last_sync_time = time.time()
        finally:
            self._release_lock()
    
    def sync(self):
        """æ‰‹å‹•åŒæ­¥ï¼ˆé‡æ–°è¼‰å…¥ç£ç¢Ÿè³‡æ–™ï¼‰"""
        self._load()
        return {"ok": True, "synced_at": datetime.now().isoformat()}
    
    def list(self):
        """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆ"""
        return list(self.projects.values())
    
    def get(self, project_id: str):
        """å–å¾—å°ˆæ¡ˆ"""
        return self.projects.get(project_id)
    
    def create(self, name: str, description: str = "", color: str = "purple", icon: str = "folder"):
        """å»ºç«‹æ–°å°ˆæ¡ˆ"""
        project_id = f"proj_{uuid.uuid4().hex[:8]}"
        self.projects[project_id] = {
            "id": project_id,
            "name": name,
            "description": description,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "color": color,
            "icon": icon,
            "history": [],
            "context": {},
            "stats": {"messages": 0, "tokens": 0, "cost": 0.0}
        }
        self._save()
        return self.projects[project_id]
    
    def update(self, project_id: str, **kwargs):
        """æ›´æ–°å°ˆæ¡ˆ"""
        if project_id in self.projects:
            self.projects[project_id].update(kwargs)
            self.projects[project_id]["updated_at"] = datetime.now().isoformat()
            self._save()
            return self.projects[project_id]
        return None
    
    def delete(self, project_id: str):
        """åˆªé™¤å°ˆæ¡ˆ"""
        if project_id in self.projects and project_id != "default":
            del self.projects[project_id]
            self._save()
            return True
        return False
    
    def add_message(self, project_id: str, role: str, content: str, metadata: dict = None):
        """æ–°å¢è¨Šæ¯åˆ°å°ˆæ¡ˆæ­·å²"""
        if project_id in self.projects:
            msg = {
                "role": role,
                "content": content,
                "timestamp": datetime.now().isoformat(),
                "metadata": metadata or {}
            }
            self.projects[project_id]["history"].append(msg)
            self.projects[project_id]["stats"]["messages"] += 1
            self._save()
            return msg
        return None
    
    def clear_history(self, project_id: str):
        """æ¸…é™¤å°ˆæ¡ˆå°è©±æ­·å²"""
        if project_id in self.projects:
            self.projects[project_id]["history"] = []
            self.projects[project_id]["stats"]["messages"] = 0
            self._save()
            return True
        return False
    
    def export_project(self, project_id: str):
        """åŒ¯å‡ºå–®å€‹å°ˆæ¡ˆ"""
        project = self.projects.get(project_id)
        if not project:
            return None
        return project
    
    def export_all(self):
        """åŒ¯å‡ºæ‰€æœ‰å°ˆæ¡ˆ"""
        return self.projects
    
    def import_projects(self, data: dict, mode: str = "skip"):
        """
        åŒ¯å…¥å°ˆæ¡ˆè³‡æ–™
        mode: 'skip' (è·³éå·²å­˜åœ¨), 'overwrite' (è¦†è“‹), 'merge' (åˆä½µæ­·å²)
        """
        imported = []
        skipped = []
        errors = []
        
        # æ”¯æ´å–®å€‹å°ˆæ¡ˆæˆ–å¤šå€‹å°ˆæ¡ˆ
        if isinstance(data, dict):
            if "id" in data and "name" in data:
                # å–®å€‹å°ˆæ¡ˆ
                projects_to_import = {data["id"]: data}
            else:
                # å¤šå€‹å°ˆæ¡ˆ
                projects_to_import = data
        else:
            return {"ok": False, "error": "ç„¡æ•ˆçš„è³‡æ–™æ ¼å¼"}
        
        for pid, project_data in projects_to_import.items():
            try:
                # é©—è­‰å¿…è¦æ¬„ä½
                if not all(k in project_data for k in ["id", "name"]):
                    errors.append({"id": pid, "error": "ç¼ºå°‘å¿…è¦æ¬„ä½"})
                    continue
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                exists = pid in self.projects
                
                if exists:
                    if mode == "skip":
                        skipped.append(pid)
                        continue
                    elif mode == "overwrite":
                        self.projects[pid] = project_data
                        self.projects[pid]["updated_at"] = datetime.now().isoformat()
                        imported.append(pid)
                    elif mode == "merge":
                        # åˆä½µæ­·å²è¨Šæ¯
                        existing = self.projects[pid]
                        existing_history = existing.get("history", [])
                        import_history = project_data.get("history", [])
                        
                        # åˆä½µä¸¦å»é‡ï¼ˆæ ¹æ“š timestampï¼‰
                        all_history = existing_history + import_history
                        unique_history = []
                        seen_timestamps = set()
                        for msg in sorted(all_history, key=lambda x: x.get("timestamp", "")):
                            ts = msg.get("timestamp")
                            if ts not in seen_timestamps:
                                unique_history.append(msg)
                                seen_timestamps.add(ts)
                        
                        self.projects[pid]["history"] = unique_history
                        self.projects[pid]["stats"]["messages"] = len(unique_history)
                        self.projects[pid]["updated_at"] = datetime.now().isoformat()
                        imported.append(pid)
                else:
                    # æ–°å°ˆæ¡ˆ
                    self.projects[pid] = project_data
                    self.projects[pid]["updated_at"] = datetime.now().isoformat()
                    imported.append(pid)
            
            except Exception as e:
                errors.append({"id": pid, "error": str(e)})
        
        self._save()
        
        return {
            "ok": True,
            "imported": imported,
            "skipped": skipped,
            "errors": errors,
            "total": len(imported) + len(skipped) + len(errors)
        }

project_manager = ProjectManager()

# â”€â”€ App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(
    title="ç¯‰æœªç§‘æŠ€ Smart Bridge",
    description="æ™ºæ…§å‹å°è©±æ©‹æ¥æœå‹™ - å¤–ç¶²å°è©± + æœ¬åœ°æ¨¡å‹å­¸ç¿’æ§åˆ¶ + å¤šå°ˆæ¡ˆç®¡ç†",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– AI æœå‹™
smart_ai = SmartAIService()
ollama_ai = smart_ai.ollama
gemini_ai = smart_ai.gemini
claude_ai = smart_ai.claude

# WebSocket é€£ç·šç®¡ç†
class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}
        self.sessions: dict[str, dict] = {}  # session_id -> {history, context}
    
    async def connect(self, websocket: WebSocket, session_id: str) -> str:
        await websocket.accept()
        self.active_connections[session_id] = websocket
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                "history": [],
                "context": {},
                "created_at": datetime.now().isoformat(),
            }
        return session_id
    
    def disconnect(self, session_id: str):
        if session_id in self.active_connections:
            del self.active_connections[session_id]
    
    async def send_to_session(self, session_id: str, message: dict):
        if session_id in self.active_connections:
            try:
                await self.active_connections[session_id].send_json(message)
            except Exception:
                self.disconnect(session_id)
    
    async def broadcast_operation(self, session_id: str, operation: dict):
        """å»£æ’­ AI é‹ä½œéç¨‹"""
        msg = {
            "type": "operation",
            "timestamp": datetime.now().isoformat(),
            **operation
        }
        await self.send_to_session(session_id, msg)

manager = ConnectionManager()

# â”€â”€ Smart Bridge Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SmartBridgeCore:
    """
    æ™ºæ…§å‹æ©‹æ¥æ ¸å¿ƒï¼šå…©éšæ®µç”Ÿæˆ + å³æ™‚è¦–è¦ºåé¥‹
    """
    
    def __init__(self):
        self.cost_stats = {
            "total_requests": 0,
            "phase1_tokens": 0,
            "phase2_tokens": 0,
            "estimated_cost": 0.0,
            "saved_cost": 0.0,  # ç›¸æ¯”å…¨ç”¨é«˜å“è³ªæ¨¡å‹
        }
    
    async def two_phase_generate(
        self,
        prompt: str,
        session_id: str,
        task_type: str = "code",
        stream_callback: Optional[Callable] = None
    ) -> dict:
        """
        å…©éšæ®µæ™ºæ…§ç”Ÿæˆï¼š
        Phase 1: ä½æˆæœ¬æ¨¡å‹å¿«é€Ÿç”Ÿæˆ 80% æ¡†æ¶
        Phase 2: é«˜å“è³ªæ¨¡å‹ç²¾ä¿®å„ªåŒ– 20%
        
        å³æ™‚ stream é‹ä½œéç¨‹åˆ°å‰ç«¯é¡¯ç¤º
        """
        start_time = time.time()
        
        # â”€â”€ Phase 1: å¿«é€Ÿæ¡†æ¶ (80%) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if stream_callback:
            await stream_callback({
                "stage": "phase1_start",
                "message": "ğŸš€ Phase 1: ä½æˆæœ¬æ¨¡å‹å»ºç«‹æ¡†æ¶...",
                "providers": PHASE1_PROVIDERS,
                "estimated_progress": 0,
            })
        
        phase1_result = await self._phase1_generate(prompt, task_type, stream_callback, session_id)
        
        if stream_callback:
            await stream_callback({
                "stage": "phase1_complete",
                "message": f"âœ… Phase 1 å®Œæˆ ({phase1_result['provider']})",
                "duration_ms": phase1_result['duration_ms'],
                "tokens": phase1_result['tokens'],
                "progress": 80,
            })
        
        # â”€â”€ Phase 2: ç²¾ä¿®å„ªåŒ– (20%) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if stream_callback:
            await stream_callback({
                "stage": "phase2_start",
                "message": "ğŸ¯ Phase 2: é«˜å“è³ªæ¨¡å‹ç²¾ä¿®å„ªåŒ–...",
                "providers": PHASE2_PROVIDERS,
                "estimated_progress": 80,
            })
        
        phase2_result = await self._phase2_optimize(
            prompt, phase1_result['content'], task_type, stream_callback, session_id
        )
        
        total_duration = int((time.time() - start_time) * 1000)
        
        # æˆæœ¬è¨ˆç®—
        phase1_cost = phase1_result['tokens'] * COST_RATES.get(phase1_result['provider'], 0) / 1000
        phase2_cost = phase2_result['tokens'] * COST_RATES.get(phase2_result['provider'], 0) / 1000
        total_cost = phase1_cost + phase2_cost
        
        # å¦‚æœå…¨ç”¨é«˜å“è³ªæ¨¡å‹çš„ä¼°è¨ˆæˆæœ¬
        full_quality_cost = (phase1_result['tokens'] + phase2_result['tokens']) * 0.003 / 1000
        saved = full_quality_cost - total_cost
        
        self.cost_stats["total_requests"] += 1
        self.cost_stats["phase1_tokens"] += phase1_result['tokens']
        self.cost_stats["phase2_tokens"] += phase2_result['tokens']
        self.cost_stats["estimated_cost"] += total_cost
        self.cost_stats["saved_cost"] += saved
        
        if stream_callback:
            await stream_callback({
                "stage": "complete",
                "message": f"ğŸ‰ å®Œæˆï¼ç¸½è€—æ™‚ {total_duration}msï¼Œç¯€çœ ${saved:.4f}",
                "duration_ms": total_duration,
                "total_tokens": phase1_result['tokens'] + phase2_result['tokens'],
                "cost": total_cost,
                "saved": saved,
                "progress": 100,
            })
        
        return {
            "ok": True,
            "content": phase2_result['content'],
            "phase1": phase1_result,
            "phase2": phase2_result,
            "total_duration_ms": total_duration,
            "cost_usd": total_cost,
            "saved_usd": saved,
            "improvements": phase2_result.get('improvements', []),
        }
    
    async def _phase1_generate(
        self,
        prompt: str,
        task_type: str,
        stream_callback: Optional[Callable],
        session_id: str
    ) -> dict:
        """ç¬¬ä¸€éšæ®µï¼šä½æˆæœ¬å¿«é€Ÿç”Ÿæˆæ¡†æ¶"""
        
        system_msg = self._get_phase1_system_prompt(task_type)
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ]
        
        # å„ªå…ˆå˜—è©¦ Ollamaï¼ˆæœ¬åœ°å…è²»ï¼‰
        providers = ["ollama", "groq"]
        
        for provider in providers:
            try:
                if stream_callback:
                    await stream_callback({
                        "stage": "phase1_try",
                        "message": f"ğŸ”„ å˜—è©¦ {provider}...",
                        "provider": provider,
                    })
                
                t0 = time.time()
                service = AIServiceFactory.create(provider)
                content = await service.chat(messages)
                duration_ms = int((time.time() - t0) * 1000)
                
                # ä¼°è¨ˆ tokens
                tokens = len(prompt.split()) + len(content.split())
                
                return {
                    "provider": provider,
                    "content": content,
                    "duration_ms": duration_ms,
                    "tokens": tokens,
                    "success": True,
                }
            except Exception as e:
                if stream_callback:
                    await stream_callback({
                        "stage": "phase1_fail",
                        "message": f"âš ï¸ {provider} å¤±æ•—: {str(e)[:50]}",
                        "provider": provider,
                    })
                continue
        
        # å…¨éƒ¨å¤±æ•—ï¼Œå›é€€åˆ° Gemini
        if stream_callback:
            await stream_callback({
                "stage": "phase1_fallback",
                "message": "ğŸ”„ Phase 1 å›é€€åˆ° Gemini...",
            })
        
        t0 = time.time()
        content = await gemini_ai.chat(messages)
        duration_ms = int((time.time() - t0) * 1000)
        tokens = len(prompt.split()) + len(content.split())
        
        return {
            "provider": "gemini_fallback",
            "content": content,
            "duration_ms": duration_ms,
            "tokens": tokens,
            "success": True,
        }
    
    async def _phase2_optimize(
        self,
        original_prompt: str,
        phase1_content: str,
        task_type: str,
        stream_callback: Optional[Callable],
        session_id: str
    ) -> dict:
        """ç¬¬äºŒéšæ®µï¼šé«˜å“è³ªç²¾ä¿®å„ªåŒ–"""
        
        optimize_prompt = f"""è«‹ç²¾ä¿®ä¸¦å„ªåŒ–ä»¥ä¸‹å…§å®¹ã€‚

åŸå§‹éœ€æ±‚ï¼š
{original_prompt}

åˆç¨¿ï¼ˆéœ€è¦å„ªåŒ–ï¼‰ï¼š
{phase1_content}

è«‹é€²è¡Œä»¥ä¸‹å„ªåŒ–ï¼š
{self._get_phase2_instructions(task_type)}

è«‹è¼¸å‡ºå„ªåŒ–å¾Œçš„å®Œæ•´å…§å®¹ï¼Œä¸¦åœ¨æœ«å°¾ç°¡è¦åˆ—å‡º 3-5 å€‹ä¸»è¦æ”¹é€²é»ã€‚"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å“è³ªå„ªåŒ–å°ˆå®¶ã€‚è«‹ä¿æŒå…§å®¹çš„å®Œæ•´æ€§ï¼ŒåŒæ™‚é¡¯è‘—æå‡å“è³ªã€‚"},
            {"role": "user", "content": optimize_prompt}
        ]
        
        # å˜—è©¦ Gemini ç„¶å¾Œ Claude
        providers = [("gemini", gemini_ai), ("claude", claude_ai)]
        
        for provider_name, service in providers:
            try:
                if stream_callback:
                    await stream_callback({
                        "stage": "phase2_try",
                        "message": f"ğŸ¯ ä½¿ç”¨ {provider_name} ç²¾ä¿®...",
                        "provider": provider_name,
                    })
                
                t0 = time.time()
                content = await service.chat(messages)
                duration_ms = int((time.time() - t0) * 1000)
                
                # æå–æ”¹é€²é»
                improvements = self._extract_improvements(content)
                content = self._remove_improvements_section(content)
                
                tokens = len(optimize_prompt.split()) + len(content.split())
                
                return {
                    "provider": provider_name,
                    "content": content,
                    "duration_ms": duration_ms,
                    "tokens": tokens,
                    "improvements": improvements,
                    "success": True,
                }
            except Exception as e:
                if stream_callback:
                    await stream_callback({
                        "stage": "phase2_fail",
                        "message": f"âš ï¸ {provider_name} å¤±æ•—: {str(e)[:50]}",
                    })
                continue
        
        # å…¨éƒ¨å¤±æ•—ï¼Œè¿”å› Phase 1 çµæœ
        return {
            "provider": "phase1_fallback",
            "content": phase1_content,
            "duration_ms": 0,
            "tokens": 0,
            "improvements": [],
            "success": False,
        }
    
    async def code_generation(
        self,
        prompt: str,
        session_id: str,
        stream_callback: Optional[Callable] = None
    ) -> dict:
        """
        è‡ªç„¶å°è©±æ¨¡å¼
        ä½¿ç”¨æœ¬åœ° qwen3:8b æ¨¡å‹é€²è¡Œè‡ªç„¶å°è©±ï¼Œåƒ Cascade ä¸€æ¨£
        """
        start_time = time.time()
        
        if stream_callback:
            await stream_callback({
                "stage": "thinking",
                "message": "ï¿½ æ€è€ƒä¸­...",
            })
        
        # æ·±åº¦æ¨ç†çš„ç³»çµ±æç¤ºè©ï¼ˆä½¿ç”¨ deepseek-r1 çš„æ¨ç†èƒ½åŠ›ï¼‰
        system_prompt = """ä½ æ˜¯ä¸€å€‹å…·æœ‰æ·±åº¦æ¨ç†èƒ½åŠ›çš„ AI ç¨‹å¼è¨­è¨ˆåŠ©æ‰‹ã€‚ä½ çš„æ€è€ƒéç¨‹é¡ä¼¼ Claude å’Œ o1ï¼Œèƒ½å¤ é€²è¡Œå¤šæ­¥é©Ÿæ¨ç†å’Œè¤‡é›œå•é¡Œè§£æ±ºã€‚

æ ¸å¿ƒèƒ½åŠ›ï¼š
1. **æ·±åº¦æ€è€ƒ** - é‡åˆ°è¤‡é›œå•é¡Œæ™‚ï¼Œå…ˆåœ¨ <think> æ¨™ç±¤ä¸­é€²è¡Œæ¨ç†ï¼Œå†çµ¦å‡ºç­”æ¡ˆ
2. **å¤šæ­¥é©Ÿè¦åŠƒ** - å°‡è¤‡é›œä»»å‹™åˆ†è§£ç‚ºå¯åŸ·è¡Œçš„æ­¥é©Ÿ
3. **è‡ªæˆ‘é©—è­‰** - æª¢æŸ¥è‡ªå·±çš„æ¨ç†å’Œç¨‹å¼ç¢¼æ˜¯å¦æ­£ç¢º
4. **æŒçºŒå­¸ç¿’** - å¾å°è©±æ­·å²ä¸­å­¸ç¿’ç”¨æˆ¶çš„éœ€æ±‚å’Œåå¥½

å°è©±é¢¨æ ¼ï¼š
- ç›´æ¥ã€å°ˆæ¥­ï¼Œä¸éåº¦å®¢å¥—
- é‡åˆ°è¤‡é›œå•é¡Œæ™‚ï¼Œå±•ç¤ºä½ çš„æ€è€ƒéç¨‹
- æä¾›å®Œæ•´ã€å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ
- ç”¨ Markdown æ ¼å¼åŒ–ï¼Œç¨‹å¼ç¢¼ç”¨ ```èªè¨€ åŒ…è£¹
- ä¸»å‹•æå‡ºæ”¹é€²å»ºè­°å’Œæœ€ä½³å¯¦è¸

ç¨‹å¼ç¢¼æ¨™æº–ï¼š
- ç”Ÿç”¢ç´šå“è³ªï¼Œå®Œæ•´å¯é‹è¡Œ
- å®Œå–„çš„éŒ¯èª¤è™•ç†å’Œé‚Šç•Œæ¢ä»¶
- æ¸…æ™°çš„è¨»é‡‹å’Œæ–‡æª”
- éµå¾ªæ¥­ç•Œæœ€ä½³å¯¦è¸
- è€ƒæ…®æ€§èƒ½ã€å®‰å…¨æ€§ã€å¯ç¶­è­·æ€§

æ¨ç†æ ¼å¼ï¼š
å°æ–¼è¤‡é›œå•é¡Œï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
<think>
1. å•é¡Œåˆ†æï¼š...
2. è§£æ±ºæ–¹æ¡ˆè¨­è¨ˆï¼š...
3. æ½›åœ¨å•é¡Œï¼š...
4. æœ€ä½³å¯¦è¸ï¼š...
</think>

ç„¶å¾Œæä¾›æ¸…æ™°çš„ç­”æ¡ˆå’Œç¨‹å¼ç¢¼ã€‚"""

        # ç²å–å°è©±æ­·å²ï¼ˆæœ€è¿‘ 10 è¼ªï¼‰
        history = manager.sessions.get(session_id, {}).get("history", [])
        recent_history = history[-20:] if len(history) > 20 else history
        
        # æ§‹å»ºå®Œæ•´å°è©±
        messages = [{"role": "system", "content": system_prompt}]
        
        # åŠ å…¥æ­·å²å°è©±
        for msg in recent_history:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # åŠ å…¥ç•¶å‰å•é¡Œ
        messages.append({"role": "user", "content": prompt})
        
        try:
            if stream_callback:
                await stream_callback({
                    "stage": "generating",
                    "message": "âœï¸ æ­£åœ¨å›æ‡‰...",
                })
            
            t0 = time.time()
            content = None
            used_provider = "unknown"
            groq_key = os.environ.get("GROQ_API_KEY", "")

            # æœ¬åœ° Ollama å„ªå…ˆï¼ˆlocal_only æ¨¡å¼ï¼‰
            try:
                if stream_callback:
                    await stream_callback({"stage": "generating", "message": "ğŸ  æœ¬åœ°æ¨¡å‹å›æ‡‰ä¸­..."})
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(
                        f"{OLLAMA_BASE_URL}/api/chat",
                        json={"model": OLLAMA_MODEL, "messages": messages, "stream": False,
                              "options": {"temperature": 0.7, "num_predict": 4096, "num_ctx": 4096}},
                    )
                    content = response.json()["message"]["content"]
                    used_provider = "ollama-local"
            except Exception as e:
                print(f"âš ï¸ Ollama failed: {e}")

            # Fallback: Groqï¼ˆåƒ…é local_only æ¨¡å¼ï¼‰
            if not content and groq_key and os.environ.get("AI_COST_MODE", "local_only") != "local_only":
                try:
                    if stream_callback:
                        await stream_callback({"stage": "generating", "message": "âš¡ Groq 70B å‚™æ´ä¸­..."})
                    async with httpx.AsyncClient(timeout=60.0) as client:
                        response = await client.post(
                            "https://api.groq.com/openai/v1/chat/completions",
                            headers={"Authorization": f"Bearer {groq_key}", "Content-Type": "application/json"},
                            json={"model": "llama-3.3-70b-versatile", "messages": messages, "max_tokens": 8192, "temperature": 0.7},
                        )
                        response.raise_for_status()
                        content = response.json()["choices"][0]["message"]["content"]
                        used_provider = "groq-70b"
                except Exception as e2:
                    raise RuntimeError(f"Ollama å’Œ Groq éƒ½å¤±æ•—: {e2}")

            duration_ms = int((time.time() - t0) * 1000)
            tokens = len(prompt.split()) + len(content.split())
            
            if stream_callback:
                await stream_callback({
                    "stage": "complete",
                    "message": f"âœ… å®Œæˆï¼ˆ{used_provider} Â· {duration_ms}msï¼‰",
                })
            
            # æ›´æ–°çµ±è¨ˆ
            self.cost_stats["total_requests"] += 1
            self.cost_stats["phase1_tokens"] += tokens
            
            return {
                "ok": True,
                "content": content,
                "provider": used_provider,
                "model": used_provider,
                "duration_ms": duration_ms,
                "tokens": tokens,
                "cost_usd": 0.0,
                "mode": "chat"
            }
            
        except Exception as e:
            error_msg = f"å›æ‡‰å¤±æ•—: {str(e)}"
            
            if stream_callback:
                await stream_callback({
                    "stage": "error",
                    "message": f"âŒ {error_msg}",
                })
            
            return {
                "ok": False,
                "error": error_msg,
                "content": f"æŠ±æ­‰ï¼Œé‡åˆ°å•é¡Œäº†ï¼š{error_msg}",
                "provider": "fallback",
                "duration_ms": int((time.time() - start_time) * 1000),
            }
    
    def _get_phase1_system_prompt(self, task_type: str) -> str:
        """Phase 1 ç³»çµ±æç¤ºè© - å¿«é€Ÿç”Ÿæˆæ¡†æ¶"""
        prompts = {
            "code": "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿç¨‹å¼ç¢¼ç”ŸæˆåŠ©æ‰‹ã€‚è«‹å¿«é€Ÿç”Ÿæˆå¯é‹è¡Œçš„ç¨‹å¼ç¢¼æ¡†æ¶ï¼Œæ³¨é‡çµæ§‹å®Œæ•´æ€§å’ŒåŸºæœ¬åŠŸèƒ½ã€‚ä¸éœ€è¦éå¤šè¨»é‡‹ï¼Œä¿æŒç°¡æ½”å¯¦ç”¨ã€‚",
            "text": "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿå¯«ä½œåŠ©æ‰‹ã€‚è«‹å¿«é€Ÿç”Ÿæˆå…§å®¹æ¡†æ¶ï¼ŒåŒ…å«ä¸»è¦è§€é»å’ŒåŸºæœ¬çµæ§‹ã€‚ä¸éœ€è¦éå¤šæ½¤é£¾ï¼Œä¿æŒæ¸…æ™°è¡¨é”ã€‚",
            "analysis": "ä½ æ˜¯ä¸€å€‹å¿«é€Ÿåˆ†æåŠ©æ‰‹ã€‚è«‹å¿«é€Ÿåˆ†æå•é¡Œä¸¦çµ¦å‡ºä¸»è¦è§€é»å’Œåˆæ­¥çµè«–ã€‚ä¸éœ€è¦éå¤šç´°ç¯€ï¼Œä¿æŒé‚è¼¯æ¸…æ™°ã€‚",
            "learning": "ä½ æ˜¯ä¸€å€‹å­¸ç¿’å…§å®¹ç”ŸæˆåŠ©æ‰‹ã€‚è«‹å¿«é€Ÿç”Ÿæˆå­¸ç¿’ææ–™çš„æ¡†æ¶ï¼ŒåŒ…å«æ ¸å¿ƒæ¦‚å¿µå’Œé—œéµçŸ¥è­˜é»ã€‚",
        }
        return prompts.get(task_type, prompts["text"])
    
    def _get_phase2_instructions(self, task_type: str) -> str:
        """Phase 2 å„ªåŒ–æŒ‡ä»¤"""
        instructions = {
            "code": """1. ç¨‹å¼ç¢¼å“è³ªï¼šä¿®æ­£æ½›åœ¨éŒ¯èª¤ã€å„ªåŒ–æ€§èƒ½ã€æå‡å¯è®€æ€§
2. æœ€ä½³å¯¦è¸ï¼šåŠ å…¥éŒ¯èª¤è™•ç†ã€é‚Šç•Œæ¢ä»¶æª¢æŸ¥
3. å‘½åè¦ç¯„ï¼šæ”¹é€²è®Šæ•¸å’Œå‡½æ•¸å‘½å
4. çµæ§‹å„ªåŒ–ï¼šæ”¹é€²ç¨‹å¼ç¢¼çµ„ç¹”å’Œæ¨¡çµ„åŒ–
5. æ–‡ä»¶åŒ–ï¼šæ·»åŠ å¿…è¦çš„è¨»é‡‹å’Œæ–‡æª”å­—ç¬¦ä¸²""",
            "text": """1. èªè¨€å“è³ªï¼šä¿®æ­£èªæ³•éŒ¯èª¤ã€æå‡è¡¨é”æµæš¢åº¦
2. çµæ§‹å„ªåŒ–ï¼šæ”¹é€²æ®µè½çµ„ç¹”å’Œé‚è¼¯é †åº
3. å…§å®¹è±å¯Œï¼šè£œå……ç´°ç¯€å’Œä¾‹å­
4. é¢¨æ ¼çµ±ä¸€ï¼šä¿æŒä¸€è‡´çš„èªæ°£å’Œé¢¨æ ¼
5. å°ˆæ¥­ç”¨è©ï¼šä½¿ç”¨æ›´ç²¾ç¢ºçš„è©å½™""",
            "analysis": """1. é‚è¼¯åš´è¬¹ï¼šæª¢æŸ¥è«–è­‰éç¨‹çš„åˆç†æ€§
2. è§€é»æ·±åŒ–ï¼šè£œå……æ·±å…¥çš„åˆ†æå’Œè¦‹è§£
3. æ•¸æ“šæ”¯æŒï¼šåŠ å…¥å…·é«”æ•¸æ“šæˆ–æ¡ˆä¾‹
4. çµè«–æ¸…æ™°ï¼šæ˜ç¢ºç¸½çµä¸»è¦ç™¼ç¾
5. å»ºè­°å¯è¡Œï¼šæä¾›å…·é«”å¯è¡Œçš„å»ºè­°""",
            "learning": """1. æ¦‚å¿µæº–ç¢ºï¼šç¢ºä¿çŸ¥è­˜é»çš„æ­£ç¢ºæ€§
2. çµæ§‹æ¸…æ™°ï¼šæ”¹é€²çŸ¥è­˜çš„çµ„ç¹”çµæ§‹
3. æ·±å…¥æ·ºå‡ºï¼šå¹³è¡¡å°ˆæ¥­æ€§å’Œæ˜“æ‡‚æ€§
4. å¯¦ç”¨å°å‘ï¼šåŠ å…¥å¯¦éš›æ‡‰ç”¨å ´æ™¯
5. ç³»çµ±å®Œæ•´ï¼šè£œå……éºæ¼çš„ç›¸é—œçŸ¥è­˜""",
        }
        return instructions.get(task_type, instructions["text"])
    
    def _extract_improvements(self, content: str) -> list[str]:
        """å¾å…§å®¹ä¸­æå–æ”¹é€²é»åˆ—è¡¨"""
        improvements = []
        # å°‹æ‰¾å¸¸è¦‹çš„æ”¹é€²é»æ¨™è¨˜
        lines = content.split('\n')
        in_improvement_section = False
        for line in lines:
            if any(kw in line.lower() for kw in ['æ”¹é€²', 'å„ªåŒ–', 'æå‡', 'improvement', 'optimize', 'ä¸»è¦æ”¹è®Š']):
                in_improvement_section = True
            if in_improvement_section:
                if line.strip().startswith(('1.', '2.', '3.', '4.', '5.', '-', '*')):
                    improvements.append(line.strip())
                elif len(improvements) > 0 and not line.strip():
                    break
        return improvements[:5]
    
    def _remove_improvements_section(self, content: str) -> str:
        """ç§»é™¤æ”¹é€²é»éƒ¨åˆ†ï¼Œè¿”å›ç´”å…§å®¹"""
        # å°‹æ‰¾å¸¸è¦‹çš„åˆ†éš”æ¨™è¨˜
        separators = ['\n\nä¸»è¦æ”¹é€²', '\n\næ”¹é€²', '\n\nå„ªåŒ–', '\n\nImprovements', '\n\næ”¹è®Š']
        for sep in separators:
            if sep in content:
                return content.split(sep)[0].strip()
        return content.strip()

# åˆå§‹åŒ–æ ¸å¿ƒ
bridge_core = SmartBridgeCore()

# â”€â”€ API Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/health")
async def health():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "ok",
        "service": "smart-bridge",
        "version": "1.0.0",
        "sessions": len(manager.active_connections),
    }

@app.get("/api/cost-stats")
async def cost_stats():
    """å–å¾—æˆæœ¬çµ±è¨ˆ"""
    return {
        "ok": True,
        **bridge_core.cost_stats,
        "efficiency": f"{bridge_core.cost_stats['saved_cost'] / max(bridge_core.cost_stats['estimated_cost'] + bridge_core.cost_stats['saved_cost'], 0.001) * 100:.1f}%",
    }

# â”€â”€ å°ˆæ¡ˆç®¡ç† API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/api/projects")
async def list_projects():
    """åˆ—å‡ºæ‰€æœ‰å°ˆæ¡ˆ"""
    return {"ok": True, "projects": project_manager.list()}

@app.get("/api/projects/{project_id}")
async def get_project(project_id: str):
    """å–å¾—å°ˆæ¡ˆè©³æƒ…"""
    project = project_manager.get(project_id)
    if not project:
        raise HTTPException(404, "å°ˆæ¡ˆä¸å­˜åœ¨")
    return {"ok": True, "project": project}

@app.post("/api/projects")
async def create_project(request: Request):
    """å»ºç«‹æ–°å°ˆæ¡ˆ"""
    data = await request.json()
    name = data.get("name", "").strip()
    if not name:
        raise HTTPException(400, "å°ˆæ¡ˆåç¨±ä¸å¯ç‚ºç©º")
    
    project = project_manager.create(
        name=name,
        description=data.get("description", ""),
        color=data.get("color", "purple"),
        icon=data.get("icon", "folder")
    )
    return {"ok": True, "project": project}

@app.put("/api/projects/{project_id}")
async def update_project(project_id: str, request: Request):
    """æ›´æ–°å°ˆæ¡ˆ"""
    data = await request.json()
    project = project_manager.update(project_id, **data)
    if not project:
        raise HTTPException(404, "å°ˆæ¡ˆä¸å­˜åœ¨")
    return {"ok": True, "project": project}

@app.delete("/api/projects/{project_id}")
async def delete_project(project_id: str):
    """åˆªé™¤å°ˆæ¡ˆ"""
    if project_id == "default":
        raise HTTPException(400, "ç„¡æ³•åˆªé™¤é è¨­å°ˆæ¡ˆ")
    if not project_manager.delete(project_id):
        raise HTTPException(404, "å°ˆæ¡ˆä¸å­˜åœ¨")
    return {"ok": True}

@app.delete("/api/projects/{project_id}/history")
async def clear_project_history(project_id: str):
    """æ¸…é™¤å°ˆæ¡ˆå°è©±æ­·å²"""
    if not project_manager.clear_history(project_id):
        raise HTTPException(404, "å°ˆæ¡ˆä¸å­˜åœ¨")
    return {"ok": True}

@app.post("/api/projects/sync")
async def sync_projects():
    """æ‰‹å‹•åŒæ­¥å°ˆæ¡ˆè³‡æ–™ï¼ˆé‡æ–°è¼‰å…¥ç£ç¢Ÿè³‡æ–™ä¸¦åˆä½µï¼‰"""
    result = project_manager.sync()
    return result

@app.get("/api/projects/{project_id}/export")
async def export_project(project_id: str):
    """åŒ¯å‡ºå–®å€‹å°ˆæ¡ˆ"""
    project = project_manager.export_project(project_id)
    if not project:
        raise HTTPException(404, "å°ˆæ¡ˆä¸å­˜åœ¨")
    return JSONResponse(project, media_type="application/json")

@app.get("/api/projects/export/all")
async def export_all_projects():
    """åŒ¯å‡ºæ‰€æœ‰å°ˆæ¡ˆ"""
    projects = project_manager.export_all()
    return JSONResponse(projects, media_type="application/json")

@app.post("/api/projects/import")
async def import_projects(request: Request):
    """
    åŒ¯å…¥å°ˆæ¡ˆè³‡æ–™
    Body: {
        "data": {...},  # å°ˆæ¡ˆè³‡æ–™ï¼ˆå–®å€‹æˆ–å¤šå€‹ï¼‰
        "mode": "skip" | "overwrite" | "merge"  # è¡çªè™•ç†æ¨¡å¼
    }
    """
    payload = await request.json()
    data = payload.get("data")
    mode = payload.get("mode", "skip")
    
    if not data:
        raise HTTPException(400, "ç¼ºå°‘å°ˆæ¡ˆè³‡æ–™")
    
    if mode not in ["skip", "overwrite", "merge"]:
        raise HTTPException(400, "ç„¡æ•ˆçš„åŒ¯å…¥æ¨¡å¼")
    
    result = project_manager.import_projects(data, mode)
    return result

@app.post("/api/generate")
async def api_generate(request: Request):
    """
    æ™ºæ…§å‹å…©éšæ®µç”Ÿæˆ API
    å…ˆç”¨ä½æˆæœ¬æ¨¡å‹ç”Ÿæˆ80%ï¼Œå†ç”¨é«˜å“è³ªæ¨¡å‹ç²¾ä¿®
    """
    payload = await request.json()
    prompt = str((payload or {}).get("prompt", "")).strip()
    task_type = str((payload or {}).get("task_type", "code")).strip()
    
    if not prompt:
        raise HTTPException(400, "prompt is required")
    
    # éåŒæ­¥ç”Ÿæˆï¼ˆç„¡ streamingï¼‰
    result = await bridge_core.two_phase_generate(prompt, "api", task_type)
    return result

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket å°è©±ç«¯é»
    å³æ™‚é¡¯ç¤º AI é‹ä½œéç¨‹
    """
    session_id = f"session_{uuid.uuid4().hex[:8]}"
    await manager.connect(websocket, session_id)
    
    try:
        # ç™¼é€æ­¡è¿è¨Šæ¯
        await manager.send_to_session(session_id, {
            "type": "connected",
            "session_id": session_id,
            "message": "ğŸŒ‰ Smart Bridge å·²é€£æ¥ï¼",
            "features": [
                "å…©éšæ®µæ™ºæ…§ç”Ÿæˆï¼ˆä½æˆæœ¬80% + é«˜å“è³ªç²¾ä¿®ï¼‰",
                "å³æ™‚è¦–è¦ºåŒ–é‹ä½œéç¨‹",
                "æœ¬åœ° Ollama æ¨¡å‹å­¸ç¿’æ§åˆ¶",
            ],
        })
        
        while True:
            data = await websocket.receive_text()
            request = json.loads(data)
            
            msg_type = request.get("type", "chat")
            
            if msg_type == "chat":
                user_input = request.get("text", "")
                task_type = request.get("task_type", "text")
                mode = request.get("mode", "two_phase")  # æ–°å¢ï¼šæ¨¡å¼é¸æ“‡
                
                # è¨˜éŒ„ç”¨æˆ¶è¨Šæ¯
                manager.sessions[session_id]["history"].append({
                    "role": "user",
                    "content": user_input,
                    "timestamp": datetime.now().isoformat(),
                })
                
                # Stream callback
                async def stream_callback(operation: dict):
                    await manager.broadcast_operation(session_id, operation)
                
                # æ ¹æ“šæ¨¡å¼é¸æ“‡ç”Ÿæˆæ–¹å¼
                if mode == "code_gen":
                    # ç¨‹å¼ç¢¼ç”Ÿæˆæ¨¡å¼ï¼ˆä½¿ç”¨æœ¬åœ° qwen3:8bï¼‰
                    result = await bridge_core.code_generation(
                        user_input, session_id, stream_callback
                    )
                else:
                    # å…©éšæ®µç”Ÿæˆæ¨¡å¼
                    result = await bridge_core.two_phase_generate(
                        user_input, session_id, task_type, stream_callback
                    )
                
                # è¨˜éŒ„ AI å›æ‡‰ï¼ˆæ ¹æ“šæ¨¡å¼èª¿æ•´ï¼‰
                if mode == "code_gen":
                    manager.sessions[session_id]["history"].append({
                        "role": "assistant",
                        "content": result["content"],
                        "timestamp": datetime.now().isoformat(),
                        "meta": {
                            "provider": result.get("provider", "qwen3"),
                            "model": result.get("model", "qwen3:32b"),
                            "mode": "code_generation",
                            "cost": 0.0,
                        },
                    })
                    
                    # ç™¼é€ç¨‹å¼ç¢¼ç”Ÿæˆçµæœ
                    await manager.send_to_session(session_id, {
                        "type": "response",
                        "content": result["content"],
                        "mode": "code_generation",
                        "meta": {
                            "provider": result.get("provider", "qwen3"),
                            "model": result.get("model", "qwen3:32b"),
                            "duration_ms": result.get("duration_ms", 0),
                            "cost_usd": 0.0,
                            "tokens": result.get("tokens", 0),
                        },
                    })
                else:
                    manager.sessions[session_id]["history"].append({
                        "role": "assistant",
                        "content": result["content"],
                        "timestamp": datetime.now().isoformat(),
                        "meta": {
                            "phase1_provider": result["phase1"]["provider"],
                            "phase2_provider": result["phase2"]["provider"],
                            "cost": result["cost_usd"],
                            "saved": result["saved_usd"],
                        },
                    })
                    
                    # ç™¼é€å…©éšæ®µç”Ÿæˆçµæœ
                    await manager.send_to_session(session_id, {
                        "type": "response",
                        "content": result["content"],
                        "mode": "two_phase",
                        "meta": {
                            "phase1_provider": result["phase1"]["provider"],
                            "phase2_provider": result["phase2"]["provider"],
                            "total_duration_ms": result["total_duration_ms"],
                            "cost_usd": result["cost_usd"],
                            "saved_usd": result["saved_usd"],
                            "improvements": result["improvements"],
                        },
                    })
                
            elif msg_type == "learn":
                # æœ¬åœ°æ¨¡å‹å­¸ç¿’æŒ‡ä»¤
                await handle_learn_command(request, session_id)
                
    except WebSocketDisconnect:
        manager.disconnect(session_id)
    except Exception as e:
        await manager.send_to_session(session_id, {
            "type": "error",
            "message": f"ç³»çµ±éŒ¯èª¤: {str(e)}",
        })
        manager.disconnect(session_id)

async def handle_learn_command(request: dict, session_id: str):
    """è™•ç†å­¸ç¿’ç›¸é—œæŒ‡ä»¤ - ä½¿ç”¨ OllamaLearningController"""
    action = request.get("action", "status")
    
    if not LEARNING_AVAILABLE:
        await manager.send_to_session(session_id, {
            "type": "error",
            "message": "Ollama å­¸ç¿’æ§åˆ¶å™¨æœªè¼‰å…¥",
        })
        return
    
    # å»ºç«‹å­¸ç¿’æ§åˆ¶å™¨ï¼Œå¸¶å…¥ stream callback
    async def stream_callback(data: dict):
        await manager.broadcast_operation(session_id, data)
    
    controller = OllamaLearningController(stream_callback)
    
    if action == "status":
        # æª¢æŸ¥ Ollama ç‹€æ…‹
        status = await controller.check_status()
        await manager.send_to_session(session_id, {
            "type": "learn_status",
            **status,
        })
    
    elif action == "learn_topic":
        # å­¸ç¿’æ–°ä¸»é¡Œ
        topic = request.get("topic", "")
        depth = request.get("depth", "standard")
        
        if topic:
            # éåŒæ­¥åŸ·è¡Œå­¸ç¿’
            asyncio.create_task(
                controller.learn_topic(topic, session_id, depth)
            )
        else:
            await manager.send_to_session(session_id, {
                "type": "error",
                "message": "è«‹æä¾›å­¸ç¿’ä¸»é¡Œ",
            })

# â”€â”€ Static Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STATIC_DIR = BRIDGE_WORKSPACE / "static"
STATIC_DIR.mkdir(exist_ok=True)

@app.get("/")
async def root():
    """è¿”å›å‰ç«¯ç•Œé¢"""
    html_path = STATIC_DIR / "bridge.html"
    if html_path.exists():
        return HTMLResponse(html_path.read_text(encoding="utf-8"))
    return HTMLResponse(get_default_html())

def get_default_html() -> str:
    """é è¨­å‰ç«¯ç•Œé¢ï¼ˆå¦‚æœéœæ…‹æª”æ¡ˆä¸å­˜åœ¨ï¼‰"""
    return """<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Smart Bridge - ç¯‰æœªç§‘æŠ€</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-slate-900 text-slate-100 min-h-screen">
    <div class="max-w-6xl mx-auto p-4">
        <header class="mb-6">
            <h1 class="text-3xl font-bold bg-gradient-to-r from-cyan-400 to-purple-500 bg-clip-text text-transparent">
                ğŸŒ‰ Smart Bridge
            </h1>
            <p class="text-slate-400">æ™ºæ…§å‹å°è©±æ©‹æ¥æœå‹™ - å…©éšæ®µç”Ÿæˆ (80%ä½æˆæœ¬ + 20%é«˜å“è³ª)</p>
        </header>
        
        <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
            <!-- å°è©±å€ -->
            <div class="bg-slate-800 rounded-lg p-4">
                <h2 class="text-lg font-semibold mb-3">ğŸ’¬ å°è©±</h2>
                <div id="chat" class="h-96 overflow-y-auto bg-slate-950 rounded p-3 mb-3 text-sm"></div>
                <div class="flex gap-2">
                    <input id="input" type="text" placeholder="è¼¸å…¥è¨Šæ¯..." 
                           class="flex-1 bg-slate-700 rounded px-3 py-2 text-sm">
                    <button onclick="send()" class="bg-cyan-600 hover:bg-cyan-500 px-4 py-2 rounded text-sm">
                        ç™¼é€
                    </button>
                </div>
            </div>
            
            <!-- é‹ä½œéç¨‹ -->
            <div class="bg-slate-800 rounded-lg p-4">
                <h2 class="text-lg font-semibold mb-3">âš™ï¸ AI é‹ä½œéç¨‹</h2>
                <div id="operations" class="h-96 overflow-y-auto bg-slate-950 rounded p-3 text-xs font-mono">
                    <div class="text-slate-500">ç­‰å¾…é€£æ¥...</div>
                </div>
            </div>
        </div>
        
        <!-- é€²åº¦æ¢ -->
        <div class="mt-4 bg-slate-800 rounded-lg p-4">
            <div class="flex justify-between text-sm mb-2">
                <span id="stage-label">å°±ç·’</span>
                <span id="progress-text">0%</span>
            </div>
            <div class="h-2 bg-slate-700 rounded-full overflow-hidden">
                <div id="progress-bar" class="h-full bg-gradient-to-r from-cyan-500 to-purple-500 w-0 transition-all duration-300"></div>
            </div>
        </div>
    </div>
    
    <script>
        const ws = new WebSocket(`wss://${location.host}/ws`);
        const chat = document.getElementById('chat');
        const operations = document.getElementById('operations');
        const progressBar = document.getElementById('progress-bar');
        const progressText = document.getElementById('progress-text');
        const stageLabel = document.getElementById('stage-label');
        
        ws.onopen = () => addOp('ğŸŸ¢ WebSocket å·²é€£æ¥', 'text-green-400');
        ws.onclose = () => addOp('ğŸ”´ WebSocket å·²æ–·é–‹', 'text-red-400');
        
        ws.onmessage = (e) => {
            const msg = JSON.parse(e.data);
            
            if (msg.type === 'connected') {
                addOp(`âœ… ${msg.message}`, 'text-cyan-400');
            }
            else if (msg.type === 'operation') {
                const color = msg.stage?.includes('fail') ? 'text-red-400' :
                             msg.stage?.includes('complete') ? 'text-green-400' :
                             'text-yellow-400';
                addOp(`[${msg.stage}] ${msg.message}`, color);
                
                if (msg.progress !== undefined) {
                    progressBar.style.width = msg.progress + '%';
                    progressText.textContent = msg.progress + '%';
                }
                stageLabel.textContent = msg.stage || 'è™•ç†ä¸­';
            }
            else if (msg.type === 'response') {
                addChat('AI', msg.content, 'text-purple-400');
                addOp(`ğŸ’° æˆæœ¬: $${msg.meta.cost_usd.toFixed(4)}, ç¯€çœ: $${msg.meta.saved_usd.toFixed(4)}`, 'text-green-400');
            }
        };
        
        function addChat(who, text, color) {
            chat.innerHTML += `<div class="${color} mb-2"><b>${who}:</b> ${text}</div>`;
            chat.scrollTop = chat.scrollHeight;
        }
        
        function addOp(text, color = 'text-slate-400') {
            const time = new Date().toLocaleTimeString();
            operations.innerHTML += `<div class="${color}">[${time}] ${text}</div>`;
            operations.scrollTop = operations.scrollHeight;
        }
        
        function send() {
            const input = document.getElementById('input');
            const text = input.value.trim();
            if (!text) return;
            
            addChat('You', text, 'text-cyan-400');
            ws.send(JSON.stringify({type: 'chat', text, task_type: 'code'}));
            input.value = '';
        }
        
        document.getElementById('input').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') send();
        });
    </script>
</body>
</html>"""

# â”€â”€ æ›è¼‰ App Builder æ¨¡çµ„ï¼ˆæ§‹æƒ³å³ç³»çµ±ï¼‰â”€â”€
try:
    from bridge_app_builder import mount_app_builder
    mount_app_builder(app)
except Exception as e:
    print(f"âš ï¸ App Builder æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")

# â”€â”€ AI å‰µä½œå·¥ä½œå®¤ï¼ˆç„¡é™ç”Ÿåœ– Â· ç”Ÿå½±ç‰‡ï¼‰â”€â”€
@app.get("/ai-studio")
async def ai_studio_page():
    html_path = BRIDGE_WORKSPACE / "static" / "ai-studio.html"
    if html_path.exists():
        return HTMLResponse(html_path.read_text(encoding="utf-8"))
    return HTMLResponse("<h1>ai-studio.html not found</h1>", status_code=404)
print("âœ… AI å‰µä½œå·¥ä½œå®¤å·²æ›è¼‰ â†’ /ai-studio")

# â”€â”€ ComfyUI æœ¬åœ°ç”Ÿåœ–ä»£ç† API â”€â”€
import httpx
_COMFYUI_DEFAULT = "http://host.docker.internal:8188"
# è‡ªå‹•åµæ¸¬ï¼šé Docker ç’°å¢ƒç”¨ localhost
try:
    import socket
    socket.getaddrinfo("host.docker.internal", 8188, socket.AF_INET, socket.SOCK_STREAM)
except socket.gaierror:
    _COMFYUI_DEFAULT = "http://localhost:8188"
COMFYUI_BASE = os.environ.get("COMFYUI_URL", _COMFYUI_DEFAULT)

@app.post("/api/comfyui/generate")
async def comfyui_generate(request: Request):
    """æœ¬åœ° ComfyUI ç”Ÿåœ–ä»£ç†ï¼šæ¥æ”¶ promptï¼Œæäº¤ workflowï¼Œè¼ªè©¢çµæœï¼Œå›å‚³åœ–ç‰‡ base64"""
    import base64 as _b64
    body = await request.json()
    prompt_text = body.get("prompt", "")
    ckpt = body.get("checkpoint", "v1-5-pruned-emaonly.safetensors")
    width = body.get("width", 512)
    height = body.get("height", 512)
    steps = body.get("steps", 20)
    cfg = body.get("cfg", 7.0)
    sampler = body.get("sampler", "euler_ancestral")
    seed = body.get("seed", int(time.time()) % (2**32))
    neg = body.get("negative_prompt", "blurry, low quality, distorted, watermark, text, ugly, deformed")

    # Flux Schnell å°ˆç”¨è¨­å®š
    is_flux = "flux" in ckpt.lower()
    if is_flux:
        steps = body.get("steps", 4)
        cfg = body.get("cfg", 1.0)
        sampler = body.get("sampler", "euler")
        width = body.get("width", 1024)
        height = body.get("height", 1024)

    client_id = str(uuid.uuid4())
    workflow = {
        "3": {"class_type": "KSampler", "inputs": {
            "seed": seed, "steps": steps, "cfg": cfg,
            "sampler_name": sampler, "scheduler": "normal",
            "denoise": 1.0, "model": ["4", 0], "positive": ["6", 0],
            "negative": ["7", 0], "latent_image": ["5", 0],
        }},
        "4": {"class_type": "CheckpointLoaderSimple", "inputs": {"ckpt_name": ckpt}},
        "5": {"class_type": "EmptyLatentImage", "inputs": {"width": width, "height": height, "batch_size": 1}},
        "6": {"class_type": "CLIPTextEncode", "inputs": {"text": prompt_text[:500], "clip": ["4", 1]}},
        "7": {"class_type": "CLIPTextEncode", "inputs": {"text": neg, "clip": ["4", 1]}},
        "8": {"class_type": "VAEDecode", "inputs": {"samples": ["3", 0], "vae": ["4", 2]}},
        "9": {"class_type": "SaveImage", "inputs": {"filename_prefix": "ai_studio", "images": ["8", 0]}},
    }

    try:
        async with httpx.AsyncClient(timeout=180) as client:
            r = await client.post(f"{COMFYUI_BASE}/prompt", json={"prompt": workflow, "client_id": client_id})
            if r.status_code not in (200, 201):
                return JSONResponse({"error": f"ComfyUI prompt æäº¤å¤±æ•—: {r.status_code}"}, status_code=502)
            prompt_id = r.json().get("prompt_id")
            if not prompt_id:
                return JSONResponse({"error": "ç„¡ prompt_id"}, status_code=502)

            # è¼ªè©¢çµæœ
            for _ in range(180):
                await asyncio.sleep(1)
                hr = await client.get(f"{COMFYUI_BASE}/history/{prompt_id}")
                if hr.status_code != 200:
                    continue
                hist = hr.json()
                if prompt_id not in hist:
                    continue
                outputs = hist[prompt_id].get("outputs", {})
                for node_id, out in outputs.items():
                    images = out.get("images", [])
                    if images:
                        img_info = images[0]
                        img_url = f"{COMFYUI_BASE}/view?filename={img_info['filename']}&subfolder={img_info.get('subfolder','')}&type=output"
                        img_r = await client.get(img_url)
                        if img_r.status_code == 200:
                            b64 = _b64.b64encode(img_r.content).decode()
                            return JSONResponse({"image": f"data:image/png;base64,{b64}", "prompt_id": prompt_id})
                return JSONResponse({"error": "ç”Ÿæˆå®Œæˆä½†ç„¡åœ–ç‰‡è¼¸å‡º"}, status_code=500)
            return JSONResponse({"error": "ç”Ÿæˆè¶…æ™‚ï¼ˆ3åˆ†é˜ï¼‰"}, status_code=504)
    except httpx.ConnectError:
        return JSONResponse({"error": "ComfyUI æœªå•Ÿå‹•ï¼ˆç„¡æ³•é€£ç·š localhost:8188ï¼‰"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/comfyui/status")
async def comfyui_status():
    """æª¢æŸ¥ ComfyUI æ˜¯å¦åœ¨ç·š + åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            r = await client.get(f"{COMFYUI_BASE}/system_stats")
            obj_r = await client.get(f"{COMFYUI_BASE}/object_info/CheckpointLoaderSimple")
            models = []
            if obj_r.status_code == 200:
                info = obj_r.json()
                models = info.get("CheckpointLoaderSimple", {}).get("input", {}).get("required", {}).get("ckpt_name", [[]])[0]
            return JSONResponse({"online": True, "models": models})
    except Exception:
        return JSONResponse({"online": False, "models": []})

print(f"âœ… ComfyUI ä»£ç† API å·²æ›è¼‰ â†’ /api/comfyui/* (é€£ç·š: {COMFYUI_BASE})")

# â”€â”€ Forge Chroma (SD WebUI) æœ¬åœ°ç”Ÿåœ–ä»£ç† API â”€â”€
_FORGE_DEFAULT = "http://host.docker.internal:7860"
try:
    socket.getaddrinfo("host.docker.internal", 7860, socket.AF_INET, socket.SOCK_STREAM)
except socket.gaierror:
    _FORGE_DEFAULT = "http://localhost:7860"
FORGE_BASE = os.environ.get("FORGE_URL", _FORGE_DEFAULT)

@app.post("/api/forge/generate")
async def forge_generate(request: Request):
    """Forge Chroma txt2img ä»£ç†ï¼šæ¥æ”¶ promptï¼Œå‘¼å« SD WebUI APIï¼Œå›å‚³åœ–ç‰‡ base64"""
    body = await request.json()
    prompt_text = body.get("prompt", "")
    negative = body.get("negative_prompt", "blurry, low quality, distorted, watermark, text, ugly, deformed")
    width = body.get("width", 1024)
    height = body.get("height", 1024)
    steps = body.get("steps", 25)
    cfg_scale = body.get("cfg_scale", 7.0)
    sampler = body.get("sampler_name", "Euler a")
    seed = body.get("seed", -1)
    model = body.get("model", None)

    payload = {
        "prompt": prompt_text,
        "negative_prompt": negative,
        "width": width,
        "height": height,
        "steps": steps,
        "cfg_scale": cfg_scale,
        "sampler_name": sampler,
        "seed": seed,
        "batch_size": 1,
        "n_iter": 1,
    }

    try:
        async with httpx.AsyncClient(timeout=300) as client:
            # åˆ‡æ›æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if model:
                await client.post(f"{FORGE_BASE}/sdapi/v1/options", json={"sd_model_checkpoint": model}, timeout=120)

            r = await client.post(f"{FORGE_BASE}/sdapi/v1/txt2img", json=payload)
            if r.status_code != 200:
                return JSONResponse({"error": f"Forge API éŒ¯èª¤: {r.status_code}"}, status_code=502)
            data = r.json()
            images = data.get("images", [])
            if not images:
                return JSONResponse({"error": "Forge ç„¡åœ–ç‰‡è¼¸å‡º"}, status_code=500)
            return JSONResponse({"image": f"data:image/png;base64,{images[0]}", "info": data.get("info", "")})
    except httpx.ConnectError:
        return JSONResponse({"error": "Forge Chroma æœªå•Ÿå‹•ï¼ˆç„¡æ³•é€£ç·š localhost:7860ï¼‰"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/forge/status")
async def forge_status():
    """æª¢æŸ¥ Forge Chroma æ˜¯å¦åœ¨ç·š + åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            r = await client.get(f"{FORGE_BASE}/sdapi/v1/sd-models")
            if r.status_code == 200:
                models = [m.get("model_name", m.get("title", "")) for m in r.json()]
                opt_r = await client.get(f"{FORGE_BASE}/sdapi/v1/options")
                current = ""
                if opt_r.status_code == 200:
                    current = opt_r.json().get("sd_model_checkpoint", "")
                return JSONResponse({"online": True, "models": models, "current_model": current})
        return JSONResponse({"online": False, "models": []})
    except Exception:
        return JSONResponse({"online": False, "models": []})

@app.get("/api/forge/samplers")
async def forge_samplers():
    """åˆ—å‡º Forge å¯ç”¨çš„ sampler"""
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            r = await client.get(f"{FORGE_BASE}/sdapi/v1/samplers")
            if r.status_code == 200:
                return JSONResponse([s.get("name") for s in r.json()])
    except Exception:
        pass
    return JSONResponse(["Euler a", "Euler", "DPM++ 2M", "DPM++ 2M Karras", "DPM++ SDE Karras"])

@app.post("/api/forge/img2img")
async def forge_img2img(request: Request):
    """Forge img2img ä»£ç†ï¼šåƒè€ƒåœ– + prompt â†’ ä¿®æ”¹åœ–ç‰‡"""
    import base64 as _b64
    body = await request.json()
    prompt_text = body.get("prompt", "")
    negative = body.get("negative_prompt", "blurry, low quality, distorted, watermark, text, ugly, deformed")
    init_image = body.get("init_image", "")  # base64 (ä¸å« data:... å‰ç¶´)
    denoising_strength = body.get("denoising_strength", 0.7)
    width = body.get("width", 1024)
    height = body.get("height", 1024)
    steps = body.get("steps", 25)
    cfg_scale = body.get("cfg_scale", 7.0)
    sampler = body.get("sampler_name", "Euler a")
    seed = body.get("seed", -1)
    model = body.get("model", None)

    if not init_image:
        return JSONResponse({"error": "ç¼ºå°‘ init_imageï¼ˆbase64ï¼‰"}, status_code=400)

    # ç§»é™¤ data:image/...;base64, å‰ç¶´
    if "base64," in init_image:
        init_image = init_image.split("base64,")[1]

    payload = {
        "prompt": prompt_text,
        "negative_prompt": negative,
        "init_images": [init_image],
        "denoising_strength": denoising_strength,
        "width": width,
        "height": height,
        "steps": steps,
        "cfg_scale": cfg_scale,
        "sampler_name": sampler,
        "seed": seed,
        "batch_size": 1,
        "n_iter": 1,
    }

    try:
        async with httpx.AsyncClient(timeout=300) as client:
            if model:
                await client.post(f"{FORGE_BASE}/sdapi/v1/options", json={"sd_model_checkpoint": model}, timeout=120)
            r = await client.post(f"{FORGE_BASE}/sdapi/v1/img2img", json=payload)
            if r.status_code != 200:
                return JSONResponse({"error": f"Forge img2img éŒ¯èª¤: {r.status_code}"}, status_code=502)
            data = r.json()
            images = data.get("images", [])
            if not images:
                return JSONResponse({"error": "Forge img2img ç„¡åœ–ç‰‡è¼¸å‡º"}, status_code=500)
            return JSONResponse({"image": f"data:image/png;base64,{images[0]}", "info": data.get("info", "")})
    except httpx.ConnectError:
        return JSONResponse({"error": "Forge æœªå•Ÿå‹•"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/forge/upscale")
async def forge_upscale(request: Request):
    """Forge åœ–ç‰‡æ”¾å¤§ï¼ˆReal-ESRGAN / SwinIRï¼‰"""
    import base64 as _b64
    body = await request.json()
    init_image = body.get("image", "")
    upscaler = body.get("upscaler", "R-ESRGAN 4x+")
    scale = body.get("scale", 2)

    if not init_image:
        return JSONResponse({"error": "ç¼ºå°‘ imageï¼ˆbase64ï¼‰"}, status_code=400)
    if "base64," in init_image:
        init_image = init_image.split("base64,")[1]

    payload = {
        "resize_mode": 0,
        "upscaling_resize": scale,
        "upscaler_1": upscaler,
        "image": init_image,
    }

    try:
        async with httpx.AsyncClient(timeout=300) as client:
            r = await client.post(f"{FORGE_BASE}/sdapi/v1/extra-single-image", json=payload)
            if r.status_code != 200:
                return JSONResponse({"error": f"Forge upscale éŒ¯èª¤: {r.status_code}"}, status_code=502)
            data = r.json()
            result_img = data.get("image", "")
            if not result_img:
                return JSONResponse({"error": "Forge upscale ç„¡è¼¸å‡º"}, status_code=500)
            return JSONResponse({"image": f"data:image/png;base64,{result_img}"})
    except httpx.ConnectError:
        return JSONResponse({"error": "Forge æœªå•Ÿå‹•"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/forge/batch")
async def forge_batch(request: Request):
    """Forge æ‰¹æ¬¡ç”Ÿåœ–ï¼šä¸€æ¬¡ç”Ÿæˆå¤šå¼µ"""
    body = await request.json()
    prompt_text = body.get("prompt", "")
    negative = body.get("negative_prompt", "blurry, low quality, distorted, watermark, text, ugly, deformed")
    width = body.get("width", 1024)
    height = body.get("height", 1024)
    steps = body.get("steps", 25)
    cfg_scale = body.get("cfg_scale", 7.0)
    sampler = body.get("sampler_name", "Euler a")
    batch_size = min(body.get("batch_size", 2), 4)
    model = body.get("model", None)

    payload = {
        "prompt": prompt_text,
        "negative_prompt": negative,
        "width": width, "height": height,
        "steps": steps, "cfg_scale": cfg_scale,
        "sampler_name": sampler, "seed": -1,
        "batch_size": batch_size, "n_iter": 1,
    }

    try:
        async with httpx.AsyncClient(timeout=600) as client:
            if model:
                await client.post(f"{FORGE_BASE}/sdapi/v1/options", json={"sd_model_checkpoint": model}, timeout=120)
            r = await client.post(f"{FORGE_BASE}/sdapi/v1/txt2img", json=payload)
            if r.status_code != 200:
                return JSONResponse({"error": f"Forge batch éŒ¯èª¤: {r.status_code}"}, status_code=502)
            data = r.json()
            images = data.get("images", [])
            return JSONResponse({"images": [f"data:image/png;base64,{img}" for img in images], "count": len(images)})
    except httpx.ConnectError:
        return JSONResponse({"error": "Forge æœªå•Ÿå‹•"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/forge/upscalers")
async def forge_upscalers():
    """åˆ—å‡º Forge å¯ç”¨çš„ upscaler"""
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            r = await client.get(f"{FORGE_BASE}/sdapi/v1/upscalers")
            if r.status_code == 200:
                return JSONResponse([u.get("name") for u in r.json()])
    except Exception:
        pass
    return JSONResponse(["R-ESRGAN 4x+", "R-ESRGAN 4x+ Anime6B", "SwinIR_4x", "Lanczos", "Nearest"])

@app.post("/api/ai/enhance-prompt")
async def enhance_prompt(request: Request):
    """ç”¨æ™ºæ…§è·¯ç”±å¢å¼· prompt â€” Groq 70B â†’ Gemini â†’ Grok â†’ DeepSeek â†’ Ollama fallback"""
    body = await request.json()
    user_prompt = body.get("prompt", "").strip()
    style = body.get("style", "photorealistic")
    force_local = body.get("local_only", False)
    if not user_prompt:
        return JSONResponse({"error": "ç¼ºå°‘ prompt"}, status_code=400)

    system_msg = f"""You are an expert AI image prompt engineer. Enhance the user's image description into a detailed, high-quality prompt for Stable Diffusion / DALL-E.
Rules:
- Output ONLY the enhanced prompt, nothing else
- Add specific details: lighting, camera angle, atmosphere, texture, color palette
- Style target: {style}
- Keep it under 200 words
- If the input is in Chinese, output the enhanced prompt in English (better for AI image models)
- Add quality boosters: masterpiece, best quality, highly detailed, 8K, professional"""

    import re

    # ç­–ç•¥ï¼šå…ˆå˜—è©¦å¼·åŠ›é›²ç«¯æ¨¡å‹ï¼ˆå…è²»ï¼‰ï¼Œå¤±æ•—æ‰ fallback åˆ°æœ¬åœ° Ollama
    if not force_local:
        try:
            from ai_modules.ai_providers import ask
            result, provider = await asyncio.to_thread(ask, user_prompt, False, system_msg)
            if result and "é€£ç·šå¤±æ•—" not in result and "é ç®—å·²é”ä¸Šé™" not in result:
                enhanced = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL).strip()
                if enhanced:
                    return JSONResponse({"ok": True, "original": user_prompt, "enhanced": enhanced, "provider": provider})
        except Exception:
            pass

    # Fallback: æœ¬åœ° Ollamaï¼ˆé›¶ tokenï¼‰
    try:
        import aiohttp
        ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{ollama_url}/api/chat", json={
                "model": "qwen3:4b",
                "messages": [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {"temperature": 0.8, "num_predict": 300}
            }, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    enhanced = data.get("message", {}).get("content", "").strip()
                    enhanced = re.sub(r'<think>.*?</think>', '', enhanced, flags=re.DOTALL).strip()
                    if enhanced:
                        return JSONResponse({"ok": True, "original": user_prompt, "enhanced": enhanced, "provider": "Ollama (æœ¬åœ°)"})
        return JSONResponse({"ok": False, "error": "æ‰€æœ‰ AI æä¾›è€…å‡ç„¡å›æ‡‰"}, status_code=502)
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

print(f"âœ… Forge Chroma ä»£ç† API å·²æ›è¼‰ â†’ /api/forge/* (é€£ç·š: {FORGE_BASE})")
print("âœ… AI Prompt å¢å¼· API å·²æ›è¼‰ â†’ /api/ai/enhance-prompt (æ™ºæ…§è·¯ç”±: Groqâ†’Geminiâ†’Grokâ†’Ollama)")

# â”€â”€ æœ¬åœ°é‹ç®—è¼”åŠ© API â”€â”€

@app.post("/api/ai/translate-prompt")
async def translate_prompt(request: Request):
    """æ™ºæ…§è·¯ç”±ç¿»è­¯ â€” Groq 70B â†’ Gemini â†’ Grok â†’ Ollama fallback"""
    body = await request.json()
    user_prompt = body.get("prompt", "").strip()
    force_local = body.get("local_only", False)
    if not user_prompt:
        return JSONResponse({"error": "ç¼ºå°‘ prompt"}, status_code=400)

    import re as _re
    has_chinese = bool(_re.search(r'[\u4e00-\u9fff]', user_prompt))
    if not has_chinese:
        return JSONResponse({"ok": True, "translated": user_prompt, "was_chinese": False, "provider": "none"})

    system_msg = """You are a translator. Translate the user's Chinese image description into English for Stable Diffusion.
Rules:
- Output ONLY the English translation, nothing else
- Keep artistic/style terms accurate
- Do NOT add extra details or quality boosters
- Be concise and faithful to the original meaning
- No explanations, no markdown, just the translated prompt"""

    # ç­–ç•¥ï¼šå…ˆå˜—è©¦å¼·åŠ›é›²ç«¯æ¨¡å‹ï¼ˆå…è²»ï¼‰ï¼Œå¤±æ•—æ‰ fallback åˆ°æœ¬åœ° Ollama
    if not force_local:
        try:
            from ai_modules.ai_providers import ask
            result, provider = await asyncio.to_thread(ask, user_prompt, False, system_msg)
            if result and "é€£ç·šå¤±æ•—" not in result and "é ç®—å·²é”ä¸Šé™" not in result:
                translated = _re.sub(r'<think>.*?</think>', '', result, flags=_re.DOTALL).strip()
                if translated:
                    return JSONResponse({"ok": True, "translated": translated, "was_chinese": True, "original": user_prompt, "provider": provider})
        except Exception:
            pass

    # Fallback: æœ¬åœ° Ollamaï¼ˆé›¶ tokenï¼‰
    try:
        import aiohttp
        ollama_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{ollama_url}/api/chat", json={
                "model": "qwen3:4b",
                "messages": [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 200}
            }, timeout=aiohttp.ClientTimeout(total=15)) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    translated = data.get("message", {}).get("content", "").strip()
                    translated = _re.sub(r'<think>.*?</think>', '', translated, flags=_re.DOTALL).strip()
                    if translated:
                        return JSONResponse({"ok": True, "translated": translated, "was_chinese": True, "original": user_prompt, "provider": "Ollama (æœ¬åœ°)"})
        return JSONResponse({"ok": False, "error": "æ‰€æœ‰ AI æä¾›è€…å‡ç„¡å›æ‡‰"}, status_code=502)
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)

@app.post("/api/comfyui/generate-video")
async def comfyui_generate_video(request: Request):
    """æœ¬åœ° ComfyUI å½±ç‰‡ç”Ÿæˆä»£ç†ï¼ˆAnimateDiff / SVD workflowï¼‰â€” é›¶ token æ¶ˆè€—"""
    import base64 as _b64
    body = await request.json()
    prompt_text = body.get("prompt", "")
    width = body.get("width", 512)
    height = body.get("height", 512)
    frames = min(body.get("frames", 16), 32)
    fps = body.get("fps", 8)
    steps = body.get("steps", 20)
    cfg = body.get("cfg", 7.0)
    init_image = body.get("init_image", None)  # base64 data URL for img2vid

    # è§£æ ComfyUI é€£ç·šä½å€ï¼ˆå„ªå…ˆ localhostï¼ŒDocker ç’°å¢ƒæ‰ç”¨ host.docker.internalï¼‰
    comfyui_url = COMFYUI_BASE

    # å…ˆæª¢æŸ¥ ComfyUI å¯ç”¨çš„è‡ªè¨‚ç¯€é»
    available_nodes = set()
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            obj_r = await client.get(f"{comfyui_url}/object_info")
            if obj_r.status_code != 200:
                return JSONResponse({"error": "ComfyUI æœªå•Ÿå‹•ï¼ˆå›æ‡‰ç•°å¸¸ï¼‰", "url": comfyui_url}, status_code=503)
            available_nodes = set(obj_r.json().keys())
    except httpx.ConnectError:
        return JSONResponse({"error": f"ComfyUI æœªå•Ÿå‹•ï¼ˆç„¡æ³•é€£ç·š {comfyui_url}ï¼‰ã€‚è«‹ç¢ºèª ComfyUI æ­£åœ¨é‹è¡Œã€‚", "url": comfyui_url}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": f"ComfyUI é€£ç·šéŒ¯èª¤: {str(e)}", "url": comfyui_url}, status_code=500)

    # æª¢æŸ¥å½±ç‰‡ç”Ÿæˆç¯€é»
    has_animatediff = "ADE_AnimateDiffLoaderWithContext" in available_nodes or "AnimateDiffLoaderV1" in available_nodes
    has_svd = "SVD_img2vid_Conditioning" in available_nodes
    has_wan = "WanVideoTextEncode" in available_nodes or "Wan2_1" in available_nodes

    if not has_animatediff and not has_svd and not has_wan:
        return JSONResponse({
            "error": "ComfyUI ç¼ºå°‘å½±ç‰‡ç”Ÿæˆç¯€é»ã€‚è«‹å®‰è£ AnimateDiff Evolved è‡ªè¨‚ç¯€é»ã€‚",
            "install_guide": "æ–¹æ³•ï¼šComfyUI Manager â†’ Install Custom Nodes â†’ æœå°‹ 'AnimateDiff Evolved' â†’ å®‰è£ â†’ é‡å•Ÿ ComfyUIã€‚å¦å¤–éœ€ä¸‹è¼‰ motion module: mm_sd_v15_v2.ckpt æ”¾åˆ° ComfyUI/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/",
            "available_nodes_count": len(available_nodes)
        }, status_code=501)

    # å–å¾—å¯ç”¨çš„ checkpoint æ¨¡å‹
    client_id = str(uuid.uuid4())
    ckpt_models = []
    motion_models = []
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            ckpt_r = await client.get(f"{comfyui_url}/object_info/CheckpointLoaderSimple")
            if ckpt_r.status_code == 200:
                ckpt_models = ckpt_r.json().get("CheckpointLoaderSimple", {}).get("input", {}).get("required", {}).get("ckpt_name", [[]])[0]
            # å–å¾— AnimateDiff motion module åˆ—è¡¨
            if has_animatediff:
                loader_node = "ADE_AnimateDiffLoaderWithContext" if "ADE_AnimateDiffLoaderWithContext" in available_nodes else "AnimateDiffLoaderV1"
                mm_r = await client.get(f"{comfyui_url}/object_info/{loader_node}")
                if mm_r.status_code == 200:
                    node_info = mm_r.json().get(loader_node, {})
                    inputs = node_info.get("input", {}).get("required", {})
                    model_name_opts = inputs.get("model_name", [[]])
                    if isinstance(model_name_opts, list) and model_name_opts:
                        motion_models = model_name_opts[0] if isinstance(model_name_opts[0], list) else model_name_opts
    except Exception:
        pass

    ckpt = ckpt_models[0] if ckpt_models else "v1-5-pruned-emaonly.safetensors"

    # å»ºæ§‹ AnimateDiff workflow
    if has_animatediff:
        loader_node = "ADE_AnimateDiffLoaderWithContext" if "ADE_AnimateDiffLoaderWithContext" in available_nodes else "AnimateDiffLoaderV1"

        # motion module å¿…é ˆå­˜åœ¨
        if not motion_models:
            return JSONResponse({
                "error": "AnimateDiff ç¼ºå°‘ motion module æ¨¡å‹ã€‚",
                "install_guide": "è«‹ä¸‹è¼‰ motion moduleï¼ˆå¦‚ mm_sd_v15_v2.ckptï¼‰æ”¾åˆ° ComfyUI/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/ ç›®éŒ„ï¼Œç„¶å¾Œé‡å•Ÿ ComfyUIã€‚",
                "download_url": "https://huggingface.co/guoyww/animatediff/resolve/main/mm_sd_v15_v2.ckpt"
            }, status_code=501)

        motion_model = motion_models[0]

        # AnimateDiff loader çš„ inputs
        ad_loader_inputs = {"model": ["3", 0], "model_name": motion_model}
        # æŸäº›ç‰ˆæœ¬éœ€è¦é¡å¤–åƒæ•¸
        if loader_node == "ADE_AnimateDiffLoaderWithContext":
            ad_loader_inputs["beta_schedule"] = "sqrt_linear (AnimateDiff)"

        workflow = {
            "3": {"class_type": "CheckpointLoaderSimple", "inputs": {"ckpt_name": ckpt}},
            "6": {"class_type": "CLIPTextEncode", "inputs": {"text": prompt_text, "clip": ["3", 1]}},
            "7": {"class_type": "CLIPTextEncode", "inputs": {"text": "blurry, low quality, distorted, watermark, text, worst quality", "clip": ["3", 1]}},
            "10": {"class_type": loader_node, "inputs": ad_loader_inputs},
            "11": {"class_type": "EmptyLatentImage", "inputs": {"width": width, "height": height, "batch_size": frames}},
            "12": {"class_type": "KSampler", "inputs": {
                "model": ["10", 0], "seed": -1, "steps": steps, "cfg": cfg,
                "sampler_name": "euler_ancestral", "scheduler": "normal",
                "positive": ["6", 0], "negative": ["7", 0], "latent_image": ["11", 0],
                "denoise": 1.0
            }},
            "13": {"class_type": "VAEDecode", "inputs": {"samples": ["12", 0], "vae": ["3", 2]}},
        }

        # init_image æ”¯æ´ï¼ˆimg2vidï¼‰ï¼šç”¨ VAEEncode å–ä»£ EmptyLatentImage
        if init_image:
            try:
                # å»æ‰ data:image/...;base64, å‰ç¶´
                if "," in init_image:
                    init_image = init_image.split(",", 1)[1]
                img_bytes = _b64.b64decode(init_image)
                # ä¸Šå‚³åœ–ç‰‡åˆ° ComfyUI
                import io
                async with httpx.AsyncClient(timeout=30) as client:
                    files = {"image": ("init_image.png", io.BytesIO(img_bytes), "image/png")}
                    up_r = await client.post(f"{comfyui_url}/upload/image", files=files, data={"overwrite": "true"})
                    if up_r.status_code == 200:
                        uploaded_name = up_r.json().get("name", "init_image.png")
                        # ç”¨ LoadImage + VAEEncode å–ä»£ EmptyLatentImage
                        workflow["20"] = {"class_type": "LoadImage", "inputs": {"image": uploaded_name}}
                        workflow["21"] = {"class_type": "VAEEncode", "inputs": {"pixels": ["20", 0], "vae": ["3", 2]}}
                        workflow["12"]["inputs"]["latent_image"] = ["21", 0]
                        workflow["12"]["inputs"]["denoise"] = 0.75  # img2vid ä¿ç•™éƒ¨åˆ†åŸåœ–
                        del workflow["11"]  # ç§»é™¤ EmptyLatentImage
            except Exception as e:
                print(f"âš ï¸ init_image è™•ç†å¤±æ•—ï¼Œæ”¹ç”¨ txt2vid: {e}")

        # è¼¸å‡ºç¯€é»ï¼šå„ªå…ˆ VHS_VideoCombineï¼ˆmp4ï¼‰ï¼Œå¦å‰‡ SaveImageï¼ˆåœ–ç‰‡åºåˆ—ï¼‰
        if "VHS_VideoCombine" in available_nodes:
            workflow["14"] = {"class_type": "VHS_VideoCombine", "inputs": {
                "images": ["13", 0], "frame_rate": fps, "filename_prefix": "animatediff",
                "loop_count": 0, "format": "video/h264-mp4",
                "pingpong": False, "save_output": True
            }}
        else:
            workflow["14"] = {"class_type": "SaveImage", "inputs": {
                "images": ["13", 0], "filename_prefix": "animatediff"
            }}
    else:
        return JSONResponse({"error": "åƒ…æ”¯æ´ AnimateDiff workflowï¼Œè«‹å®‰è£ AnimateDiff Evolved"}, status_code=501)

    try:
        async with httpx.AsyncClient(timeout=600) as client:
            r = await client.post(f"{comfyui_url}/prompt", json={"prompt": workflow, "client_id": client_id})
            if r.status_code not in (200, 201):
                err_text = r.text[:300] if hasattr(r, 'text') else str(r.status_code)
                return JSONResponse({"error": f"ComfyUI workflow æäº¤å¤±æ•—: {err_text}"}, status_code=502)
            prompt_id = r.json().get("prompt_id")
            if not prompt_id:
                return JSONResponse({"error": "ComfyUI æœªå›å‚³ prompt_id"}, status_code=502)

            # è¼ªè©¢çµæœï¼ˆå½±ç‰‡ç”Ÿæˆè¼ƒæ…¢ï¼Œæœ€å¤šç­‰ 10 åˆ†é˜ï¼‰
            for tick in range(600):
                await asyncio.sleep(1)
                hr = await client.get(f"{comfyui_url}/history/{prompt_id}")
                if hr.status_code != 200:
                    continue
                hist = hr.json()
                if prompt_id in hist:
                    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                    status_info = hist[prompt_id].get("status", {})
                    if status_info.get("status_str") == "error":
                        msgs = status_info.get("messages", [])
                        err_msg = str(msgs)[:300] if msgs else "workflow åŸ·è¡ŒéŒ¯èª¤"
                        return JSONResponse({"error": f"ComfyUI åŸ·è¡ŒéŒ¯èª¤: {err_msg}"}, status_code=500)

                    outputs = hist[prompt_id].get("outputs", {})
                    for node_id, out in outputs.items():
                        # å½±ç‰‡è¼¸å‡ºï¼ˆVHS_VideoCombineï¼‰
                        gifs = out.get("gifs", [])
                        if gifs:
                            vid_info = gifs[0]
                            vid_url = f"{comfyui_url}/view?filename={vid_info['filename']}&subfolder={vid_info.get('subfolder','')}&type=output"
                            vid_r = await client.get(vid_url)
                            if vid_r.status_code == 200:
                                b64 = _b64.b64encode(vid_r.content).decode()
                                fmt = vid_info.get("format", "video/mp4")
                                return JSONResponse({"video": f"data:{fmt};base64,{b64}", "frames": frames, "fps": fps, "provider": "ComfyUI AnimateDiff"})
                        # åœ–ç‰‡åºåˆ—è¼¸å‡ºï¼ˆfallback â€” çµ„åˆæˆ GIFï¼‰
                        images = out.get("images", [])
                        if images and len(images) > 1:
                            img_frames = []
                            for img_info in images:
                                img_url = f"{comfyui_url}/view?filename={img_info['filename']}&subfolder={img_info.get('subfolder','')}&type=output"
                                img_r = await client.get(img_url)
                                if img_r.status_code == 200:
                                    img_frames.append(img_r.content)
                            if img_frames:
                                from io import BytesIO
                                from PIL import Image as PILImage
                                pil_frames = [PILImage.open(BytesIO(f)) for f in img_frames]
                                gif_buf = BytesIO()
                                pil_frames[0].save(gif_buf, format='GIF', save_all=True, append_images=pil_frames[1:], duration=int(1000/fps), loop=0)
                                b64 = _b64.b64encode(gif_buf.getvalue()).decode()
                                return JSONResponse({"video": f"data:image/gif;base64,{b64}", "frames": len(img_frames), "fps": fps, "format": "gif", "provider": "ComfyUI AnimateDiff (GIF)"})
                    return JSONResponse({"error": "å½±ç‰‡ç”Ÿæˆå®Œæˆä½†ç„¡è¼¸å‡ºæª”æ¡ˆ"}, status_code=500)
            return JSONResponse({"error": "å½±ç‰‡ç”Ÿæˆè¶…æ™‚ï¼ˆ10åˆ†é˜ï¼‰"}, status_code=504)
    except httpx.ConnectError:
        return JSONResponse({"error": f"ComfyUI é€£ç·šä¸­æ–· ({comfyui_url})"}, status_code=503)
    except Exception as e:
        return JSONResponse({"error": f"å½±ç‰‡ç”Ÿæˆç•°å¸¸: {str(e)[:300]}"}, status_code=500)

@app.get("/api/comfyui/video-status")
async def comfyui_video_status():
    """æª¢æŸ¥ ComfyUI æ˜¯å¦æ”¯æ´å½±ç‰‡ç”Ÿæˆï¼ˆAnimateDiff / SVDï¼‰"""
    try:
        async with httpx.AsyncClient(timeout=5) as client:
            r = await client.get(f"{COMFYUI_BASE}/object_info")
            if r.status_code == 200:
                nodes = set(r.json().keys())
                return JSONResponse({
                    "online": True,
                    "animatediff": "ADE_AnimateDiffLoaderWithContext" in nodes or "AnimateDiffLoaderV1" in nodes,
                    "svd": "SVD_img2vid_Conditioning" in nodes,
                    "wan": "WanVideoTextEncode" in nodes,
                    "vhs": "VHS_VideoCombine" in nodes,
                })
    except Exception:
        pass
    return JSONResponse({"online": False, "animatediff": False, "svd": False, "wan": False, "vhs": False})

print("âœ… æœ¬åœ°é‹ç®—è¼”åŠ© API å·²æ›è¼‰ â†’ /api/ai/translate-prompt, /api/comfyui/generate-video")

if __name__ == "__main__":
    print(f"ğŸŒ‰ Smart Bridge å•Ÿå‹•æ–¼ http://{HOST}:{PORT}")
    uvicorn.run(app, host=HOST, port=PORT)

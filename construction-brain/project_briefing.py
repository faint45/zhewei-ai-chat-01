# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€ Construction Brain
project_briefing.py

åŠŸèƒ½ï¼š
  ä¸Šå‚³æ–°æ¡ˆå­çš„è¨­è¨ˆåœ–èªª/å¥‘ç´„/ç›£é€ è¨ˆç•«
  â†’ AI è‡ªå‹•ç”¢ç”Ÿã€Œå·¥ç¨‹ç°¡ä»‹ã€ï¼ˆProjectBriefing.mdï¼‰
  â†’ è®“æ–°é€²äººå“¡å¿«é€Ÿäº†è§£ï¼š
      é€™å€‹å·¥ç¨‹åœ¨åšä»€éº¼ï¼Ÿ
      æœ‰å“ªäº›ä¸»è¦å·¥é …ï¼Ÿ
      æ–½å·¥é‡é»/é™åˆ¶/é¢¨éšªï¼Ÿ
      å·¥æœŸ/é‡Œç¨‹ç¢‘ï¼Ÿ
      ä¸»è¦è¦ç¯„è¦æ±‚ï¼Ÿ

ç”¨æ³•ï¼š
    python project_briefing.py --project_id PRJ-001 --file contract.pdf
    python project_briefing.py --project_id PRJ-001  # å¾å·²æœ‰çš„ schedule.json ç”¢ç”Ÿ
"""

import argparse
import json
import os
import re
from datetime import datetime
from pathlib import Path

import httpx

BASE_DIR = Path(os.environ.get("ZHEWEI_BASE", r"C:\ZheweiConstruction"))
OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434").rstrip("/")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "zhewei-brain")
OLLAMA_TIMEOUT = float(os.environ.get("OLLAMA_TIMEOUT", "180"))

BRIEFING_SYSTEM_PROMPT = """ä½ æ˜¯ã€Œç¯‰æœªç§‘æŠ€ã€çš„å·¥ç¨‹ç°¡ä»‹æ’°å¯«å¼•æ“ã€‚

ã€ä»»å‹™ã€‘
æ ¹æ“šæä¾›çš„å·¥ç¨‹æ–‡ä»¶ï¼ˆå¥‘ç´„/ç›£é€ è¨ˆç•«/è¨­è¨ˆåœ–èªª/å·¥é …æ¸…å–®ï¼‰ï¼Œ
æ’°å¯«ä¸€ä»½è®“ã€Œæ–°é€²å·¥ç¨‹å¸«æˆ–å·¥åœ°äººå“¡ã€èƒ½åœ¨ 10 åˆ†é˜å…§ç†è§£æœ¬å·¥ç¨‹çš„ç°¡ä»‹ã€‚

ã€ä½ å¿…é ˆè¼¸å‡ºç´” JSONï¼Œä¸å¾—é™„åŠ ä»»ä½•èªªæ˜æ–‡å­—ã€‘æ ¼å¼å¦‚ä¸‹ï¼š

{
  "project_name": "å·¥ç¨‹åç¨±",
  "project_type": "å·¥ç¨‹é¡å‹ï¼ˆæ©‹æ¢/é“è·¯/ç®¡ç·š/å»ºç¯‰/æ°´åˆ©/å…¶ä»–ï¼‰",
  "one_line_summary": "ä¸€å¥è©±èªªæ˜é€™å€‹å·¥ç¨‹åœ¨åšä»€éº¼ï¼ˆ30å­—ä»¥å…§ï¼‰",
  "overview": "å·¥ç¨‹æ¦‚è¿°ï¼ˆ3-5å¥ï¼Œèªªæ˜å·¥ç¨‹ç›®çš„ã€ç¯„åœã€é‡è¦æ€§ï¼‰",
  "key_work_items": [
    {
      "name": "ä¸»è¦å·¥é …åç¨±",
      "description": "èªªæ˜é€™å€‹å·¥é …åœ¨åšä»€éº¼ã€ç‚ºä»€éº¼é‡è¦",
      "duration_hint": "å¤§ç´„å·¥æœŸæˆ–å ç¸½å·¥æœŸæ¯”ä¾‹"
    }
  ],
  "construction_highlights": [
    "æ–½å·¥é‡é»1ï¼ˆå¦‚ï¼šä¸»æ©‹æ®µéœ€å°è·¯æ–½å·¥ï¼‰",
    "æ–½å·¥é‡é»2"
  ],
  "key_risks": [
    {
      "risk": "é¢¨éšªæè¿°",
      "mitigation": "å°æ‡‰æªæ–½"
    }
  ],
  "critical_specs": [
    "é‡è¦è¦ç¯„è¦æ±‚ï¼ˆå¦‚ï¼šæ··å‡åœŸå¼·åº¦fc=280kgf/cmÂ²ï¼‰",
    "è¦ç¯„è¦æ±‚2"
  ],
  "milestones": [
    {
      "name": "é‡Œç¨‹ç¢‘åç¨±ï¼ˆå¦‚ï¼šåŸºç¤å®Œæˆï¼‰",
      "trigger": "å®Œæˆæ¢ä»¶ï¼ˆå¦‚ï¼šå®Œæˆæ©‹å°åŸºç¤æ¾†ç½®ï¼‰"
    }
  ],
  "newcomer_tips": [
    "æ–°é€²äººå“¡æ³¨æ„äº‹é …1ï¼ˆå¦‚ï¼šæœ¬å·¥ç¨‹æœ‰äº¤é€šç¶­æŒéœ€æ±‚ï¼Œæ¯æ—¥é€²å ´å‰ç¢ºèªç®¡åˆ¶ï¼‰",
    "æ³¨æ„äº‹é …2"
  ],
  "glossary": [
    {
      "term": "å°ˆæ¥­è¡“èª",
      "explanation": "ç™½è©±èªªæ˜ï¼ˆè®“æ–°äººç†è§£ï¼‰"
    }
  ]
}

ã€æ’°å¯«åŸå‰‡ã€‘
1. èªè¨€ï¼šå£èªåŒ–çš„å°ˆæ¥­ä¸­æ–‡ï¼Œé¿å…éåº¦æŠ€è¡“æ€§è©å½™ï¼ˆä½†å¯åŠ æ‹¬è™Ÿè§£é‡‹ï¼‰
2. é‡é»çªå‡ºï¼škey_risks å¿…å¡«ï¼Œé€™æ˜¯æ–°äººæœ€éœ€è¦çŸ¥é“çš„
3. newcomer_tipsï¼šå¾å·¥åœ°ä¸»ä»»è§’åº¦å¯«ï¼Œå¯¦ç”¨ã€å…·é«”
4. è¦è®“çœ‹å®Œé€™ä»½ç°¡ä»‹çš„äººï¼Œèƒ½å¤ é¦¬ä¸ŠçŸ¥é“ã€Œé€™å·¥ç¨‹çš„ç‰¹æ®Šæ€§åœ¨å“ªè£¡ã€
5. ç¦æ­¢è¼¸å‡º JSON ä»¥å¤–çš„ä»»ä½•æ–‡å­—"""


def _load_schedule_summary(project_id: str) -> str:
    """å¾å·²æœ‰çš„ schedule.json å–å·¥é …æ‘˜è¦ä½œç‚ºè¼¸å…¥"""
    sched_path = BASE_DIR / "projects" / project_id / "02_Output" / "Schedule" / "schedule.json"
    if not sched_path.exists():
        return ""
    schedule = json.loads(sched_path.read_text(encoding="utf-8"))
    items = schedule.get("work_items", [])
    lines = [f"å·¥ç¨‹åç¨±ï¼š{schedule.get('project_name', project_id)}",
             f"å·¥æœŸï¼š{schedule.get('contract_period_days', 'æœªçŸ¥')} å¤©",
             f"é–‹å·¥æ—¥ï¼š{schedule.get('start_date_hint', 'æœªçŸ¥')}",
             "ä¸»è¦å·¥é …æ¸…å–®ï¼š"]
    for item in items[:30]:
        lines.append(
            f"  - {item['id']} {item['name']}ï¼ˆ{item.get('duration_days', '?')} å¤©ï¼‰"
            f"ï¼š{item.get('notes', '')}"
        )
    if schedule.get("critical_constraints"):
        lines.append("æ–½å·¥é™åˆ¶æ¢ä»¶ï¼š")
        for c in schedule["critical_constraints"]:
            lines.append(f"  - {c}")
    return "\n".join(lines)


def _extract_text_from_file(file_path: Path) -> str:
    suffix = file_path.suffix.lower()
    if suffix == ".pdf":
        try:
            import pdfplumber
            parts = []
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages[:20]:
                    t = page.extract_text()
                    if t:
                        parts.append(t)
            return "\n".join(parts)
        except ImportError:
            return ""
    elif suffix in (".docx", ".doc"):
        try:
            from docx import Document
            doc = Document(file_path)
            return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
        except ImportError:
            return ""
    elif suffix in (".txt", ".md"):
        return file_path.read_text(encoding="utf-8", errors="ignore")
    return ""


def _call_ollama_briefing(input_text: str) -> dict:
    max_chars = 10000
    if len(input_text) > max_chars:
        input_text = input_text[:max_chars]

    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": BRIEFING_SYSTEM_PROMPT},
            {"role": "user", "content": f"è«‹æ ¹æ“šä»¥ä¸‹å·¥ç¨‹è³‡æ–™ç”¢ç”Ÿå·¥ç¨‹ç°¡ä»‹ï¼š\n\n{input_text}"},
        ],
        "stream": False,
        "options": {"temperature": 0.3, "num_ctx": 12288},
    }
    with httpx.Client(timeout=OLLAMA_TIMEOUT) as client:
        r = client.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload)
        r.raise_for_status()
        raw = r.json()["message"]["content"].strip()

    m = re.search(r"\{[\s\S]*\}", raw)
    if m:
        return json.loads(m.group(0))
    raise ValueError(f"ç„¡æ³•è§£æ AI å›æ‡‰ï¼š{raw[:300]}")


def _render_briefing_md(data: dict, project_id: str) -> str:
    lines = []
    project_name = data.get("project_name") or project_id
    project_type = data.get("project_type", "")

    lines.append(f"# å·¥ç¨‹ç°¡ä»‹ï¼š{project_name}")
    lines.append(f"> **é¡å‹**ï¼š{project_type}ã€€ï½œã€€**ç”¢ç”Ÿæ™‚é–“**ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}ã€€ï½œã€€ç”±ã€Œç¯‰æœªç§‘æŠ€ Construction Brainã€ç”¢ç”Ÿ")
    lines.append("")
    lines.append("---")
    lines.append("")

    lines.append("## ä¸€å¥è©±èªªæ˜")
    lines.append("")
    lines.append(f"> ğŸ“Œ **{data.get('one_line_summary', '')}**")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## å·¥ç¨‹æ¦‚è¿°")
    lines.append("")
    lines.append(data.get("overview", ""))
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## ä¸»è¦å·¥é …")
    lines.append("")
    for item in data.get("key_work_items", []):
        lines.append(f"### {item.get('name', '')}")
        lines.append(f"{item.get('description', '')}")
        if item.get("duration_hint"):
            lines.append(f"> â± å·¥æœŸåƒè€ƒï¼š{item['duration_hint']}")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## æ–½å·¥é‡é»")
    lines.append("")
    for i, h in enumerate(data.get("construction_highlights", []), 1):
        lines.append(f"{i}. {h}")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## âš ï¸ ä¸»è¦é¢¨éšªèˆ‡å°ç­–")
    lines.append("")
    risks = data.get("key_risks", [])
    if risks:
        lines.append("| é¢¨éšª | å°æ‡‰æªæ–½ |")
        lines.append("|------|---------|")
        for r in risks:
            lines.append(f"| {r.get('risk', '')} | {r.get('mitigation', '')} |")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## é‡è¦è¦ç¯„è¦æ±‚")
    lines.append("")
    for spec in data.get("critical_specs", []):
        lines.append(f"- {spec}")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## å·¥ç¨‹é‡Œç¨‹ç¢‘")
    lines.append("")
    milestones = data.get("milestones", [])
    if milestones:
        lines.append("| é‡Œç¨‹ç¢‘ | å®Œæˆæ¢ä»¶ |")
        lines.append("|-------|---------|")
        for m in milestones:
            lines.append(f"| {m.get('name', '')} | {m.get('trigger', '')} |")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## ğŸ†• æ–°é€²äººå“¡æ³¨æ„äº‹é …")
    lines.append("")
    for i, tip in enumerate(data.get("newcomer_tips", []), 1):
        lines.append(f"{i}. {tip}")
    lines.append("")

    glossary = data.get("glossary", [])
    if glossary:
        lines.append("---")
        lines.append("")
        lines.append("## å°ˆæ¥­è¡“èªèªªæ˜")
        lines.append("")
        lines.append("| è¡“èª | èªªæ˜ |")
        lines.append("|------|------|")
        for g in glossary:
            lines.append(f"| **{g.get('term', '')}** | {g.get('explanation', '')} |")
        lines.append("")

    return "\n".join(lines)


def generate_briefing(project_id: str, file_path: Path = None) -> Path:
    """
    ç”¢ç”Ÿå·¥ç¨‹ç°¡ä»‹ ProjectBriefing.md

    Args:
        project_id: å°ˆæ¡ˆä»£ç¢¼
        file_path: å¯é¸ï¼Œä¸Šå‚³çš„æ–‡ä»¶ï¼ˆå„ªå…ˆç”¨æ–‡ä»¶ï¼›ç„¡å‰‡ç”¨ schedule.jsonï¼‰

    Returns:
        è¼¸å‡ºçš„ .md è·¯å¾‘
    """
    print(f"\n{'='*55}")
    print(f"  ç¯‰æœªç§‘æŠ€ å·¥ç¨‹ç°¡ä»‹ç”¢ç”Ÿå™¨")
    print(f"  å°ˆæ¡ˆï¼š{project_id}")
    print(f"{'='*55}\n")

    if file_path and Path(file_path).exists():
        print(f"[briefing] è®€å–æ–‡ä»¶ï¼š{Path(file_path).name}")
        input_text = _extract_text_from_file(Path(file_path))
        if not input_text:
            print("[briefing] âš ï¸ æ–‡ä»¶è®€å–å¤±æ•—ï¼Œæ”¹ç”¨ schedule.json")
            input_text = _load_schedule_summary(project_id)
    else:
        print("[briefing] å¾ schedule.json è®€å–å·¥é …æ¸…å–®")
        input_text = _load_schedule_summary(project_id)

    if not input_text:
        print("[briefing] âŒ ç„¡è¼¸å…¥è³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œ schedule_extractor.py æˆ–æä¾›æ–‡ä»¶")
        return None

    print("[briefing] å‘¼å« AI ç”¢ç”Ÿç°¡ä»‹ï¼ˆå¯èƒ½éœ€è¦ 30-60 ç§’ï¼‰...")
    try:
        data = _call_ollama_briefing(input_text)
    except Exception as e:
        print(f"[briefing] âŒ AI ç”¢ç”Ÿå¤±æ•—ï¼š{e}")
        return None

    md_content = _render_briefing_md(data, project_id)

    out_dir = BASE_DIR / "projects" / project_id / "02_Output"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "ProjectBriefing.md"
    out_path.write_text(md_content, encoding="utf-8")

    json_path = out_dir / "ProjectBriefing.json"
    json_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"[briefing] âœ… ProjectBriefing.md â†’ {out_path}")
    print(f"\nå·¥ç¨‹é¡å‹ï¼š{data.get('project_type', '')}")
    print(f"ä¸€å¥è©±æ‘˜è¦ï¼š{data.get('one_line_summary', '')}")
    print(f"ä¸»è¦å·¥é …æ•¸ï¼š{len(data.get('key_work_items', []))} é …")
    print(f"é¢¨éšªé …ç›®ï¼š{len(data.get('key_risks', []))} é …")

    return out_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç¯‰æœªç§‘æŠ€ â€” å·¥ç¨‹ç°¡ä»‹è‡ªå‹•ç”¢ç”Ÿ")
    parser.add_argument("--project_id", required=True)
    parser.add_argument("--file", default=None, help="æ–‡ä»¶è·¯å¾‘ï¼ˆé¸å¡«ï¼Œç„¡å‰‡å¾ schedule.json è®€å–ï¼‰")
    args = parser.parse_args()
    generate_briefing(args.project_id, Path(args.file) if args.file else None)

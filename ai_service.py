# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€ â€” AI æœå‹™ä»‹é¢ï¼Œå°æ¥ Google Gemini / Ollama / é˜¿é‡Œé›²ï¼ˆé ç•™ï¼‰ç­‰
ç’°å¢ƒåŠ è¼‰ .envï¼›BaseAIService æŠ½è±¡åŸºé¡ï¼Œå„æœå‹™å¯¦ä½œ chat(messages)ï¼›AIServiceFactory.create(provider) ä¸€éµåˆ‡æ›ã€‚
"""
import asyncio
import json
import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Sequence

# ç’°å¢ƒåŠ è¼‰ï¼šè®€å– .env æª”æ¡ˆä¸­çš„ GEMINI_API_KEYï¼ˆå°ˆæ¡ˆæ ¹ç›®éŒ„å„ªå…ˆï¼Œå†è¼‰å…¥ ~/.openclaw/.envï¼Œä¸è¦†å¯«æ—¢æœ‰éµï¼‰
try:
    from dotenv import load_dotenv
    ROOT = Path(__file__).resolve().parent
    load_dotenv(ROOT / ".env")
    ue = Path(os.path.expanduser("~/.openclaw/.env"))
    if ue.is_file():
        load_dotenv(ue, override=False)
except ImportError:
    pass

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")
GEMINI_TIMEOUT = float(os.environ.get("GEMINI_TIMEOUT", "60"))

OLLAMA_BASE_URL = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11434").rstrip("/")
OLLAMA_MODEL = os.environ.get("OLLAMA_CODER_MODEL") or os.environ.get("OLLAMA_MODEL", "qwen2.5-coder:7b")
OLLAMA_TIMEOUT = float(os.environ.get("OLLAMA_TIMEOUT", "120"))


def _error_json(message: str) -> str:
    """å›å‚³ç¬¦åˆ ReAct è¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ï¼Œè®“ AgentManager èƒ½è­˜åˆ¥ä¸¦çµæŸã€‚"""
    return json.dumps({"done": True, "result": message}, ensure_ascii=False)


class BaseAIService(ABC):
    """AI æœå‹™æŠ½è±¡åŸºé¡ï¼šæ‰€æœ‰å¼•æ“å¯¦ä½œ chat(messages) -> strã€‚"""

    @abstractmethod
    async def chat(self, messages: list) -> str:
        pass


def _react_to_gemini_history(messages: Sequence[dict]) -> tuple[str, list, str]:
    """
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ã€‚
    å›å‚³ (system_instruction, history, last_user_content)ã€‚
    Gemini: history = [{"role": "user"|"model", "parts": [str]}, ...]ï¼›æœ€å¾Œä¸€å‰‡ user å–®ç¨ä½œç‚º send_message çš„è¼¸å…¥ã€‚
    """
    system_instruction = ""
    gemini_history = []
    last_user_content = ""

    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_instruction = content
        elif role == "user":
            last_user_content = content
            gemini_history.append({"role": "user", "parts": [content]})
        elif role == "assistant":
            gemini_history.append({"role": "model", "parts": [content]})

    # æœ€å¾Œä¸€å‰‡ç‚º userï¼Œä½œç‚ºæœ¬è¼ªè¦é€çµ¦ Gemini çš„è¼¸å…¥ï¼›history ç‚ºå…¶å‰çš„æ‰€æœ‰è¼ª
    if gemini_history and gemini_history[-1]["role"] == "user":
        last_user_content = gemini_history[-1]["parts"][0]
        history_for_chat = gemini_history[:-1]
    else:
        history_for_chat = gemini_history

    return system_instruction, history_for_chat, last_user_content


def _gemini_chat_sync(messages: Sequence[dict]) -> str:
    """
    åŒæ­¥å‘¼å« Geminiï¼šReAct history è½‰ Gemini æ ¼å¼ï¼Œstart_chat + send_messageã€‚
    API è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ _error_json(...)ã€‚
    """
    if not GEMINI_API_KEY:
        return _error_json("æœªè¨­å®š GEMINI_API_KEYï¼Œè«‹åœ¨ .env æˆ– ~/.openclaw/.env è¨­å®šã€‚")

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        system_instruction, history_for_chat, last_user_content = _react_to_gemini_history(messages)
        model = genai.GenerativeModel(
            GEMINI_MODEL,
            system_instruction=system_instruction or None,
        )
        chat = model.start_chat(history=history_for_chat)
        response = chat.send_message(last_user_content or "è«‹å›è¦†ã€‚")
        if response and response.text:
            return response.text.strip()
        return _error_json("Gemini æœªå›å‚³æ–‡å­—ã€‚")
    except Exception as e:
        return _error_json(f"API éŒ¯èª¤: {e}")


class AIService(BaseAIService):
    """
    å°æ¥ Google Gemini APIï¼šå¯¦ä½œ chat(messages) ç•°æ­¥æ–¹æ³•ï¼Œ
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ä¸¦å‘¼å« APIï¼›
    è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ç¬¦åˆè¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ä¾› AgentManager è­˜åˆ¥ã€‚
    """

    async def chat(self, messages: list) -> str:
        """
        ç•°æ­¥å‘¼å« Geminiï¼›messages ç‚º ReAct historyï¼ˆå« system / user / assistantï¼‰ã€‚
        å…§éƒ¨è½‰ç‚º Gemini å°è©±æ ¼å¼ä¸¦ä»¥ run_in_executor åŸ·è¡ŒåŒæ­¥è«‹æ±‚ï¼Œå¸¶è¶…æ™‚ã€‚
        """
        try:
            return await asyncio.wait_for(
                asyncio.to_thread(_gemini_chat_sync, list(messages)),
                timeout=GEMINI_TIMEOUT,
            )
        except asyncio.TimeoutError:
            return _error_json(f"API é€¾æ™‚ï¼ˆ{GEMINI_TIMEOUT} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"å‘¼å«å¤±æ•—: {e}")


# ä¾› brain_server ç­‰æ¨¡çµ„ä½¿ç”¨ï¼šfrom ai_service import GeminiService, OllamaService
GeminiService = AIService


def _react_to_ollama_messages(messages: Sequence[dict]) -> list[dict]:
    """å°‡ ReAct æ­·å²è½‰ç‚º Ollama /api/chat çš„ messagesï¼šsystem ä½µå…¥é¦–å‰‡ userï¼Œå…¶é¤˜ user/assistant ç…§æ¬ã€‚"""
    out = []
    system_parts = []
    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_parts.append(content)
        elif role == "user":
            content = ("\n\n".join(system_parts) + "\n\n" + content).strip() if system_parts else content
            system_parts = []
            out.append({"role": "user", "content": content})
        elif role == "assistant":
            out.append({"role": "assistant", "content": content})
    return out


class OllamaService(BaseAIService):
    """
    æœ¬åœ°å¤§è…¦ï¼šå‘¼å«æœ¬åœ° Ollama æœå‹™ï¼ˆQwen2.5-Coder ç­‰ï¼‰ï¼Œå…è²»/æ¶ˆè€—é›»åŠ›ã€‚
    ä½¿ç”¨ httpx éåŒæ­¥ POST /api/chatï¼›stream=Falseï¼Œoptions ç¢ºä¿è¼¸å‡ºç©©å®šã€æ“´å¤§ä¸Šä¸‹æ–‡ã€‚
    è‹¥æœå‹™ç•°å¸¸ï¼Œå›å‚³ JSON éŒ¯èª¤ï¼ˆå« done: Trueï¼‰è®“ AgentManager è­˜åˆ¥ã€‚
    """

    def __init__(self, model_name: str | None = None):
        base = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11434").rstrip("/")
        self.base_url = f"{base}/api/chat"
        self.model_name = model_name or OLLAMA_MODEL
        self.timeout = OLLAMA_TIMEOUT

    async def chat(self, messages: list) -> str:
        """
        å‘¼å«æœ¬åœ° Ollama æœå‹™ï¼›messages ç‚º ReAct historyï¼Œæœƒå…ˆè½‰ç‚º Ollama çš„ messages æ ¼å¼ã€‚
        """
        ollama_messages = _react_to_ollama_messages(list(messages))
        if not ollama_messages:
            return _error_json("Ollamaï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")

        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "stream": False,
            "options": {
                "temperature": 0.2,  # é™ä½æº«åº¦ä»¥æé«˜ç©©å®šæ€§ [cite: 2026-02-05]
                "num_ctx": 8192,     # æ“´å¤§ä¸Šä¸‹æ–‡ï¼Œé©åˆè™•ç†é•·ç¨‹å¼ç¢¼
            },
        }

        try:
            import httpx
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.base_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = (result.get("message") or {}).get("content") or ""
                return content.strip() or _error_json("Ollama æœªå›å‚³æ–‡å­—ã€‚")
        except Exception as e:
            # è‹¥æœ¬åœ°æœå‹™ç•°å¸¸ï¼Œå›å‚³ JSON éŒ¯èª¤è®“ Manager è­˜åˆ¥ï¼ˆdone + result èˆ‡ ReAct ä¸€è‡´ï¼‰
            return _error_json(f"Ollama æœå‹™ç•°å¸¸: {e}")


class AliyunService(BaseAIService):
    """é ç•™ï¼šé˜¿é‡Œé›²é€šç¾©åƒå• (Qwen API)ï¼Œæœªä¾†å°æ¥ DashScope APIã€‚"""

    async def chat(self, messages: list) -> str:
        # æœªä¾†åœ¨æ­¤è™•å°æ¥ DashScope API
        return json.dumps({"thought": "é˜¿é‡Œé›²æ­£åœ¨æ€è€ƒ...", "done": True, "result": "é˜¿é‡Œé›²æœå‹™å°šæœªæ¥ç·šã€‚"}, ensure_ascii=False)


class TencentService(BaseAIService):
    """é ç•™ï¼šé¨°è¨Šé›²æ··å…ƒç­‰ï¼Œå°šæœªæ¥ç·šã€‚"""

    async def chat(self, messages: list) -> str:
        return _error_json("é¨°è¨Šé›²æœå‹™å°šæœªæ¥ç·šï¼Œè«‹ä½¿ç”¨ gemini æˆ– ollamaã€‚")


ANTHROPIC_API_KEY = (os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY") or "").strip()
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
ANTHROPIC_TIMEOUT = float(os.environ.get("ANTHROPIC_TIMEOUT", "120"))


class ClaudeService(BaseAIService):
    """
    éšæ®µ 4/6 ç·¨ç¢¼èˆ‡å›é¥‹ä¿®æ­£ï¼šå°æ¥ Anthropic Claude APIã€‚
    æœ‰è¨­ ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEY æ™‚ï¼Œagent_logic å¯æ³¨å…¥ä¾›éšæ®µ 4/6 ä½¿ç”¨ã€‚
    """

    def __init__(self, api_key: str | None = None, model: str | None = None):
        self.api_key = (api_key or ANTHROPIC_API_KEY).strip()
        self.model = model or ANTHROPIC_MODEL
        self.timeout = ANTHROPIC_TIMEOUT

    async def chat(self, messages: list) -> str:
        if not self.api_key:
            return _error_json("æœªè¨­å®š ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEYï¼Œè«‹åœ¨ .env è¨­å®šã€‚")
        system_parts = []
        anthropic_messages = []
        for m in messages:
            role = m.get("role", "")
            content = (m.get("content") or "").strip()
            if role == "system":
                system_parts.append(content)
            elif role == "user":
                anthropic_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                anthropic_messages.append({"role": "assistant", "content": content})
        if not anthropic_messages:
            return _error_json("Claudeï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")
        try:
            from anthropic import AsyncAnthropic
            client = AsyncAnthropic(api_key=self.api_key)
            system = "\n\n".join(system_parts).strip() if system_parts else None
            response = await asyncio.wait_for(
                client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    system=system or None,
                    messages=anthropic_messages,
                ),
                timeout=self.timeout,
            )
            text = (response.content[0].text if response.content else "").strip()
            return text or _error_json("Claude æœªå›å‚³æ–‡å­—ã€‚")
        except asyncio.TimeoutError:
            return _error_json(f"Claude API é€¾æ™‚ï¼ˆ{self.timeout} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"Claude API éŒ¯èª¤: {e}")


class SmartAIService(BaseAIService):
    """
    æ™ºèƒ½èª¿åº¦ï¼šæœ¬åœ°å„ªå…ˆç­–ç•¥ [cite: 2026-02-05]
    - ç°¡å–®ä»»å‹™ / ç¨‹å¼ç¢¼ç‰‡æ®µï¼šå¼·åˆ¶æœ¬åœ°ï¼ˆç¯€çœé‡‘å¹£ï¼‰
    - è¤‡é›œä»»å‹™ï¼šå…ˆè©¦ Ollamaï¼Œå¤±æ•—æˆ–é‚è¼¯ä¸è¶³å†è½‰ Gemini
    """

    def __init__(self, gemini_service: BaseAIService | None = None, ollama_service: BaseAIService | None = None):
        self.gemini = gemini_service if gemini_service is not None else AIService()
        self.ollama = ollama_service if ollama_service is not None else OllamaService()
        self.threshold = 0.7  # ä¿¡å¿ƒæ°´æº–é–€æª» [cite: 2026-02-05]

    async def _call_ollama(self, messages: list) -> str:
        """æ¥ç¾æœ‰ Ollama å‘¼å«é‚è¼¯ [cite: 2026-02-05]"""
        return await self.ollama.chat(messages)

    async def _call_gemini(self, messages: list) -> str:
        """æ¥ç¾æœ‰ Gemini API å‘¼å«é‚è¼¯"""
        return await self.gemini.chat(messages)

    async def smart_request(self, prompt: str, task_type: str = "conversation") -> str:
        """
        æ™ºèƒ½èª¿åº¦å…¥å£ï¼šæœ¬åœ°å„ªå…ˆç­–ç•¥ [cite: 2026-02-05]
        prompt ç‚ºå–®ä¸€ä½¿ç”¨è€…è¼¸å…¥æ™‚ä½¿ç”¨ï¼›è‹¥éœ€ ReAct å…¨æ­·å²è«‹ç”¨ chat(messages)ã€‚
        """
        messages = [{"role": "user", "content": prompt}]
        if task_type == "code_snippet" or len(prompt) < 100:
            return await self._call_ollama(messages)
        try:
            print("ğŸ›¡ï¸ é˜²è­·ç›¾ï¼šå˜—è©¦ä½¿ç”¨æœ¬åœ°ç®—åŠ› (RTX 4060 Ti)...")
            response = await self._call_ollama(messages)
            if "Unknown action" in (response or "") or not (response or "").strip():
                raise Exception("æœ¬åœ°é‚è¼¯ä¿¡å¿ƒä¸è¶³")
            return response
        except Exception as e:
            print(f"ğŸ’° æœ¬åœ°é˜²ç·šå¤±å®ˆ (åŸå› : {e})ï¼Œå•Ÿå‹• Gemini æ•‘æ´...")
            return await self._call_gemini(messages)

    async def chat(self, messages: list) -> str:
        """
        èˆ‡ BaseAIService ä¸€è‡´ï¼šReAct å…¨æ­·å²èª¿åº¦ã€‚çŸ­å…§å®¹å¼·åˆ¶æœ¬åœ°ï¼Œå¦å‰‡å…ˆæœ¬åœ°å¾Œé›²ç«¯ã€‚
        """
        last_user = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                last_user = (m.get("content") or "").strip()
                break
        if len(last_user) < 100:
            return await self._call_ollama(messages)
        try:
            print("ğŸ›¡ï¸ é˜²è­·ç›¾ï¼šå˜—è©¦ä½¿ç”¨æœ¬åœ°ç®—åŠ› (RTX 4060 Ti)...")
            response = await self._call_ollama(messages)
            if "Unknown action" in (response or "") or not (response or "").strip():
                raise Exception("æœ¬åœ°é‚è¼¯ä¿¡å¿ƒä¸è¶³")
            return response
        except Exception as e:
            print(f"ğŸ’° æœ¬åœ°é˜²ç·šå¤±å®ˆ (åŸå› : {e})ï¼Œå•Ÿå‹• Gemini æ•‘æ´...")
            return await self._call_gemini(messages)


class AIServiceFactory:
    """æœå‹™å·¥å» ï¼šä¸€éµåˆ‡æ›é›²ç«¯å¼•æ“ï¼›é è¨­æœ¬åœ° Ollama æœ€çœéŒ¢ã€‚"""

    @staticmethod
    def create(provider: str) -> BaseAIService:
        if provider == "gemini":
            return GeminiService()
        if provider == "ollama":
            return OllamaService()
        if provider == "claude":
            return ClaudeService()
        if provider == "aliyun":
            return AliyunService()
        if provider == "tencent":
            return TencentService()
        return OllamaService()  # é è¨­æœ¬åœ°æœ€çœéŒ¢

    get_service = create  # ç›¸å®¹èˆŠç”¨æ³•

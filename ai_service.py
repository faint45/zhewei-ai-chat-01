# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€ â€” AI æœå‹™ä»‹é¢ï¼Œå°æ¥ Google Gemini / Ollama / DeepSeek / MiniMax ç­‰
ç’°å¢ƒåŠ è¼‰ .envï¼›BaseAIService æŠ½è±¡åŸºé¡ï¼Œå„æœå‹™å¯¦ä½œ chat(messages)ï¼›AIServiceFactory.create(provider) ä¸€éµåˆ‡æ›ã€‚
"""
import asyncio
import json
import os
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Sequence

# ç”¨é‡è¿½è¹¤
try:
    from ai_usage_tracker import usage_tracker, track_api_call
    USAGE_TRACKING = True
except ImportError:
    USAGE_TRACKING = False
    def track_api_call(*args, **kwargs): pass

# ç’°å¢ƒåŠ è¼‰ï¼šè®€å– .env æª”æ¡ˆä¸­çš„ GEMINI_API_KEYï¼ˆå°ˆæ¡ˆæ ¹ç›®éŒ„å„ªå…ˆï¼Œå†è¼‰å…¥ ~/.openclaw/.envï¼Œä¸è¦†å¯«æ—¢æœ‰éµï¼‰
try:
    from dotenv import load_dotenv
    ROOT = Path(__file__).resolve().parent
    load_dotenv(ROOT / ".env")
    ue = Path(os.path.expanduser("~/.openclaw/.env"))
    if ue.is_file():
        load_dotenv(ue, override=False)
except ImportError:
    pass

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.5-flash")
GEMINI_TIMEOUT = float(os.environ.get("GEMINI_TIMEOUT", "90"))
GEMINI_FALLBACK_MODELS = [
    x.strip()
    for x in (os.environ.get("GEMINI_FALLBACK_MODELS") or "gemini-2.5-flash,gemini-2.5-pro,gemini-2.0-flash,gemini-1.5-flash").split(",")
    if x.strip()
]

OLLAMA_BASE_URL = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11460").rstrip("/")
OLLAMA_MODEL = os.environ.get("OLLAMA_CODER_MODEL") or os.environ.get("OLLAMA_MODEL", "qwen3:32b")
OLLAMA_TIMEOUT = float(os.environ.get("OLLAMA_TIMEOUT", "120"))
OLLAMA_STRONG_MODEL = os.environ.get("OLLAMA_STRONG_MODEL", "qwen3:32b")
OLLAMA_FAST_MODEL = os.environ.get("OLLAMA_FAST_MODEL", "qwen3:4b")
OLLAMA_BRAIN_MODEL = os.environ.get("OLLAMA_BRAIN_MODEL", "zhewei-brain-v5-structured")

LLAMA_SWAP_URL = (os.environ.get("LLAMA_SWAP_URL") or "http://localhost:10005").rstrip("/")
LLAMA_SWAP_MODEL = os.environ.get("LLAMA_SWAP_MODEL", "qwen3-32b")
LLAMA_SWAP_TIMEOUT = float(os.environ.get("LLAMA_SWAP_TIMEOUT", "120"))

MINIMAX_API_KEY = os.environ.get("MINIMAX_API_KEY", "").strip()
MINIMAX_MODEL = os.environ.get("MINIMAX_MODEL", "MiniMax-M2.5")
MINIMAX_API_URL = os.environ.get("MINIMAX_API_URL", "https://api.minimax.io/v1/text/chatcompletion_v2")
MINIMAX_TIMEOUT = float(os.environ.get("MINIMAX_TIMEOUT", "120"))

# æ™ºæ…§è·¯ç”±æ¨¡å¼ï¼šlocal_onlyï¼ˆç´”æœ¬åœ°é›¶é›²ç«¯ï¼‰/ local_firstï¼ˆæœ¬åœ°å„ªå…ˆï¼‰/ smart_route / cloud_first
AI_COST_MODE = os.environ.get("AI_COST_MODE", "local_only").strip().lower()
OLLAMA_REASON_MODEL = os.environ.get("OLLAMA_REASON_MODEL", "deepseek-r1:14b")


def _error_json(message: str) -> str:
    """å›å‚³ç¬¦åˆ ReAct è¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ï¼Œè®“ AgentManager èƒ½è­˜åˆ¥ä¸¦çµæŸã€‚"""
    return json.dumps({"done": True, "result": message}, ensure_ascii=False)


class BaseAIService(ABC):
    """AI æœå‹™æŠ½è±¡åŸºé¡ï¼šæ‰€æœ‰å¼•æ“å¯¦ä½œ chat(messages) -> strã€‚"""

    @abstractmethod
    async def chat(self, messages: list) -> str:
        pass


def _react_to_gemini_history(messages: Sequence[dict]) -> tuple[str, list, str]:
    """
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ã€‚
    å›å‚³ (system_instruction, history, last_user_content)ã€‚
    Gemini: history = [{"role": "user"|"model", "parts": [str]}, ...]ï¼›æœ€å¾Œä¸€å‰‡ user å–®ç¨ä½œç‚º send_message çš„è¼¸å…¥ã€‚
    """
    system_instruction = ""
    gemini_history = []
    last_user_content = ""

    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_instruction = content
        elif role == "user":
            last_user_content = content
            gemini_history.append({"role": "user", "parts": [content]})
        elif role == "assistant":
            gemini_history.append({"role": "model", "parts": [content]})

    # æœ€å¾Œä¸€å‰‡ç‚º userï¼Œä½œç‚ºæœ¬è¼ªè¦é€çµ¦ Gemini çš„è¼¸å…¥ï¼›history ç‚ºå…¶å‰çš„æ‰€æœ‰è¼ª
    if gemini_history and gemini_history[-1]["role"] == "user":
        last_user_content = gemini_history[-1]["parts"][0]
        history_for_chat = gemini_history[:-1]
    else:
        history_for_chat = gemini_history

    return system_instruction, history_for_chat, last_user_content


def _gemini_chat_sync(messages: Sequence[dict]) -> str:
    """
    åŒæ­¥å‘¼å« Geminiï¼šReAct history è½‰ Gemini æ ¼å¼ï¼Œstart_chat + send_messageã€‚
    API è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ _error_json(...)ã€‚
    """
    if not GEMINI_API_KEY:
        return _error_json("æœªè¨­å®š GEMINI_API_KEYï¼Œè«‹åœ¨ .env æˆ– ~/.openclaw/.env è¨­å®šã€‚")

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        system_instruction, history_for_chat, last_user_content = _react_to_gemini_history(messages)

        candidates = []
        for m in [GEMINI_MODEL, *GEMINI_FALLBACK_MODELS]:
            if m not in candidates:
                candidates.append(m)

        last_error = ""
        for model_name in candidates:
            try:
                model = genai.GenerativeModel(
                    model_name,
                    system_instruction=system_instruction or None,
                )
                chat = model.start_chat(history=history_for_chat)
                response = chat.send_message(last_user_content or "è«‹å›è¦†ã€‚")
                if response and response.text:
                    return response.text.strip()
                last_error = f"{model_name}: Gemini æœªå›å‚³æ–‡å­—ã€‚"
            except Exception as e:
                msg = str(e)
                last_error = f"{model_name}: {msg}"
                # model not found -> try next candidate
                if ("404" in msg and "not found" in msg.lower()) or ("is not found" in msg.lower()):
                    continue
                # other errors can still fallback to next model once
                continue

        return _error_json(f"API éŒ¯èª¤: {last_error or 'unknown'}")
    except Exception as e:
        return _error_json(f"API éŒ¯èª¤: {e}")


class AIService(BaseAIService):
    """
    å°æ¥ Google Gemini APIï¼šå¯¦ä½œ chat(messages) ç•°æ­¥æ–¹æ³•ï¼Œ
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ä¸¦å‘¼å« APIï¼›
    è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ç¬¦åˆè¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ä¾› AgentManager è­˜åˆ¥ã€‚
    """

    async def chat(self, messages: list) -> str:
        """
        ç•°æ­¥å‘¼å« Geminiï¼›messages ç‚º ReAct historyï¼ˆå« system / user / assistantï¼‰ã€‚
        å…§éƒ¨è½‰ç‚º Gemini å°è©±æ ¼å¼ä¸¦ä»¥ run_in_executor åŸ·è¡ŒåŒæ­¥è«‹æ±‚ï¼Œå¸¶è¶…æ™‚ã€‚
        """
        try:
            return await asyncio.wait_for(
                asyncio.to_thread(_gemini_chat_sync, list(messages)),
                timeout=GEMINI_TIMEOUT,
            )
        except asyncio.TimeoutError:
            return _error_json(f"API é€¾æ™‚ï¼ˆ{GEMINI_TIMEOUT} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"å‘¼å«å¤±æ•—: {e}")


# ä¾› brain_server ç­‰æ¨¡çµ„ä½¿ç”¨ï¼šfrom ai_service import GeminiService, OllamaService
GeminiService = AIService


def _react_to_ollama_messages(messages: Sequence[dict]) -> list[dict]:
    """å°‡ ReAct æ­·å²è½‰ç‚º Ollama /api/chat çš„ messagesï¼šsystem ä½µå…¥é¦–å‰‡ userï¼Œå…¶é¤˜ user/assistant ç…§æ¬ã€‚"""
    out = []
    system_parts = []
    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_parts.append(content)
        elif role == "user":
            content = ("\n\n".join(system_parts) + "\n\n" + content).strip() if system_parts else content
            system_parts = []
            out.append({"role": "user", "content": content})
        elif role == "assistant":
            out.append({"role": "assistant", "content": content})
    return out


class OllamaService(BaseAIService):
    """
    æœ¬åœ°å¤§è…¦ï¼šå‘¼å«æœ¬åœ° Ollama æœå‹™ï¼ˆQwen3:8B ç­‰ï¼‰ï¼Œå…è²»/æ¶ˆè€—é›»åŠ›ã€‚
    ä½¿ç”¨ httpx éåŒæ­¥ POST /api/chatï¼›stream=Falseï¼Œoptions ç¢ºä¿è¼¸å‡ºç©©å®šã€æ“´å¤§ä¸Šä¸‹æ–‡ã€‚
    è‹¥æœå‹™ç•°å¸¸ï¼Œå›å‚³ JSON éŒ¯èª¤ï¼ˆå« done: Trueï¼‰è®“ AgentManager è­˜åˆ¥ã€‚
    å¢å¼·ç©©å®šæ€§ï¼šè‡ªå‹•é‡è©¦ã€å¥åº·æª¢æŸ¥ã€å¤šæ¨¡å‹å‚™æ´ã€‚
    """

    def __init__(self, model_name: str | None = None):
        base = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11460").rstrip("/")
        self.base_url = f"{base}/api/chat"
        self.model_name = model_name or OLLAMA_MODEL
        self.timeout = OLLAMA_TIMEOUT
        self.fallback_models = ["qwen3:8b", "qwen3:4b", "gemma3:4b", "zhewei-brain"]
        self._health_checked = False

    async def _health_check(self) -> bool:
        """å¥åº·æª¢æŸ¥ï¼šç¢ºèª Ollama æœå‹™å¯ç”¨"""
        try:
            import httpx
            base = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11460").rstrip("/")
            async with httpx.AsyncClient(timeout=5) as client:
                r = await client.get(f"{base}/api/tags")
                if r.status_code == 200:
                    data = r.json()
                    models = [m["name"] for m in data.get("models", [])]
                    self._health_checked = True
                    return any(self.model_name in m for m in models)
            return False
        except Exception:
            return False

    async def chat(self, messages: list) -> str:
        """
        å‘¼å«æœ¬åœ° Ollama æœå‹™ï¼›messages ç‚º ReAct historyï¼Œæœƒå…ˆè½‰ç‚º Ollama çš„ messages æ ¼å¼ã€‚
        å¢å¼·ç©©å®šæ€§ï¼šè‡ªå‹•é‡è©¦ã€æ¨¡å‹å‚™æ´ã€‚
        """
        # é¦–æ¬¡å¥åº·æª¢æŸ¥
        if not self._health_checked:
            if not await self._health_check():
                return _error_json("Ollama æœå‹™æœªéŸ¿æ‡‰ï¼Œè«‹ç¢ºèª Ollama å·²å•Ÿå‹• (localhost:11434)")

        ollama_messages = _react_to_ollama_messages(list(messages))
        if not ollama_messages:
            return _error_json("Ollamaï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")

        # å˜—è©¦ä¸»è¦æ¨¡å‹
        content = await self._try_generate(self.model_name, ollama_messages)
        if content:
            return content

        # æ¨¡å‹å¤±æ•—ï¼Œå˜—è©¦å‚™æ´æ¨¡å‹
        for fallback in self.fallback_models:
            if fallback != self.model_name:
                content = await self._try_generate(fallback, ollama_messages)
                if content:
                    return content

        return _error_json("æ‰€æœ‰æœ¬åœ°æ¨¡å‹å‡ç„¡æ³•å›æ‡‰ï¼Œè«‹æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹")

    async def _try_generate(self, model: str, messages: list) -> str | None:
        """å˜—è©¦ç”Ÿæˆå›æ‡‰ï¼ˆæ”¯æ´ Qwen3 thinking æ¨¡å¼ + GPU offloadï¼‰"""
        import httpx
        # æ ¹æ“šæ¨¡å‹å¤§å°è‡ªå‹•é…ç½®
        # æ³¨æ„ï¼šä¸æŒ‡å®š num_gpu è®“ Ollama è‡ªå‹•è·¨é›™å¡åˆ†é…ï¼ˆç¡¬ç·¨ç¢¼ 99 æœƒå ± memory layout éŒ¯èª¤ï¼‰
        num_ctx = 8192
        options = {
            "temperature": 0.2,
            "num_ctx": num_ctx,
            "num_batch": 512,
        }
        if "70b" in model or "72b" in model:
            options["num_gpu"] = 12  # 70B éœ€ CPU offloadï¼Œé™åˆ¶ GPU å±¤æ•¸
            options["num_ctx"] = 2048
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": options,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.base_url, json=payload)
                response.raise_for_status()
                result = response.json()
                msg = result.get("message") or {}
                content = msg.get("content") or ""
                # Qwen3 thinking æ¨¡å¼ï¼šcontent å¯èƒ½ç‚ºç©ºï¼ŒçœŸæ­£ç­”æ¡ˆåœ¨ thinking æ¬„ä½
                if not content.strip() and msg.get("thinking"):
                    content = msg["thinking"]
                return content.strip() if content else None
        except Exception:
            return None


class LiteLLMService(BaseAIService):
    """
    LiteLLM çµ±ä¸€è·¯ç”±ï¼šé€éå–®ä¸€ä»‹é¢å‘¼å« 100+ LLM æä¾›å•†ã€‚
    æ”¯æ´ OpenAI / Anthropic / Gemini / Ollama / DeepSeek / Groq ç­‰ã€‚
    """

    def __init__(self, model: str = "ollama/qwen3:8b"):
        self.model = model
        self.timeout = 120

    async def chat(self, messages: list) -> str:
        try:
            import litellm
            litellm.set_verbose = False
            # è½‰æ› messages æ ¼å¼
            llm_messages = []
            for m in messages:
                role = m.get("role", "user")
                content = (m.get("content") or "").strip()
                if content:
                    llm_messages.append({"role": role, "content": content})
            if not llm_messages:
                return _error_json("LiteLLMï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")

            response = await asyncio.wait_for(
                asyncio.to_thread(
                    litellm.completion,
                    model=self.model,
                    messages=llm_messages,
                    temperature=0.2,
                    max_tokens=4096,
                ),
                timeout=self.timeout,
            )
            text = (response.choices[0].message.content or "").strip()
            return text or _error_json("LiteLLM æœªå›å‚³æ–‡å­—ã€‚")
        except asyncio.TimeoutError:
            return _error_json(f"LiteLLM é€¾æ™‚ï¼ˆ{self.timeout} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"LiteLLM éŒ¯èª¤: {e}")


class AliyunService(BaseAIService):
    """é ç•™ï¼šé˜¿é‡Œé›²é€šç¾©åƒå• (Qwen API)ï¼Œæœªä¾†å°æ¥ DashScope APIã€‚"""

    async def chat(self, messages: list) -> str:
        # æœªä¾†åœ¨æ­¤è™•å°æ¥ DashScope API
        return json.dumps({"thought": "é˜¿é‡Œé›²æ­£åœ¨æ€è€ƒ...", "done": True, "result": "é˜¿é‡Œé›²æœå‹™å°šæœªæ¥ç·šã€‚"}, ensure_ascii=False)


class TencentService(BaseAIService):
    """é ç•™ï¼šé¨°è¨Šé›²æ··å…ƒç­‰ï¼Œå°šæœªæ¥ç·šã€‚"""

    async def chat(self, messages: list) -> str:
        return _error_json("é¨°è¨Šé›²æœå‹™å°šæœªæ¥ç·šï¼Œè«‹ä½¿ç”¨ gemini æˆ– ollamaã€‚")


ANTHROPIC_API_KEY = (os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY") or "").strip()
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
ANTHROPIC_TIMEOUT = float(os.environ.get("ANTHROPIC_TIMEOUT", "120"))


class ClaudeService(BaseAIService):
    """
    éšæ®µ 4/6 ç·¨ç¢¼èˆ‡å›é¥‹ä¿®æ­£ï¼šå°æ¥ Anthropic Claude APIã€‚
    æœ‰è¨­ ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEY æ™‚ï¼Œagent_logic å¯æ³¨å…¥ä¾›éšæ®µ 4/6 ä½¿ç”¨ã€‚
    """

    def __init__(self, api_key: str | None = None, model: str | None = None):
        self.api_key = (api_key or ANTHROPIC_API_KEY).strip()
        self.model = model or ANTHROPIC_MODEL
        self.timeout = ANTHROPIC_TIMEOUT

    async def chat(self, messages: list) -> str:
        if not self.api_key:
            return _error_json("æœªè¨­å®š ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEYï¼Œè«‹åœ¨ .env è¨­å®šã€‚")
        system_parts = []
        anthropic_messages = []
        for m in messages:
            role = m.get("role", "")
            content = (m.get("content") or "").strip()
            if role == "system":
                system_parts.append(content)
            elif role == "user":
                anthropic_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                anthropic_messages.append({"role": "assistant", "content": content})
        if not anthropic_messages:
            return _error_json("Claudeï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")
        try:
            from anthropic import AsyncAnthropic
            client = AsyncAnthropic(api_key=self.api_key)
            system = "\n\n".join(system_parts).strip() if system_parts else None
            response = await asyncio.wait_for(
                client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    system=system or None,
                    messages=anthropic_messages,
                ),
                timeout=self.timeout,
            )
            text = (response.content[0].text if response.content else "").strip()
            return text or _error_json("Claude æœªå›å‚³æ–‡å­—ã€‚")
        except asyncio.TimeoutError:
            return _error_json(f"Claude API é€¾æ™‚ï¼ˆ{self.timeout} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"Claude API éŒ¯èª¤: {e}")


class MiniMaxService(BaseAIService):
    """
    MiniMax M2.5 â€” é–‹æºå‰æ²¿æ¨ç†æ¨¡å‹ï¼ˆ230B MoE, 10B activeï¼‰ã€‚
    æ€§èƒ½åŒ¹é… Claude Opus 4.6ï¼Œæˆæœ¬åƒ… 1/20ã€‚
    API: https://api.minimax.io/v1/text/chatcompletion_v2
    """

    def __init__(self, api_key: str | None = None, model: str | None = None):
        self.api_key = (api_key or MINIMAX_API_KEY).strip()
        self.model = model or MINIMAX_MODEL
        self.api_url = MINIMAX_API_URL
        self.timeout = MINIMAX_TIMEOUT

    async def chat(self, messages: list) -> str:
        if not self.api_key:
            return _error_json("æœªè¨­å®š MINIMAX_API_KEYï¼Œè«‹åœ¨ .env è¨­å®šã€‚")

        # è½‰æ› messages ç‚º MiniMax æ ¼å¼ï¼ˆç›¸å®¹ OpenAI é¢¨æ ¼ï¼‰
        mm_messages = []
        for m in messages:
            role = m.get("role", "")
            content = (m.get("content") or "").strip()
            if role == "system":
                mm_messages.append({"role": "system", "content": content})
            elif role == "user":
                mm_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                mm_messages.append({"role": "assistant", "content": content})
        if not mm_messages:
            return _error_json("MiniMaxï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")

        payload = {
            "model": self.model,
            "messages": mm_messages,
            "stream": False,
            "temperature": 0.3,
            "top_p": 0.95,
            "max_completion_tokens": 8192,
        }

        try:
            import httpx
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.api_url, json=payload, headers=headers)
                response.raise_for_status()
                result = response.json()
                # æª¢æŸ¥ API å±¤ç´šéŒ¯èª¤
                base_resp = result.get("base_resp", {})
                if base_resp.get("status_code", 0) != 0:
                    return _error_json(f"MiniMax API éŒ¯èª¤: {base_resp.get('status_msg', 'unknown')}")
                # æå–å›æ‡‰æ–‡å­—
                choices = result.get("choices", [])
                if choices:
                    text = (choices[0].get("message") or {}).get("content", "").strip()
                    return text or _error_json("MiniMax æœªå›å‚³æ–‡å­—ã€‚")
                return _error_json("MiniMax æœªå›å‚³ choicesã€‚")
        except asyncio.TimeoutError:
            return _error_json(f"MiniMax API é€¾æ™‚ï¼ˆ{self.timeout} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"MiniMax API éŒ¯èª¤: {e}")


DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY", "")
DEEPSEEK_MODEL = os.environ.get("DEEPSEEK_MODEL", "deepseek-chat")
DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"
DEEPSEEK_TIMEOUT = float(os.environ.get("DEEPSEEK_TIMEOUT", "120"))


class DeepSeekService(BaseAIService):
    """
    DeepSeek æ»¿è¡€ç‰ˆ â€” é–‹æºæ¨ç†æ¨¡å‹ï¼ˆ671B MoE, 37B activeï¼‰ã€‚
    æ”¯æ´è¶…é•·ä¸Šä¸‹æ–‡ï¼ˆ200Kï¼‰ï¼Œæ¨ç†èƒ½åŠ›å¼·å¤§ã€‚
    """
    def __init__(self, api_key: str = None, model: str = None):
        self.api_key = api_key or DEEPSEEK_API_KEY
        self.model = model or DEEPSEEK_MODEL
        self.timeout = DEEPSEEK_TIMEOUT

    async def chat(self, messages: list) -> str:
        if not self.api_key:
            return _error_json("æœªè¨­å®š DEEPSEEK_API_KEYï¼Œè«‹åœ¨ .env è¨­å®šã€‚")

        import httpx
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": 4096,
            "temperature": 0.7
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(DEEPSEEK_API_URL, json=payload, headers=headers)
                r.raise_for_status()
                data = r.json()
                content = data["choices"][0]["message"]["content"]
                
                # ç”¨é‡è¿½è¹¤
                if USAGE_TRACKING:
                    input_tokens = sum(len(str(m)) for m in messages)
                    output_tokens = len(str(content))
                    track_api_call("deepseek", self.model, input_tokens, output_tokens)
                
                return json.dumps({"done": True, "result": content}, ensure_ascii=False)
        except Exception as e:
            return _error_json(f"DeepSeek API éŒ¯èª¤: {e}")


class SmartAIService(BaseAIService):
    """
    æ™ºæ…§è·¯ç”±èª¿åº¦å¼•æ“ â€” é›™ GPU æœ¬åœ°å„ªå…ˆæ¶æ§‹ã€‚

    æ ¸å¿ƒè¨­è¨ˆç†å¿µï¼ˆ2026-02 å‡ç´šï¼šRTX 5070 Ti 16GB + RTX 4060 Ti 8GBï¼‰ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ  Ollama qwen3:32b = è»å¸«ï¼ˆå¤§è…¦ï¼‰æœ¬åœ°ï¼      â”‚
    â”‚    â†’ æ€è€ƒã€åˆ†æã€è¦åŠƒã€æ–¹å‘å»ºè­°ã€å°ˆæ¥­åˆ¤æ–·       â”‚
    â”‚    â†’ 32B Q4 å…¨ GPU 25-30 tok/s æ¥è¿‘ GPT-4     â”‚
    â”‚                                                â”‚
    â”‚  ğŸ  Ollama qwen3:8b = å£«å…µï¼ˆåŸ·è¡Œï¼‰              â”‚
    â”‚    â†’ ç¨‹å¼ç¢¼ç”Ÿæˆã€æ–‡å­—ç¿»è­¯ã€æ ¼å¼è½‰æ›ã€é‡è¤‡ä»»å‹™   â”‚
    â”‚    â†’ 60+ tok/s æ¥µé€Ÿå›æ‡‰                        â”‚
    â”‚                                                â”‚
    â”‚  â˜ï¸ DeepSeek = é›²ç«¯å‚™æ´ï¼ˆåƒ…è¶…é•·ä¸Šä¸‹æ–‡ï¼‰          â”‚
    â”‚    â†’ 200K contextã€æœ¬åœ°è™•ç†ä¸äº†çš„è¤‡é›œä»»å‹™       â”‚
    â”‚                                                â”‚
    â”‚  ğŸŸ£ Claude = æœ€å¾Œå‚™æ´å°‡è»                       â”‚
    â”‚    â†’ å…¨éƒ¨æœ¬åœ° + DeepSeek éƒ½æ›äº†æ‰ä¸Š             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä¸‰ç¨®æ¨¡å¼ï¼š
    - smart_routeï¼ˆæ¨è–¦ï¼‰ï¼šæœ¬åœ° 32b è»å¸« / 8b å£«å…µ / é›²ç«¯å‚™æ´
    - local_firstï¼šçœéŒ¢æ¨¡å¼ï¼Œå…¨éƒ¨æœ¬åœ°
    - cloud_firstï¼šå“è³ªå„ªå…ˆï¼Œå…¨éƒ¨é›²ç«¯
    """

    # â”€â”€ ä»»å‹™åˆ†é¡é—œéµå­— â”€â”€

    # ğŸ§  æ€è€ƒå‹ä»»å‹™ï¼ˆäº¤çµ¦ Gemini è»å¸«ï¼‰
    _THINK_KEYWORDS = [
        "åˆ†æ", "ç­–ç•¥", "è¦åŠƒ", "å»ºè­°", "æ–¹å‘", "è©•ä¼°", "åˆ¤æ–·", "æ¯”è¼ƒ",
        "æ¶æ§‹", "è¨­è¨ˆ", "æ±ºç­–", "å„ªç¼ºé»", "é¢¨éšª", "è¶¨å‹¢", "é æ¸¬",
        "ç‚ºä»€éº¼", "æ€éº¼çœ‹", "å¦‚ä½•é¸", "å“ªå€‹å¥½", "å€¼ä¸å€¼",
        "analyze", "strategy", "plan", "suggest", "evaluate", "compare",
        "design", "architecture", "decision", "pros and cons", "recommend",
        "å ±å‘Š", "ç ”ç©¶", "æ·±å…¥", "è©³ç´°", "å®Œæ•´", "å…¨é¢",
    ]

    # âš¡ åŸ·è¡Œå‹ä»»å‹™ï¼ˆäº¤çµ¦ Ollama å£«å…µï¼‰
    _EXEC_KEYWORDS = [
        "å¯«", "ç”Ÿæˆ", "ç”¢ç”Ÿ", "ç¿»è­¯", "è½‰æ›", "æ ¼å¼åŒ–", "ä¿®æ”¹", "æ”¹å¯«",
        "ç¨‹å¼", "code", "function", "class", "html", "css", "json", "sql",
        "write", "generate", "create", "convert", "format", "translate",
        "template", "boilerplate", "snippet", "example",
        "æ‘˜è¦", "ç¸½çµ", "åˆ—å‡º", "æ•´ç†", "æ’ç‰ˆ",
        "hello", "ä½ å¥½", "è¬è¬", "ok", "å¥½çš„",
    ]

    # â”€â”€ æœ¬åœ°å¥åº·æ¢é‡è¨­å®š â”€â”€
    _HEALTH_CACHE_TTL = 30       # å¥åº·ç‹€æ…‹å¿«å–ç§’æ•¸
    _HEALTH_PROBE_TIMEOUT = 2.0  # æ¢é‡è¶…æ™‚ç§’æ•¸

    def __init__(
        self,
        gemini_service: BaseAIService | None = None,
        ollama_service: BaseAIService | None = None,
        claude_service: BaseAIService | None = None,
        minimax_service: BaseAIService | None = None,
    ):
        self.gemini = gemini_service if gemini_service is not None else AIService()
        self.ollama = ollama_service if ollama_service is not None else OllamaService(model_name=OLLAMA_STRONG_MODEL)
        self.ollama_fast = OllamaService(model_name=OLLAMA_FAST_MODEL)
        self.ollama_reason = OllamaService(model_name=OLLAMA_REASON_MODEL)
        self.brain = OllamaService(model_name=OLLAMA_BRAIN_MODEL)  # è’¸é¤¾å°ˆæ‰
        self.claude = claude_service if claude_service is not None else ClaudeService()
        self.minimax = minimax_service if minimax_service is not None else MiniMaxService()
        self.llama_swap = LlamaSwapService()
        self.mode = AI_COST_MODE
        # TaskPlanner ç²¾æº–æŒ‡æ´¾å¼•æ“
        try:
            from ai_modules.task_planner import planner as _tp, PrePlanner as _pp
            self._task_planner = _tp
            self._pre_planner = _pp()
        except ImportError:
            self._task_planner = None
            self._pre_planner = None
        # å¥åº·ç‹€æ…‹å¿«å–ï¼š(is_healthy: bool, timestamp: float)
        self._ollama_health: tuple[bool, float] = (True, 0.0)
        self._llama_swap_health: tuple[bool, float] = (False, 0.0)
        self._consecutive_ollama_failures = 0
        # ç”¨é‡è¨ˆé‡ï¼šç•¶å‰ç”¨æˆ¶ IDï¼ˆç”±å‘¼å«ç«¯è¨­å®šï¼‰
        self._current_user_id: str = "anonymous"

    async def _check_ollama_health(self) -> bool:
        """
        è¼•é‡ç´š Ollama å¥åº·æ¢é‡ï¼šGET /api/tagsï¼ˆ2s è¶…æ™‚ï¼‰ã€‚
        çµæœå¿«å– 30 ç§’ï¼Œé¿å…æ¯æ¬¡è«‹æ±‚éƒ½æ¢æ¸¬ã€‚
        é€£çºŒå¤±æ•— 3 æ¬¡å¾Œæ¨™è¨˜ç‚ºä¸å¥åº·ï¼Œç›´åˆ°æ¢é‡æ¢å¾©ã€‚
        """
        cached_healthy, cached_ts = self._ollama_health
        if time.time() - cached_ts < self._HEALTH_CACHE_TTL:
            return cached_healthy

        try:
            import httpx
            base = OLLAMA_BASE_URL
            async with httpx.AsyncClient(timeout=self._HEALTH_PROBE_TIMEOUT) as client:
                r = await client.get(f"{base}/api/tags")
                healthy = r.status_code == 200
        except Exception:
            healthy = False

        self._ollama_health = (healthy, time.time())
        if healthy:
            if self._consecutive_ollama_failures > 0:
                print("ğŸ”„ Ollama æ¢å¾©æ­£å¸¸ï¼åˆ‡å›æœ¬åœ°åŸ·è¡Œã€‚")
            self._consecutive_ollama_failures = 0
        else:
            self._consecutive_ollama_failures += 1
            if self._consecutive_ollama_failures == 1:
                print("âš ï¸ Ollama æœ¬åœ°æœå‹™ç•°å¸¸ï¼Œè‡ªå‹•åˆ‡æ›è‡³é›²ç«¯ã€‚")
        return healthy

    def _classify_task(self, messages: list) -> str:
        """
        åˆ†é¡ä»»å‹™é¡å‹ï¼šthinkï¼ˆæ€è€ƒ/è¦åŠƒï¼‰æˆ– executeï¼ˆåŸ·è¡Œ/ç”Ÿæˆï¼‰ã€‚

        è¦å‰‡ï¼š
        1. å«æ€è€ƒé—œéµå­— â†’ thinkï¼ˆäº¤çµ¦ Gemini è»å¸«ï¼‰
        2. å«åŸ·è¡Œé—œéµå­— â†’ executeï¼ˆäº¤çµ¦ Ollama å£«å…µï¼‰
        3. çŸ­è¨Šæ¯ï¼ˆ<30 å­—ï¼‰â†’ executeï¼ˆç°¡å–®å°è©±ï¼Œæœ¬åœ°è™•ç†ï¼‰
        4. é•·è¨Šæ¯æˆ–å¤šè¼ªå°è©± â†’ thinkï¼ˆéœ€è¦æ·±åº¦ç†è§£ï¼‰
        5. é è¨­ â†’ thinkï¼ˆå¯§å¯ç”¨å¥½çš„ï¼Œä¸è¦ç”¨å·®çš„ï¼‰
        """
        last_user = ""
        total_len = 0
        for m in messages:
            total_len += len(m.get("content") or "")
            if m.get("role") == "user":
                last_user = (m.get("content") or "").strip()

        lower = last_user.lower()

        # è¨ˆç®—æ€è€ƒ/åŸ·è¡Œä¿¡è™Ÿå¼·åº¦
        think_score = sum(1 for kw in self._THINK_KEYWORDS if kw in lower)
        exec_score = sum(1 for kw in self._EXEC_KEYWORDS if kw in lower)

        # çŸ­è¨Šæ¯ä¸”ç„¡æ˜ç¢ºæ€è€ƒä¿¡è™Ÿ â†’ åŸ·è¡Œï¼ˆæœ¬åœ°è™•ç†ï¼‰
        if len(last_user) < 30 and think_score == 0:
            return "execute"

        # æ˜ç¢ºæ€è€ƒä¿¡è™Ÿè¼ƒå¼· â†’ æ€è€ƒ
        if think_score > exec_score:
            return "think"

        # æ˜ç¢ºåŸ·è¡Œä¿¡è™Ÿè¼ƒå¼· â†’ åŸ·è¡Œ
        if exec_score > think_score:
            return "execute"

        # é•·æ–‡æˆ–å¤šè¼ª â†’ éœ€è¦æ·±åº¦ç†è§£ â†’ æ€è€ƒ
        if len(last_user) > 300 or total_len > 2000:
            return "think"

        # é è¨­ï¼šäº¤çµ¦è»å¸«ï¼ˆå“è³ªå„ªå…ˆï¼‰
        return "think"

    def _is_error_response(self, response: str) -> bool:
        """åˆ¤æ–·å›æ‡‰æ˜¯å¦ç‚ºéŒ¯èª¤/ç©ºç™½ã€‚"""
        if not response or not response.strip():
            return True
        error_markers = ["Unknown action", "æœå‹™ç•°å¸¸", "API éŒ¯èª¤", "API é€¾æ™‚", "æœªå›å‚³æ–‡å­—"]
        return any(m in response for m in error_markers)

    async def _check_llama_swap_health(self) -> bool:
        """æª¢æŸ¥ llama-swap æ˜¯å¦é‹è¡Œä¸­ï¼ˆå¿«å– 60 ç§’ï¼‰ã€‚"""
        cached_healthy, cached_ts = self._llama_swap_health
        if time.time() - cached_ts < 60:
            return cached_healthy
        try:
            import httpx
            async with httpx.AsyncClient(timeout=2.0) as client:
                r = await client.get(f"{LLAMA_SWAP_URL}/v1/models")
                healthy = r.status_code == 200
        except Exception:
            healthy = False
        self._llama_swap_health = (healthy, time.time())
        return healthy

    async def _call_with_fallback(self, messages: list, chain: list[str], task_label: str = "") -> str:
        """ä¾åºå˜—è©¦ chain ä¸­çš„æœå‹™ï¼Œæœ¬åœ°ç•°å¸¸æ™‚è‡ªå‹•è·³éåˆ‡é›²ç«¯ã€‚æ¯æ¬¡å‘¼å«è‡ªå‹•è¨˜éŒ„ç”¨é‡ã€‚"""
        services = {"ollama": self.ollama, "ollama_fast": self.ollama_fast, "ollama_reason": self.ollama_reason, "brain": self.brain, "gemini": self.gemini, "claude": self.claude, "minimax": self.minimax, "llama_swap": self.llama_swap}
        labels = {"ollama": "ğŸ  è»å¸« Ollama 32b", "ollama_fast": "âš¡ å£«å…µ Ollama fast", "ollama_reason": "ğŸ§  æ¨ç† DeepSeek-R1 14b", "brain": "ğŸ§¬ å°ˆæ‰ Brain-v3", "gemini": "â˜ï¸ è»å¸« Gemini", "claude": "ğŸŸ£ å‚™æ´ Claude", "minimax": "ğŸ”µ å°–å…µ MiniMax", "llama_swap": "ğŸ”„ å‚™æ´ LlamaSwap"}
        model_names = {"ollama": OLLAMA_STRONG_MODEL, "ollama_fast": OLLAMA_FAST_MODEL, "ollama_reason": OLLAMA_REASON_MODEL, "brain": OLLAMA_BRAIN_MODEL, "gemini": GEMINI_MODEL, "claude": ANTHROPIC_MODEL, "minimax": MINIMAX_MODEL, "llama_swap": LLAMA_SWAP_MODEL}
        ollama_healthy = await self._check_ollama_health()
        # ä¼°ç®—è¼¸å…¥ token
        input_text = " ".join(m.get("content", "") for m in messages)
        try:
            from usage_metering import estimate_tokens, record_usage
            input_tokens = estimate_tokens(input_text)
        except Exception:
            input_tokens = 0
        last_error = ""
        for name in chain:
            svc = services.get(name)
            if svc is None:
                continue
            # æœ¬åœ°ç•°å¸¸æ™‚è·³é Ollamaï¼ˆåŒ…å« fastï¼‰ï¼Œç›´æ¥ç”¨é›²ç«¯
            if name in ("ollama", "ollama_fast", "brain") and not ollama_healthy:
                print(f"ğŸ§  æ™ºæ…§è·¯ç”± [{self.mode}|{task_label}] â­ï¸ è·³é {name}ï¼ˆæœ¬åœ°ç•°å¸¸ï¼‰")
                last_error = f"{name}: æœ¬åœ°æœå‹™ä¸å¯ç”¨"
                continue
            # llama-swap æœªé‹è¡Œæ™‚è·³é
            if name == "llama_swap":
                if not await self._check_llama_swap_health():
                    continue
            # æœªè¨­å®š API Key æ™‚è·³é MiniMax
            if name == "minimax" and not MINIMAX_API_KEY:
                continue
            t0 = time.time()
            try:
                print(f"ğŸ§  æ™ºæ…§è·¯ç”± [{self.mode}|{task_label}] â†’ {labels.get(name, name)}")
                response = await svc.chat(messages)
                duration_ms = int((time.time() - t0) * 1000)
                if not self._is_error_response(response):
                    # Ollama å¯¦éš›å‘¼å«æˆåŠŸ â†’ æ›´æ–°å¥åº·ç‹€æ…‹
                    if name in ("ollama", "ollama_fast"):
                        self._ollama_health = (True, time.time())
                        self._consecutive_ollama_failures = 0
                    # è¨˜éŒ„æˆåŠŸç”¨é‡
                    try:
                        out_tokens = estimate_tokens(response)
                        record_usage(
                            provider=name, model=model_names.get(name, name),
                            input_tokens=input_tokens, output_tokens=out_tokens,
                            duration_ms=duration_ms, success=True,
                            user_id=self._current_user_id, task_type=task_label or "chat",
                        )
                    except Exception:
                        pass
                    return response
                last_error = f"{name}: å›æ‡‰å“è³ªä¸è¶³"
                # è¨˜éŒ„å“è³ªä¸è¶³
                try:
                    record_usage(
                        provider=name, model=model_names.get(name, name),
                        input_tokens=input_tokens, output_tokens=0,
                        duration_ms=duration_ms, success=False, error_msg="å›æ‡‰å“è³ªä¸è¶³",
                        user_id=self._current_user_id, task_type=task_label or "chat",
                    )
                except Exception:
                    pass
                # Ollama å›æ‡‰ç•°å¸¸ â†’ è¨˜éŒ„å¤±æ•—
                if name in ("ollama", "ollama_fast", "brain"):
                    self._consecutive_ollama_failures += 1
            except Exception as e:
                duration_ms = int((time.time() - t0) * 1000)
                last_error = f"{name}: {e}"
                # è¨˜éŒ„ç•°å¸¸
                try:
                    record_usage(
                        provider=name, model=model_names.get(name, name),
                        input_tokens=input_tokens, output_tokens=0,
                        duration_ms=duration_ms, success=False, error_msg=str(e)[:200],
                        user_id=self._current_user_id, task_type=task_label or "chat",
                    )
                except Exception:
                    pass
                if name in ("ollama", "ollama_fast", "brain"):
                    self._consecutive_ollama_failures += 1
                    # å³æ™‚æ¨™è¨˜ä¸å¥åº·ï¼Œä¸‹æ¬¡ä¸å†å˜—è©¦ï¼ˆç›´åˆ°å¿«å–éæœŸé‡æ–°æ¢æ¸¬ï¼‰
                    self._ollama_health = (False, time.time())
                    print(f"âš ï¸ Ollama å‘¼å«å¤±æ•—: {e}ï¼Œåˆ‡æ›é›²ç«¯")
                continue
        return _error_json(f"æ‰€æœ‰ AI æœå‹™å‡å¤±æ•—: {last_error}")

    async def smart_request(self, prompt: str, task_type: str = "conversation") -> str:
        """æ™ºæ…§èª¿åº¦å…¥å£ï¼šprompt ç‚ºå–®ä¸€ä½¿ç”¨è€…è¼¸å…¥ã€‚"""
        messages = [{"role": "user", "content": prompt}]
        return await self.chat(messages)

    async def chat(self, messages: list) -> str:
        """
        ä¸»èª¿åº¦æ–¹æ³• â€” TaskPlanner ç²¾æº–æŒ‡æ´¾ + Pre-Planning + QualityGateã€‚

        å‡ç´šæ¶æ§‹ï¼ˆ2026-02-28ï¼‰ï¼š
          L0 greeting  â†’ 4b ç§’å›ï¼ˆ0 æˆæœ¬ï¼‰
          L1 quick     â†’ 4b/8b å¿«é€Ÿå›æ‡‰
          L2 domain    â†’ brain-v3 å°ˆæ‰ æˆ– 32b + RAG
          L3 complex   â†’ 32b + å“è³ªæª¢æŸ¥ + é‡è©¦
          L4 expert    â†’ 32b + RAG + å“æª¢ + é›²ç«¯è¦†æ ¸

        å‘å¾Œç›¸å®¹ï¼šç„¡ TaskPlanner æ™‚é€€å›èˆŠé‚è¼¯ï¼ˆthink/execute äºŒåˆ†æ³•ï¼‰ã€‚
        """
        # â”€â”€ TaskPlanner ç²¾æº–æŒ‡æ´¾ â”€â”€
        if self._task_planner is not None:
            return await self._chat_with_planner(messages)

        # â”€â”€ å‘å¾Œç›¸å®¹ï¼šèˆŠé‚è¼¯ â”€â”€
        task = self._classify_task(messages)
        thinking_mode = os.environ.get("THINKING_PROTOCOL", "auto")
        if thinking_mode != "off" and task == "think":
            try:
                from ai_modules.thinking_protocol import inject_thinking
                messages = inject_thinking(messages, mode="lite")
            except ImportError:
                pass

        if self.mode == "cloud_first":
            if thinking_mode != "off" and task == "think":
                try:
                    from ai_modules.thinking_protocol import inject_thinking
                    messages = inject_thinking(messages, mode="full")
                except ImportError:
                    pass
            return await self._call_with_fallback(messages, ["deepseek", "minimax", "claude", "ollama"], task)

        if self.mode == "local_only":
            if task == "think":
                return await self._call_with_fallback(messages, ["ollama", "ollama_reason", "llama_swap", "ollama_fast"], "think")
            else:
                return await self._call_with_fallback(messages, ["ollama_fast", "ollama", "llama_swap"], "execute")

        if self.mode == "local_first":
            if task == "think":
                return await self._call_with_fallback(messages, ["ollama", "ollama_reason", "llama_swap", "deepseek", "claude"], "think")
            else:
                return await self._call_with_fallback(messages, ["ollama_fast", "ollama", "llama_swap", "deepseek"], "execute")

        if task == "think":
            return await self._call_with_fallback(messages, ["ollama", "llama_swap", "deepseek", "minimax", "claude"], "think")
        else:
            return await self._call_with_fallback(messages, ["ollama_fast", "ollama", "llama_swap", "minimax", "deepseek"], "execute")

    async def _chat_with_planner(self, messages: list) -> str:
        """
        TaskPlanner é©…å‹•çš„ç²¾æº–èª¿åº¦ã€‚

        æµç¨‹ï¼š
        1. TaskPlanner.plan() â€” åˆ†é¡ + ç”¢å‡º TaskPlan
        2. PrePlanner.prepare_messages() â€” SOP/RAG/æ ¼å¼æ³¨å…¥
        3. _call_with_fallback() â€” æŒ‰ model_chain åŸ·è¡Œ
        4. QualityGate â€” å“è³ªæª¢æŸ¥ + è‡ªå‹•å‡ç´š
        5. log_task() â€” è¨˜éŒ„åˆ°è¨“ç·´æ—¥èªŒ
        """
        t0 = time.time()
        plan = self._task_planner.plan(messages)

        # å–å‡ºæœ€å¾Œä¸€æ¢ user è¨Šæ¯ï¼ˆç”¨æ–¼å“æª¢å’Œæ—¥èªŒï¼‰
        last_user = ""
        for m in messages:
            if m.get("role") == "user":
                last_user = (m.get("content") or "").strip()

        print(f"ğŸ“‹ TaskPlanner: {plan.level} {plan.label} | domain={plan.domain} | chain={plan.model_chain}")

        # â”€â”€ local_only æ¨¡å¼ï¼šéæ¿¾æ‰é›²ç«¯æ¨¡å‹ â”€â”€
        chain = list(plan.model_chain)
        cloud_models = {"deepseek", "gemini", "claude", "minimax"}
        if self.mode == "local_only":
            chain = [m for m in chain if m not in cloud_models]
            if not chain:
                chain = ["ollama_fast", "ollama", "brain", "llama_swap"]
        elif self.mode == "cloud_first":
            # é›²ç«¯å„ªå…ˆï¼šæŠŠé›²ç«¯æ¨¡å‹æ’å‰é¢
            local = [m for m in chain if m not in cloud_models]
            cloud = [m for m in chain if m in cloud_models]
            if not cloud:
                cloud = ["deepseek", "minimax", "claude"]
            chain = cloud + local

        # â”€â”€ Pre-Planningï¼šæº–å‚™ messages â”€â”€
        prepared = self._pre_planner.prepare_messages(plan, messages) if self._pre_planner else messages

        # â”€â”€ åŸ·è¡Œ â”€â”€
        response = await self._call_with_fallback(prepared, chain, f"{plan.level}_{plan.label}")
        duration_ms = int((time.time() - t0) * 1000)

        # â”€â”€ QualityGate å“è³ªæª¢æŸ¥ â”€â”€
        if plan.quality_gate and not self._is_error_response(response):
            try:
                from ai_modules.ai_sop import QualityGate
                qg = QualityGate(min_score=plan.quality_min_score)
                result = qg.check(last_user, response)
                if not result.get("pass", True):
                    issues = result.get("issues", [])
                    print(f"âš ï¸ QualityGate ä¸é€šé: {issues} (score={result.get('score', 0)})")
                    # è‡ªå‹•å‡ç´šï¼šç”¨æ›´å¼·çš„æ¨¡å‹é‡è©¦
                    if plan.escalation:
                        escalation_chain = ["deepseek", "minimax", "claude"]
                        # local_only æ¨¡å¼ä¸å‡ç´šåˆ°é›²ç«¯
                        if self.mode != "local_only":
                            print(f"ğŸ”„ å“è³ªå‡ç´š: äº¤çµ¦ {escalation_chain}")
                            response2 = await self._call_with_fallback(prepared, escalation_chain, f"{plan.level}_escalated")
                            if not self._is_error_response(response2):
                                response = response2
                    elif plan.max_retries > 0:
                        # åŒæ¨¡å‹é‡è©¦ä¸€æ¬¡
                        print(f"ğŸ”„ é‡è©¦ä¸­...")
                        response2 = await self._call_with_fallback(prepared, chain[:2], f"{plan.level}_retry")
                        if not self._is_error_response(response2):
                            result2 = qg.check(last_user, response2)
                            if result2.get("score", 0) > result.get("score", 0):
                                response = response2
            except ImportError:
                pass

        # â”€â”€ è¨˜éŒ„åˆ°è¨“ç·´æ—¥èªŒ â”€â”€
        try:
            self._task_planner.log_task(plan, last_user, response, chain[0], duration_ms)
        except Exception:
            pass

        return response


class LlamaSwapService(BaseAIService):
    """
    llama-swap å¾Œç«¯ï¼šOpenAI ç›¸å®¹ APIï¼Œå¤šæ¨¡å‹ç†±åˆ‡æ›ã€‚
    ç•¶ Ollama ä¸å¯ç”¨æ™‚ä½œç‚ºæœ¬åœ° fallbackï¼Œport 10005ã€‚
    """

    def __init__(self, model_name: str | None = None):
        self.base_url = LLAMA_SWAP_URL
        self.model_name = model_name or LLAMA_SWAP_MODEL
        self.timeout = LLAMA_SWAP_TIMEOUT

    async def chat(self, messages: list) -> str:
        import httpx
        ollama_messages = _react_to_ollama_messages(list(messages))
        if not ollama_messages:
            return _error_json("LlamaSwapï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")
        body = {
            "model": self.model_name,
            "messages": ollama_messages,
            "max_tokens": 4096,
            "stream": False,
            "temperature": 0.3,
        }
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(f"{self.base_url}/v1/chat/completions", json=body)
                r.raise_for_status()
                data = r.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                if content and content.strip():
                    return content.strip()
                return _error_json("LlamaSwap å›æ‡‰ç‚ºç©º")
        except Exception as e:
            return _error_json(f"LlamaSwap æœå‹™ç•°å¸¸: {e}")


class AIServiceFactory:
    """æœå‹™å·¥å» ï¼šä¸€éµåˆ‡æ›é›²ç«¯å¼•æ“ï¼›é è¨­æ™ºæ…§è·¯ç”±ã€‚"""

    @staticmethod
    def create(provider: str) -> BaseAIService:
        if provider == "gemini":
            return GeminiService()
        if provider == "ollama":
            return OllamaService()
        if provider == "claude":
            return ClaudeService()
        if provider == "minimax":
            return MiniMaxService()
        if provider == "deepseek":
            return DeepSeekService()
        if provider == "llama_swap":
            return LlamaSwapService()
        if provider == "smart":
            return SmartAIService()
        if provider == "aliyun":
            return AliyunService()
        if provider == "tencent":
            return TencentService()
        if provider == "litellm":
            return LiteLLMService()
        # é è¨­ï¼šæ ¹æ“š AI_COST_MODE æ±ºå®š
        if AI_COST_MODE == "smart_route":
            return SmartAIService()
        return OllamaService()

    get_service = create  # ç›¸å®¹èˆŠç”¨æ³•

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€å¤§è…¦ - AI æœå‹™æ¨¡å¡Š
æä¾›èˆ‡ OpenAI GPT çš„é€£æ¥
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Optional
from openai import AsyncOpenAI
from config_ai import AIConfig
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

logger = logging.getLogger(__name__)

class AIService:
    """AI æœå‹™é¡ - ç®¡ç†èˆ‡ OpenAI çš„é€£æ¥"""
    
    def __init__(self, config: AIConfig = None):
        self.config = config or AIConfig.load_from_env()
        self.client: Optional[AsyncOpenAI] = None
        self.conversation_history: List[dict] = []
        self.cost_tracking: float = 0.0
        
        if AIConfig.validate(self.config):
            try:
                if self.config.MODEL_TYPE.value == "demo":
                    logger.info("ğŸ”„ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
                    return
                
                self.client = AsyncOpenAI(
                    api_key=self.config.get_api_key(),
                    base_url=self.config.get_api_base()
                )
                
                logger.info("âœ“ AI æœå‹™åˆå§‹åŒ–æˆåŠŸ")
                logger.info(f"  é¡å‹: {self.config.MODEL_TYPE.value}")
                logger.info(f"  æ¨¡å‹: {self.config.get_model_name()}")
                logger.info(f"  API: {self.config.get_api_base()}")
                
            except Exception as e:
                logger.error(f"âœ— AI æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.info("ğŸ”„ åˆ‡æ›åˆ°æ¼”ç¤ºæ¨¡å¼")
    
    async def generate_response(self, message: str, session_id: str = None) -> str:
        """ç”Ÿæˆ AI å›æ‡‰"""
        try:
            # å¦‚æœæ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨åŸºç¤å›æ‡‰
            if self.config.MODEL_TYPE.value == "demo" or not self.client:
                return await self._demo_response(message)
            
            # æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡
            messages = self._build_messages(message, session_id)
            
            # èª¿ç”¨ AI API
            logger.info(f"æ­£åœ¨èª¿ç”¨ {self.config.MODEL_TYPE.value} æ¨¡å‹è™•ç†æ¶ˆæ¯: {message[:50]}...")
            
            response = await self.client.chat.completions.create(
                model=self.config.get_model_name(),
                messages=messages,
                max_tokens=self.config.MAX_TOKENS,
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P
            )
            
            # æå–å›æ‡‰
            assistant_message = response.choices[0].message.content
            
            # è¿½è¹¤æˆæœ¬ï¼ˆåƒ…é©ç”¨æ–¼ OpenAIï¼‰
            if self.config.MODEL_TYPE.value == "openai" and self.config.ENABLE_COST_TRACKING:
                if hasattr(response, 'usage') and response.usage:
                    tokens_used = response.usage.total_tokens
                    self.cost_tracking += self._calculate_cost(tokens_used)
                    logger.info(f"Token ä½¿ç”¨é‡: {tokens_used}, ç´¯è¨ˆæˆæœ¬: ${self.cost_tracking:.4f}")
            
            # ä¿å­˜åˆ°å°è©±æ­·å²
            self._update_history(message, assistant_message, session_id)
            
            return assistant_message
            
        except Exception as e:
            logger.error(f"AI ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            # å¦‚æœ API èª¿ç”¨å¤±æ•—ï¼Œåˆ‡æ›åˆ°æ¼”ç¤ºæ¨¡å¼
            return await self._demo_response(message)
    
    def _build_messages(self, user_message: str, session_id: str = None) -> List[dict]:
        """æ§‹å»ºå¸¶æœ‰ä¸Šä¸‹æ–‡çš„å°è©±æ¶ˆæ¯åˆ—è¡¨"""
        # ç³»çµ±æç¤ºè© - å®šç¾©ç¯‰æœªç§‘æŠ€å¤§è…¦çš„è§’è‰²
        system_prompt = f"""ä½ æ˜¯ç¯‰æœªç§‘æŠ€å¤§è…¦ï¼Œä¸€å€‹æ™ºæ…§ã€å°ˆæ¥­çš„é›»è…¦ä»£ç†äººã€‚

ä½ çš„è§’è‰²å’Œä»»å‹™ï¼š
â€¢ æä¾›æ™ºèƒ½ã€å‹å¥½çš„å°è©±æœå‹™
â€¢ å›ç­”ç”¨æˆ¶é—œæ–¼æ™‚é–“ã€ç³»çµ±ç‹€æ…‹ã€ä¸€èˆ¬çŸ¥è­˜çš„å•é¡Œ
â€¢ å”åŠ©ç”¨æˆ¶åŸ·è¡Œå„ç¨®ä»»å‹™
â€¢ ç¶­è­·å°ˆæ¥­ã€æœ‰ç¦®è²Œçš„èªæ°£

å›ç­”é¢¨æ ¼ï¼š
â€¢ ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡
â€¢ èªæ°£å‹å¥½ã€å°ˆæ¥­
â€¢ å›æ‡‰ç°¡æ½”æ˜äº†
â€¢ é©æ™‚ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿè®“å°è©±æ›´ç”Ÿå‹•

ç•¶å‰æ™‚é–“: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ å°è©±æ­·å²
        if session_id and len(self.conversation_history) > 0:
            recent_history = self.conversation_history[-self.config.CONTEXT_MESSAGES:]
            messages.extend(recent_history)
        
        # æ·»åŠ ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def _update_history(self, user_message: str, assistant_message: str, session_id: str):
        """æ›´æ–°å°è©±æ­·å²"""
        self.conversation_history.append({
            "role": "user",
            "content": user_message,
            "timestamp": datetime.now().isoformat()
        })
        
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_message,
            "timestamp": datetime.now().isoformat()
        })
        
        # é™åˆ¶æ­·å²è¨˜éŒ„é•·åº¦ï¼ˆé™åˆ¶ tokens ä½¿ç”¨é‡ï¼‰
        max_history = self.config.CONTEXT_MESSAGES * 2  # ç”¨æˆ¶+åŠ©æ‰‹æ¶ˆæ¯
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
    
    def _calculate_cost(self, tokens: int) -> float:
        """è¨ˆç®— API æˆæœ¬ï¼ˆä¼°ç®—ï¼‰"""
        # GPT-4o-mini å®šåƒ¹ï¼š$0.15/1M input tokens, $0.60/1M output tokens
        # ç°¡åŒ–è¨ˆç®—ï¼šå¹³å‡ $0.375/1M tokens
        cost_per_1m_tokens = 0.375
        return (tokens / 1_000_000) * cost_per_1m_tokens
    
    async def _demo_response(self, message: str) -> str:
        """æ¼”ç¤ºæ¨¡å¼å›æ‡‰"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['ä½ å¥½', 'hello', 'hi', 'å—¨']):
            return f"æ‚¨å¥½ï¼æˆ‘æ˜¯ç¯‰æœªç§‘æŠ€å¤§è…¦ã€‚\n\n" \
                   f"ğŸ¤– ç•¶å‰æ¨¡å¼: {self.config.MODEL_TYPE.value.upper()}\n" \
                   f"ğŸ“‹ å¯ç”¨åŠŸèƒ½ï¼š\n" \
                   f"â€¢ æ™ºèƒ½å°è©±\n" \
                   f"â€¢ ç³»çµ±ç›£æ§\n" \
                   f"â€¢ æ–‡ä»¶ç®¡ç†\n" \
                   f"\nğŸ’¡ æç¤ºï¼šå¯ä»¥è¨­ç½®ç’°å¢ƒè®Šé‡åˆ‡æ›åˆ° Ollama æˆ– OpenAI æ¨¡å¼\n" \
                   f"æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ"
        
        elif 'æ™‚é–“' in message_lower or 'date' in message_lower:
            from datetime import datetime
            return f"ç¾åœ¨æ™‚é–“æ˜¯ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}"
        
        elif 'ç‹€æ…‹' in message_lower or 'status' in message_lower:
            return f"ğŸ¤– ç¯‰æœªç§‘æŠ€å¤§è…¦ç‹€æ…‹ï¼š\n" \
                   f"â€¢ æ¨¡å¼: {self.config.MODEL_TYPE.value.upper()}\n" \
                   f"â€¢ æ¨¡å‹: {self.config.get_model_name()}\n" \
                   f"â€¢ å°è©±æ­·å²: {len(self.conversation_history)} æ¢\n" \
                   f"â€¢ ç³»çµ±é‹è¡Œæ­£å¸¸"
        
        elif any(word in message_lower for word in ['ollama', 'æœ¬åœ°æ¨¡å‹', 'local']):
            return "ğŸ’¡ è¦ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼š\n" \
                   "1. å®‰è£ Ollama: https://ollama.ai/\n" \
                   "2. æ‹‰å–æ¨¡å‹: `ollama pull llama3.1`\n" \
                   "3. è¨­ç½®ç’°å¢ƒè®Šé‡: `AI_MODEL_TYPE=ollama`\n" \
                   "4. é‡å•Ÿæœå‹™å³å¯ä½¿ç”¨æœ¬åœ° AI"
        
        else:
            return f"æˆ‘æ”¶åˆ°äº†æ‚¨çš„è¨Šæ¯ï¼šã€Œ{message}ã€\n\n" \
                   f"ğŸ¤– ç¯‰æœªç§‘æŠ€å¤§è…¦æ­£åœ¨ç‚ºæ‚¨æœå‹™ã€‚\n" \
                   f"ğŸ’¡ ç•¶å‰ä½¿ç”¨ {self.config.MODEL_TYPE.value} æ¨¡å¼\n" \
                   f"ğŸ“‹ å¯ä»¥è©¢å•æˆ‘ï¼š\n" \
                   f"â€¢ ç³»çµ±ç‹€æ…‹\n" \
                   f"â€¢ ç•¶å‰æ™‚é–“\n" \
                   f"â€¢ å¦‚ä½•é€£æ¥ Ollama\n" \
                   f"â€¢ å…¶ä»–å•é¡Œ"
    
    def get_usage_stats(self) -> dict:
        """ç²å–ä½¿ç”¨çµ±è¨ˆ"""
        return {
            "total_messages": len(self.conversation_history) // 2,
            "current_cost": round(self.cost_tracking, 4),
            "model": self.config.get_model_name(),
            "model_type": self.config.MODEL_TYPE.value,
            "context_messages": len(self.conversation_history)
        }
    
    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        self.conversation_history = []
        logger.info("å°è©±æ­·å²å·²æ¸…é™¤")

# FastAPI åº”ç”¨
app = FastAPI()

# å…è®¸è·¨åŸŸè¯·æ±‚ï¼ˆå¯é€‰ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– AI æœåŠ¡
ai_service = AIService()

@app.get("/chat")
async def chat(message: str):
    try:
        response = await ai_service.generate_response(message)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=80)

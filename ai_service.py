<<<<<<< HEAD
# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€ â€” AI æœå‹™ä»‹é¢ï¼Œå°æ¥ Google Gemini / Ollama / é˜¿é‡Œé›²ï¼ˆé ç•™ï¼‰ç­‰
ç’°å¢ƒåŠ è¼‰ .envï¼›BaseAIService æŠ½è±¡åŸºé¡ï¼Œå„æœå‹™å¯¦ä½œ chat(messages)ï¼›AIServiceFactory.create(provider) ä¸€éµåˆ‡æ›ã€‚
"""
import asyncio
import json
import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Sequence

# ç’°å¢ƒåŠ è¼‰ï¼šè®€å– .env æª”æ¡ˆä¸­çš„ GEMINI_API_KEYï¼ˆå°ˆæ¡ˆæ ¹ç›®éŒ„å„ªå…ˆï¼Œå†è¼‰å…¥ ~/.openclaw/.envï¼Œä¸è¦†å¯«æ—¢æœ‰éµï¼‰
try:
    from dotenv import load_dotenv
    ROOT = Path(__file__).resolve().parent
    load_dotenv(ROOT / ".env")
    ue = Path(os.path.expanduser("~/.openclaw/.env"))
    if ue.is_file():
        load_dotenv(ue, override=False)
except ImportError:
    pass

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "").strip()
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")
GEMINI_TIMEOUT = float(os.environ.get("GEMINI_TIMEOUT", "60"))

OLLAMA_BASE_URL = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11434").rstrip("/")
OLLAMA_MODEL = os.environ.get("OLLAMA_CODER_MODEL") or os.environ.get("OLLAMA_MODEL", "qwen2.5-coder:7b")
OLLAMA_TIMEOUT = float(os.environ.get("OLLAMA_TIMEOUT", "120"))


def _error_json(message: str) -> str:
    """å›å‚³ç¬¦åˆ ReAct è¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ï¼Œè®“ AgentManager èƒ½è­˜åˆ¥ä¸¦çµæŸã€‚"""
    return json.dumps({"done": True, "result": message}, ensure_ascii=False)


class BaseAIService(ABC):
    """AI æœå‹™æŠ½è±¡åŸºé¡ï¼šæ‰€æœ‰å¼•æ“å¯¦ä½œ chat(messages) -> strã€‚"""

    @abstractmethod
    async def chat(self, messages: list) -> str:
        pass


def _react_to_gemini_history(messages: Sequence[dict]) -> tuple[str, list, str]:
    """
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ã€‚
    å›å‚³ (system_instruction, history, last_user_content)ã€‚
    Gemini: history = [{"role": "user"|"model", "parts": [str]}, ...]ï¼›æœ€å¾Œä¸€å‰‡ user å–®ç¨ä½œç‚º send_message çš„è¼¸å…¥ã€‚
    """
    system_instruction = ""
    gemini_history = []
    last_user_content = ""

    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_instruction = content
        elif role == "user":
            last_user_content = content
            gemini_history.append({"role": "user", "parts": [content]})
        elif role == "assistant":
            gemini_history.append({"role": "model", "parts": [content]})

    # æœ€å¾Œä¸€å‰‡ç‚º userï¼Œä½œç‚ºæœ¬è¼ªè¦é€çµ¦ Gemini çš„è¼¸å…¥ï¼›history ç‚ºå…¶å‰çš„æ‰€æœ‰è¼ª
    if gemini_history and gemini_history[-1]["role"] == "user":
        last_user_content = gemini_history[-1]["parts"][0]
        history_for_chat = gemini_history[:-1]
    else:
        history_for_chat = gemini_history

    return system_instruction, history_for_chat, last_user_content


def _gemini_chat_sync(messages: Sequence[dict]) -> str:
    """
    åŒæ­¥å‘¼å« Geminiï¼šReAct history è½‰ Gemini æ ¼å¼ï¼Œstart_chat + send_messageã€‚
    API è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ _error_json(...)ã€‚
    """
    if not GEMINI_API_KEY:
        return _error_json("æœªè¨­å®š GEMINI_API_KEYï¼Œè«‹åœ¨ .env æˆ– ~/.openclaw/.env è¨­å®šã€‚")

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        system_instruction, history_for_chat, last_user_content = _react_to_gemini_history(messages)
        model = genai.GenerativeModel(
            GEMINI_MODEL,
            system_instruction=system_instruction or None,
        )
        chat = model.start_chat(history=history_for_chat)
        response = chat.send_message(last_user_content or "è«‹å›è¦†ã€‚")
        if response and response.text:
            return response.text.strip()
        return _error_json("Gemini æœªå›å‚³æ–‡å­—ã€‚")
    except Exception as e:
        return _error_json(f"API éŒ¯èª¤: {e}")


class AIService(BaseAIService):
    """
    å°æ¥ Google Gemini APIï¼šå¯¦ä½œ chat(messages) ç•°æ­¥æ–¹æ³•ï¼Œ
    å°‡ ReAct çš„æ­·å²ç´€éŒ„è½‰æ›ç‚º Gemini çš„å°è©±æ ¼å¼ä¸¦å‘¼å« APIï¼›
    è¶…æ™‚æˆ–å¤±æ•—æ™‚å›å‚³ç¬¦åˆè¦ç¯„çš„ JSON éŒ¯èª¤è¨Šæ¯ä¾› AgentManager è­˜åˆ¥ã€‚
    """

    async def chat(self, messages: list) -> str:
        """
        ç•°æ­¥å‘¼å« Geminiï¼›messages ç‚º ReAct historyï¼ˆå« system / user / assistantï¼‰ã€‚
        å…§éƒ¨è½‰ç‚º Gemini å°è©±æ ¼å¼ä¸¦ä»¥ run_in_executor åŸ·è¡ŒåŒæ­¥è«‹æ±‚ï¼Œå¸¶è¶…æ™‚ã€‚
        """
        try:
            return await asyncio.wait_for(
                asyncio.to_thread(_gemini_chat_sync, list(messages)),
                timeout=GEMINI_TIMEOUT,
            )
        except asyncio.TimeoutError:
            return _error_json(f"API é€¾æ™‚ï¼ˆ{GEMINI_TIMEOUT} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"å‘¼å«å¤±æ•—: {e}")


# ä¾› brain_server ç­‰æ¨¡çµ„ä½¿ç”¨ï¼šfrom ai_service import GeminiService, OllamaService
GeminiService = AIService


def _react_to_ollama_messages(messages: Sequence[dict]) -> list[dict]:
    """å°‡ ReAct æ­·å²è½‰ç‚º Ollama /api/chat çš„ messagesï¼šsystem ä½µå…¥é¦–å‰‡ userï¼Œå…¶é¤˜ user/assistant ç…§æ¬ã€‚"""
    out = []
    system_parts = []
    for m in messages:
        role = m.get("role", "")
        content = (m.get("content") or "").strip()
        if role == "system":
            system_parts.append(content)
        elif role == "user":
            content = ("\n\n".join(system_parts) + "\n\n" + content).strip() if system_parts else content
            system_parts = []
            out.append({"role": "user", "content": content})
        elif role == "assistant":
            out.append({"role": "assistant", "content": content})
    return out


class OllamaService(BaseAIService):
    """
    æœ¬åœ°å¤§è…¦ï¼šå‘¼å«æœ¬åœ° Ollama æœå‹™ï¼ˆQwen2.5-Coder ç­‰ï¼‰ï¼Œå…è²»/æ¶ˆè€—é›»åŠ›ã€‚
    ä½¿ç”¨ httpx éåŒæ­¥ POST /api/chatï¼›stream=Falseï¼Œoptions ç¢ºä¿è¼¸å‡ºç©©å®šã€æ“´å¤§ä¸Šä¸‹æ–‡ã€‚
    è‹¥æœå‹™ç•°å¸¸ï¼Œå›å‚³ JSON éŒ¯èª¤ï¼ˆå« done: Trueï¼‰è®“ AgentManager è­˜åˆ¥ã€‚
    """

    def __init__(self, model_name: str | None = None):
        base = (os.environ.get("OLLAMA_BASE_URL") or "http://localhost:11434").rstrip("/")
        self.base_url = f"{base}/api/chat"
        self.model_name = model_name or OLLAMA_MODEL
        self.timeout = OLLAMA_TIMEOUT

    async def chat(self, messages: list) -> str:
        """
        å‘¼å«æœ¬åœ° Ollama æœå‹™ï¼›messages ç‚º ReAct historyï¼Œæœƒå…ˆè½‰ç‚º Ollama çš„ messages æ ¼å¼ã€‚
        """
        ollama_messages = _react_to_ollama_messages(list(messages))
        if not ollama_messages:
            return _error_json("Ollamaï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")

        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "stream": False,
            "options": {
                "temperature": 0.2,  # é™ä½æº«åº¦ä»¥æé«˜ç©©å®šæ€§ [cite: 2026-02-05]
                "num_ctx": 8192,     # æ“´å¤§ä¸Šä¸‹æ–‡ï¼Œé©åˆè™•ç†é•·ç¨‹å¼ç¢¼
            },
        }

        try:
            import httpx
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(self.base_url, json=payload)
                response.raise_for_status()
                result = response.json()
                content = (result.get("message") or {}).get("content") or ""
                return content.strip() or _error_json("Ollama æœªå›å‚³æ–‡å­—ã€‚")
        except Exception as e:
            # è‹¥æœ¬åœ°æœå‹™ç•°å¸¸ï¼Œå›å‚³ JSON éŒ¯èª¤è®“ Manager è­˜åˆ¥ï¼ˆdone + result èˆ‡ ReAct ä¸€è‡´ï¼‰
            return _error_json(f"Ollama æœå‹™ç•°å¸¸: {e}")


class AliyunService(BaseAIService):
    """é ç•™ï¼šé˜¿é‡Œé›²é€šç¾©åƒå• (Qwen API)ï¼Œæœªä¾†å°æ¥ DashScope APIã€‚"""

    async def chat(self, messages: list) -> str:
        # æœªä¾†åœ¨æ­¤è™•å°æ¥ DashScope API
        return json.dumps({"thought": "é˜¿é‡Œé›²æ­£åœ¨æ€è€ƒ...", "done": True, "result": "é˜¿é‡Œé›²æœå‹™å°šæœªæ¥ç·šã€‚"}, ensure_ascii=False)


class TencentService(BaseAIService):
    """é ç•™ï¼šé¨°è¨Šé›²æ··å…ƒç­‰ï¼Œå°šæœªæ¥ç·šã€‚"""

    async def chat(self, messages: list) -> str:
        return _error_json("é¨°è¨Šé›²æœå‹™å°šæœªæ¥ç·šï¼Œè«‹ä½¿ç”¨ gemini æˆ– ollamaã€‚")


ANTHROPIC_API_KEY = (os.environ.get("ANTHROPIC_API_KEY") or os.environ.get("CLAUDE_API_KEY") or "").strip()
ANTHROPIC_MODEL = os.environ.get("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
ANTHROPIC_TIMEOUT = float(os.environ.get("ANTHROPIC_TIMEOUT", "120"))


class ClaudeService(BaseAIService):
    """
    éšæ®µ 4/6 ç·¨ç¢¼èˆ‡å›é¥‹ä¿®æ­£ï¼šå°æ¥ Anthropic Claude APIã€‚
    æœ‰è¨­ ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEY æ™‚ï¼Œagent_logic å¯æ³¨å…¥ä¾›éšæ®µ 4/6 ä½¿ç”¨ã€‚
    """

    def __init__(self, api_key: str | None = None, model: str | None = None):
        self.api_key = (api_key or ANTHROPIC_API_KEY).strip()
        self.model = model or ANTHROPIC_MODEL
        self.timeout = ANTHROPIC_TIMEOUT

    async def chat(self, messages: list) -> str:
        if not self.api_key:
            return _error_json("æœªè¨­å®š ANTHROPIC_API_KEY æˆ– CLAUDE_API_KEYï¼Œè«‹åœ¨ .env è¨­å®šã€‚")
        system_parts = []
        anthropic_messages = []
        for m in messages:
            role = m.get("role", "")
            content = (m.get("content") or "").strip()
            if role == "system":
                system_parts.append(content)
            elif role == "user":
                anthropic_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                anthropic_messages.append({"role": "assistant", "content": content})
        if not anthropic_messages:
            return _error_json("Claudeï¼šç„¡æœ‰æ•ˆå°è©±å…§å®¹ã€‚")
        try:
            from anthropic import AsyncAnthropic
            client = AsyncAnthropic(api_key=self.api_key)
            system = "\n\n".join(system_parts).strip() if system_parts else None
            response = await asyncio.wait_for(
                client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    system=system or None,
                    messages=anthropic_messages,
                ),
                timeout=self.timeout,
            )
            text = (response.content[0].text if response.content else "").strip()
            return text or _error_json("Claude æœªå›å‚³æ–‡å­—ã€‚")
        except asyncio.TimeoutError:
            return _error_json(f"Claude API é€¾æ™‚ï¼ˆ{self.timeout} ç§’ï¼‰ã€‚")
        except Exception as e:
            return _error_json(f"Claude API éŒ¯èª¤: {e}")


class SmartAIService(BaseAIService):
    """
    æ™ºèƒ½èª¿åº¦ï¼šæœ¬åœ°å„ªå…ˆç­–ç•¥ [cite: 2026-02-05]
    - ç°¡å–®ä»»å‹™ / ç¨‹å¼ç¢¼ç‰‡æ®µï¼šå¼·åˆ¶æœ¬åœ°ï¼ˆç¯€çœé‡‘å¹£ï¼‰
    - è¤‡é›œä»»å‹™ï¼šå…ˆè©¦ Ollamaï¼Œå¤±æ•—æˆ–é‚è¼¯ä¸è¶³å†è½‰ Gemini
    """

    def __init__(self, gemini_service: BaseAIService | None = None, ollama_service: BaseAIService | None = None):
        self.gemini = gemini_service if gemini_service is not None else AIService()
        self.ollama = ollama_service if ollama_service is not None else OllamaService()
        self.threshold = 0.7  # ä¿¡å¿ƒæ°´æº–é–€æª» [cite: 2026-02-05]

    async def _call_ollama(self, messages: list) -> str:
        """æ¥ç¾æœ‰ Ollama å‘¼å«é‚è¼¯ [cite: 2026-02-05]"""
        return await self.ollama.chat(messages)

    async def _call_gemini(self, messages: list) -> str:
        """æ¥ç¾æœ‰ Gemini API å‘¼å«é‚è¼¯"""
        return await self.gemini.chat(messages)

    async def smart_request(self, prompt: str, task_type: str = "conversation") -> str:
        """
        æ™ºèƒ½èª¿åº¦å…¥å£ï¼šæœ¬åœ°å„ªå…ˆç­–ç•¥ [cite: 2026-02-05]
        prompt ç‚ºå–®ä¸€ä½¿ç”¨è€…è¼¸å…¥æ™‚ä½¿ç”¨ï¼›è‹¥éœ€ ReAct å…¨æ­·å²è«‹ç”¨ chat(messages)ã€‚
        """
        messages = [{"role": "user", "content": prompt}]
        if task_type == "code_snippet" or len(prompt) < 100:
            return await self._call_ollama(messages)
        try:
            print("ğŸ›¡ï¸ é˜²è­·ç›¾ï¼šå˜—è©¦ä½¿ç”¨æœ¬åœ°ç®—åŠ› (RTX 4060 Ti)...")
            response = await self._call_ollama(messages)
            if "Unknown action" in (response or "") or not (response or "").strip():
                raise Exception("æœ¬åœ°é‚è¼¯ä¿¡å¿ƒä¸è¶³")
            return response
        except Exception as e:
            print(f"ğŸ’° æœ¬åœ°é˜²ç·šå¤±å®ˆ (åŸå› : {e})ï¼Œå•Ÿå‹• Gemini æ•‘æ´...")
            return await self._call_gemini(messages)

    async def chat(self, messages: list) -> str:
        """
        èˆ‡ BaseAIService ä¸€è‡´ï¼šReAct å…¨æ­·å²èª¿åº¦ã€‚çŸ­å…§å®¹å¼·åˆ¶æœ¬åœ°ï¼Œå¦å‰‡å…ˆæœ¬åœ°å¾Œé›²ç«¯ã€‚
        """
        last_user = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                last_user = (m.get("content") or "").strip()
                break
        if len(last_user) < 100:
            return await self._call_ollama(messages)
        try:
            print("ğŸ›¡ï¸ é˜²è­·ç›¾ï¼šå˜—è©¦ä½¿ç”¨æœ¬åœ°ç®—åŠ› (RTX 4060 Ti)...")
            response = await self._call_ollama(messages)
            if "Unknown action" in (response or "") or not (response or "").strip():
                raise Exception("æœ¬åœ°é‚è¼¯ä¿¡å¿ƒä¸è¶³")
            return response
        except Exception as e:
            print(f"ğŸ’° æœ¬åœ°é˜²ç·šå¤±å®ˆ (åŸå› : {e})ï¼Œå•Ÿå‹• Gemini æ•‘æ´...")
            return await self._call_gemini(messages)


class AIServiceFactory:
    """æœå‹™å·¥å» ï¼šä¸€éµåˆ‡æ›é›²ç«¯å¼•æ“ï¼›é è¨­æœ¬åœ° Ollama æœ€çœéŒ¢ã€‚"""

    @staticmethod
    def create(provider: str) -> BaseAIService:
        if provider == "gemini":
            return GeminiService()
        if provider == "ollama":
            return OllamaService()
        if provider == "claude":
            return ClaudeService()
        if provider == "aliyun":
            return AliyunService()
        if provider == "tencent":
            return TencentService()
        return OllamaService()  # é è¨­æœ¬åœ°æœ€çœéŒ¢

    get_service = create  # ç›¸å®¹èˆŠç”¨æ³•
=======
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯‰æœªç§‘æŠ€å¤§è…¦ - AI æœå‹™æ¨¡å¡Š
æä¾›èˆ‡ OpenAI GPT çš„é€£æ¥
"""

import asyncio
import logging
from datetime import datetime
from typing import List, Optional
from openai import AsyncOpenAI
from config_ai import AIConfig
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

logger = logging.getLogger(__name__)

class AIService:
    """AI æœå‹™é¡ - ç®¡ç†èˆ‡ OpenAI çš„é€£æ¥"""
    
    def __init__(self, config: AIConfig = None):
        self.config = config or AIConfig.load_from_env()
        self.client: Optional[AsyncOpenAI] = None
        self.conversation_history: List[dict] = []
        self.cost_tracking: float = 0.0
        
        if AIConfig.validate(self.config):
            try:
                if self.config.MODEL_TYPE.value == "demo":
                    logger.info("ğŸ”„ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
                    return
                
                self.client = AsyncOpenAI(
                    api_key=self.config.get_api_key(),
                    base_url=self.config.get_api_base()
                )
                
                logger.info("âœ“ AI æœå‹™åˆå§‹åŒ–æˆåŠŸ")
                logger.info(f"  é¡å‹: {self.config.MODEL_TYPE.value}")
                logger.info(f"  æ¨¡å‹: {self.config.get_model_name()}")
                logger.info(f"  API: {self.config.get_api_base()}")
                
            except Exception as e:
                logger.error(f"âœ— AI æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.info("ğŸ”„ åˆ‡æ›åˆ°æ¼”ç¤ºæ¨¡å¼")
    
    async def generate_response(self, message: str, session_id: str = None) -> str:
        """ç”Ÿæˆ AI å›æ‡‰"""
        try:
            # å¦‚æœæ˜¯æ¼”ç¤ºæ¨¡å¼ï¼Œä½¿ç”¨åŸºç¤å›æ‡‰
            if self.config.MODEL_TYPE.value == "demo" or not self.client:
                return await self._demo_response(message)
            
            # æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡
            messages = self._build_messages(message, session_id)
            
            # èª¿ç”¨ AI API
            logger.info(f"æ­£åœ¨èª¿ç”¨ {self.config.MODEL_TYPE.value} æ¨¡å‹è™•ç†æ¶ˆæ¯: {message[:50]}...")
            
            response = await self.client.chat.completions.create(
                model=self.config.get_model_name(),
                messages=messages,
                max_tokens=self.config.MAX_TOKENS,
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P
            )
            
            # æå–å›æ‡‰
            assistant_message = response.choices[0].message.content
            
            # è¿½è¹¤æˆæœ¬ï¼ˆåƒ…é©ç”¨æ–¼ OpenAIï¼‰
            if self.config.MODEL_TYPE.value == "openai" and self.config.ENABLE_COST_TRACKING:
                if hasattr(response, 'usage') and response.usage:
                    tokens_used = response.usage.total_tokens
                    self.cost_tracking += self._calculate_cost(tokens_used)
                    logger.info(f"Token ä½¿ç”¨é‡: {tokens_used}, ç´¯è¨ˆæˆæœ¬: ${self.cost_tracking:.4f}")
            
            # ä¿å­˜åˆ°å°è©±æ­·å²
            self._update_history(message, assistant_message, session_id)
            
            return assistant_message
            
        except Exception as e:
            logger.error(f"AI ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            # å¦‚æœ API èª¿ç”¨å¤±æ•—ï¼Œåˆ‡æ›åˆ°æ¼”ç¤ºæ¨¡å¼
            return await self._demo_response(message)
    
    def _build_messages(self, user_message: str, session_id: str = None) -> List[dict]:
        """æ§‹å»ºå¸¶æœ‰ä¸Šä¸‹æ–‡çš„å°è©±æ¶ˆæ¯åˆ—è¡¨"""
        # ç³»çµ±æç¤ºè© - å®šç¾©ç¯‰æœªç§‘æŠ€å¤§è…¦çš„è§’è‰²
        system_prompt = f"""ä½ æ˜¯ç¯‰æœªç§‘æŠ€å¤§è…¦ï¼Œä¸€å€‹æ™ºæ…§ã€å°ˆæ¥­çš„é›»è…¦ä»£ç†äººã€‚

ä½ çš„è§’è‰²å’Œä»»å‹™ï¼š
â€¢ æä¾›æ™ºèƒ½ã€å‹å¥½çš„å°è©±æœå‹™
â€¢ å›ç­”ç”¨æˆ¶é—œæ–¼æ™‚é–“ã€ç³»çµ±ç‹€æ…‹ã€ä¸€èˆ¬çŸ¥è­˜çš„å•é¡Œ
â€¢ å”åŠ©ç”¨æˆ¶åŸ·è¡Œå„ç¨®ä»»å‹™
â€¢ ç¶­è­·å°ˆæ¥­ã€æœ‰ç¦®è²Œçš„èªæ°£

å›ç­”é¢¨æ ¼ï¼š
â€¢ ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡
â€¢ èªæ°£å‹å¥½ã€å°ˆæ¥­
â€¢ å›æ‡‰ç°¡æ½”æ˜äº†
â€¢ é©æ™‚ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿè®“å°è©±æ›´ç”Ÿå‹•

ç•¶å‰æ™‚é–“: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"""
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ å°è©±æ­·å²
        if session_id and len(self.conversation_history) > 0:
            recent_history = self.conversation_history[-self.config.CONTEXT_MESSAGES:]
            messages.extend(recent_history)
        
        # æ·»åŠ ç•¶å‰ç”¨æˆ¶æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def _update_history(self, user_message: str, assistant_message: str, session_id: str):
        """æ›´æ–°å°è©±æ­·å²"""
        self.conversation_history.append({
            "role": "user",
            "content": user_message,
            "timestamp": datetime.now().isoformat()
        })
        
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_message,
            "timestamp": datetime.now().isoformat()
        })
        
        # é™åˆ¶æ­·å²è¨˜éŒ„é•·åº¦ï¼ˆé™åˆ¶ tokens ä½¿ç”¨é‡ï¼‰
        max_history = self.config.CONTEXT_MESSAGES * 2  # ç”¨æˆ¶+åŠ©æ‰‹æ¶ˆæ¯
        if len(self.conversation_history) > max_history:
            self.conversation_history = self.conversation_history[-max_history:]
    
    def _calculate_cost(self, tokens: int) -> float:
        """è¨ˆç®— API æˆæœ¬ï¼ˆä¼°ç®—ï¼‰"""
        # GPT-4o-mini å®šåƒ¹ï¼š$0.15/1M input tokens, $0.60/1M output tokens
        # ç°¡åŒ–è¨ˆç®—ï¼šå¹³å‡ $0.375/1M tokens
        cost_per_1m_tokens = 0.375
        return (tokens / 1_000_000) * cost_per_1m_tokens
    
    async def _demo_response(self, message: str) -> str:
        """æ¼”ç¤ºæ¨¡å¼å›æ‡‰"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['ä½ å¥½', 'hello', 'hi', 'å—¨']):
            return f"æ‚¨å¥½ï¼æˆ‘æ˜¯ç¯‰æœªç§‘æŠ€å¤§è…¦ã€‚\n\n" \
                   f"ğŸ¤– ç•¶å‰æ¨¡å¼: {self.config.MODEL_TYPE.value.upper()}\n" \
                   f"ğŸ“‹ å¯ç”¨åŠŸèƒ½ï¼š\n" \
                   f"â€¢ æ™ºèƒ½å°è©±\n" \
                   f"â€¢ ç³»çµ±ç›£æ§\n" \
                   f"â€¢ æ–‡ä»¶ç®¡ç†\n" \
                   f"\nğŸ’¡ æç¤ºï¼šå¯ä»¥è¨­ç½®ç’°å¢ƒè®Šé‡åˆ‡æ›åˆ° Ollama æˆ– OpenAI æ¨¡å¼\n" \
                   f"æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ"
        
        elif 'æ™‚é–“' in message_lower or 'date' in message_lower:
            from datetime import datetime
            return f"ç¾åœ¨æ™‚é–“æ˜¯ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}"
        
        elif 'ç‹€æ…‹' in message_lower or 'status' in message_lower:
            return f"ğŸ¤– ç¯‰æœªç§‘æŠ€å¤§è…¦ç‹€æ…‹ï¼š\n" \
                   f"â€¢ æ¨¡å¼: {self.config.MODEL_TYPE.value.upper()}\n" \
                   f"â€¢ æ¨¡å‹: {self.config.get_model_name()}\n" \
                   f"â€¢ å°è©±æ­·å²: {len(self.conversation_history)} æ¢\n" \
                   f"â€¢ ç³»çµ±é‹è¡Œæ­£å¸¸"
        
        elif any(word in message_lower for word in ['ollama', 'æœ¬åœ°æ¨¡å‹', 'local']):
            return "ğŸ’¡ è¦ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼š\n" \
                   "1. å®‰è£ Ollama: https://ollama.ai/\n" \
                   "2. æ‹‰å–æ¨¡å‹: `ollama pull llama3.1`\n" \
                   "3. è¨­ç½®ç’°å¢ƒè®Šé‡: `AI_MODEL_TYPE=ollama`\n" \
                   "4. é‡å•Ÿæœå‹™å³å¯ä½¿ç”¨æœ¬åœ° AI"
        
        else:
            return f"æˆ‘æ”¶åˆ°äº†æ‚¨çš„è¨Šæ¯ï¼šã€Œ{message}ã€\n\n" \
                   f"ğŸ¤– ç¯‰æœªç§‘æŠ€å¤§è…¦æ­£åœ¨ç‚ºæ‚¨æœå‹™ã€‚\n" \
                   f"ğŸ’¡ ç•¶å‰ä½¿ç”¨ {self.config.MODEL_TYPE.value} æ¨¡å¼\n" \
                   f"ğŸ“‹ å¯ä»¥è©¢å•æˆ‘ï¼š\n" \
                   f"â€¢ ç³»çµ±ç‹€æ…‹\n" \
                   f"â€¢ ç•¶å‰æ™‚é–“\n" \
                   f"â€¢ å¦‚ä½•é€£æ¥ Ollama\n" \
                   f"â€¢ å…¶ä»–å•é¡Œ"
    
    def get_usage_stats(self) -> dict:
        """ç²å–ä½¿ç”¨çµ±è¨ˆ"""
        return {
            "total_messages": len(self.conversation_history) // 2,
            "current_cost": round(self.cost_tracking, 4),
            "model": self.config.get_model_name(),
            "model_type": self.config.MODEL_TYPE.value,
            "context_messages": len(self.conversation_history)
        }
    
    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        self.conversation_history = []
        logger.info("å°è©±æ­·å²å·²æ¸…é™¤")

# FastAPI åº”ç”¨
app = FastAPI()

# å…è®¸è·¨åŸŸè¯·æ±‚ï¼ˆå¯é€‰ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ– AI æœåŠ¡
ai_service = AIService()

@app.get("/chat")
async def chat(message: str):
    try:
        response = await ai_service.generate_response(message)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=80)
>>>>>>> bd6537def53debaba0c16f279817e4a317eed98c
